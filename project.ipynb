{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "import tqdm.notebook as tq\n",
    "from datasets import Dataset\n",
    "from typing import Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load corpus\n",
    "\n",
    "data_path = Path(\"data/MELD_train_efr.json\")\n",
    "assert data_path.exists(), \"Data file is not present\"\n",
    "raw_df = pd.read_json(\n",
    "    data_path, dtype={\"speakers\": np.array}\n",
    ")  # , \"triggers\": np.array})\n",
    "EPISODE, SPEAKERS, EMOTIONS, UTTERANCES, TRIGGERS = raw_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Look for how many groups of episodes with the same first utterance there are and their lenghts\n",
    "\n",
    "raw_df.sort_values(by=UTTERANCES, inplace=True)\n",
    "groups = np.zeros((850,), dtype=int)\n",
    "\n",
    "index = 0\n",
    "count = 1\n",
    "for i in range(1, len(raw_df[UTTERANCES])):\n",
    "    if raw_df[UTTERANCES][i][0] == raw_df[UTTERANCES][i - 1][0]:\n",
    "        ### still in the same group\n",
    "        count += 1\n",
    "    else:\n",
    "        ### found new group\n",
    "        groups[index] = count\n",
    "        index += 1\n",
    "        count = 1\n",
    "\n",
    "groups = groups[groups != 0]\n",
    "\n",
    "print(f\"Number of groups: {len(groups)}\")\n",
    "print(f\"Avg group len: {np.average(groups):.1f}\")\n",
    "print(f\"Longest group: {np.max(groups)}\")\n",
    "print(f\"Episodes not in a group: {groups[groups == 1].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count how many speakers there are in each episode\n",
    "\n",
    "speakers_count = raw_df[SPEAKERS].apply(lambda arr: np.unique(arr).shape[0]).to_numpy()\n",
    "min_sp = np.min(speakers_count)\n",
    "max_sp = np.max(speakers_count)\n",
    "print(\"Distribution of number of speakers:\")\n",
    "for count in range(min_sp, max_sp + 1):\n",
    "    print(f\"{count} speakers:  {np.sum(speakers_count == count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class imbalance check\n",
    "classes_count = {}\n",
    "for emotions in raw_df[\"emotions\"]:\n",
    "    for emotion in emotions:\n",
    "        if emotion in classes_count:\n",
    "            classes_count[emotion] += 1\n",
    "        else:\n",
    "            classes_count[emotion] = 1\n",
    "\n",
    "### then we sort the dictionary by occurences\n",
    "emotions_dict = {\n",
    "    k: v\n",
    "    for k, v in sorted(classes_count.items(), key=lambda item: item[1], reverse=True)\n",
    "}\n",
    "print(\"Classes values:\")\n",
    "print(emotions_dict)\n",
    "\n",
    "### Classes counts are not balanced: the use of weights is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop not useful column\n",
    "\n",
    "raw_df.drop(columns=[SPEAKERS], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove Nones from the triggers\n",
    "\n",
    "raw_df[TRIGGERS] = raw_df[TRIGGERS].apply(\n",
    "    lambda trig_seq: np.array([0.0 if t is None else t for t in trig_seq])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change column \"episode\" from utterance_xyz to episode_xyz\n",
    "for i in range(len(raw_df)):\n",
    "    raw_df[EPISODE][i] = f\"episode_{raw_df[EPISODE][i][10:]}\"\n",
    "\n",
    "clean_df = raw_df\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If an episode contains the same utterances of the previous and a few more then the triggers from the previous episode are replicated in the current episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{raw_df[TRIGGERS][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replicate triggers\n",
    "\n",
    "count = 0\n",
    "for i in range(1, len(clean_df)):\n",
    "    is_continuation = np.all(\n",
    "        [u in clean_df[UTTERANCES][i] for u in clean_df[UTTERANCES][i - 1]]\n",
    "    )\n",
    "    if is_continuation:\n",
    "        count += 1\n",
    "        for k, t in enumerate(clean_df[TRIGGERS][i - 1]):\n",
    "            clean_df[TRIGGERS][i][k] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{raw_df[TRIGGERS][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Val Test split: 80/10/10\n",
    "\n",
    "\n",
    "def split_data(\n",
    "    df: pd.DataFrame, seed: int = 42\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, test_size=0.2, train_size=0.8, random_state=seed\n",
    "    )\n",
    "\n",
    "    df_val, df_test = train_test_split(\n",
    "        df_test, test_size=0.5, train_size=0.5, random_state=seed\n",
    "    )\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check\n",
    "df_train_t, df_val_t, df_test_t = split_data(clean_df)\n",
    "print(f\"df_train len: {len(df_train_t)}\")\n",
    "print(f\"df_val len: {len(df_val_t)}\")\n",
    "print(f\"df_test len: {len(df_test_t)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explode the dataframe: <br>\n",
    "each rows contains: previous utterance, target utterance, next utterance (for context). <br>\n",
    "Except for the first and lat utterance of each episode that have no previous and next utterance, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_add_context_old(df: pd.DataFrame, ctxt_win_len: int = 1) -> pd.DataFrame:\n",
    "    # TODO ok che context_window sia il numero di utt future == numero utt passate invece che la len di tutta la window?\n",
    "\n",
    "    ### Flatten the lists of utterances,triggers,emotions into new rows of the dataframe\n",
    "    exploded_df = df.explode([UTTERANCES, TRIGGERS, EMOTIONS], ignore_index=True)\n",
    "    exploded_df.rename(columns={UTTERANCES: \"current\"}, inplace=True)\n",
    "    exploded_df.head(10)\n",
    "\n",
    "    ### Pair shifted columns of utterances to the exploded df to make previous and next\n",
    "    for i in range(1, ctxt_win_len + 1):\n",
    "        padding_cells = pd.Series([\" \" for _ in range(i)])\n",
    "\n",
    "        previous_col = pd.concat(\n",
    "            (padding_cells, exploded_df[\"current\"][:-i]), copy=False\n",
    "        ).to_list()\n",
    "        exploded_df.insert(loc=2, column=f\"previous_-{i}\", value=previous_col)\n",
    "\n",
    "        next_col = pd.concat(\n",
    "            (exploded_df[\"current\"][i:], padding_cells), copy=False\n",
    "        ).to_list()\n",
    "        exploded_df.insert(loc=2 + 2 * i, column=f\"next_{i}\", value=next_col)\n",
    "\n",
    "    ### Remove the previous of the first utterance and the next of the last utterance of each episode\n",
    "    for i in range(1, len(exploded_df) - 1):\n",
    "        for j in range(1, ctxt_win_len + 1):\n",
    "            if exploded_df[EPISODE][i] != exploded_df[EPISODE][i - 1]:\n",
    "                exploded_df[f\"next_{j}\"][i - j] = \" \"\n",
    "                exploded_df[f\"previous_-{j}\"][i] = \" \"\n",
    "\n",
    "    exploded_df.sort_values(by=EPISODE, inplace=True)\n",
    "    # TODO sortare prima su episode poi sull'indice\n",
    "    return exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emotions = df_train_t[EMOTIONS].explode().to_numpy()\n",
    "uniq_emotions = np.sort(np.unique(all_emotions))\n",
    "# uniq_emotions = [\"neutral\", \"joy\", \"surprise\", \"anger\", \"sadness\", \"disgust\", \"fear\"]\n",
    "one_hot = np.identity(len(uniq_emotions))\n",
    "emotion_mapping = {e: one_hot[i] for i, e in enumerate(uniq_emotions)}\n",
    "print(emotion_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove\n",
    "\" separator \".join([\"A\", \"B\", \"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_add_context(df: pd.DataFrame, ctxt_win_len: int = 1) -> pd.DataFrame:\n",
    "\n",
    "    new_df = {\"emotion\": [], \"trigger\": [], \"hist_curr\": [], \"next\": [], \"episode\": []}\n",
    "\n",
    "    utt_separator = \" \"  # TODO va bene ' ' come separatore?\n",
    "    ### for each original row\n",
    "    for _, row in df.iterrows():\n",
    "        ### create as many new rows as utterances in the original row\n",
    "        for utt_idx, (emo, trig) in enumerate(zip(row[EMOTIONS], row[TRIGGERS])):\n",
    "            # new_df[\"emotion\"].append(emotion_mapping[emo]) # TODO not possible here: baselines do not like it\n",
    "            new_df[\"emotion\"].append(emo)\n",
    "            new_df[\"trigger\"].append(trig)\n",
    "            new_df[\"episode\"].append(row[\"episode\"])\n",
    "\n",
    "            ### previous utterances + current (same way triggers are defined)\n",
    "            start_hist = np.max((0, utt_idx - ctxt_win_len))\n",
    "            curr_incl = utt_idx + 1\n",
    "            hist_curr = utt_separator.join(row[UTTERANCES][start_hist:curr_incl])\n",
    "            new_df[\"hist_curr\"].append(hist_curr)\n",
    "\n",
    "            ### next utterances\n",
    "            curr_excl = utt_idx + 1\n",
    "            last_next_excl = utt_idx + ctxt_win_len + 1\n",
    "            next = utt_separator.join(row[UTTERANCES][curr_excl:last_next_excl])\n",
    "            new_df[\"next\"].append(next)\n",
    "\n",
    "    new_df = pd.DataFrame(new_df)\n",
    "    ### important: the new_df still has utterances grouped by episode (to make the metrics work), otherwise we sort\n",
    "    new_df.sort_values(by=EPISODE, inplace=True)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = explode_add_context(df_train_t, 2)\n",
    "df_val = explode_add_context(df_val_t, 2)\n",
    "df_test = explode_add_context(df_test_t, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, a in df_train.iterrows():\n",
    "    if idx < 10:\n",
    "        print(f\"{a['hist_curr']}  |||  {a['next']}\\n\")\n",
    "    else:\n",
    "        # TODO qualcuno mi spieghi perchè con il break stampa solo la prima volta: iterrows non si rigenera?!\n",
    "        pass\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = explode_add_context(df_train_t, 2)\n",
    "dff.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = explode_add_context(df_train_t, 3)\n",
    "dff.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = explode_add_context(df_train_t, 5)\n",
    "dff.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute F1 score for each flattened dialogue and return avg over dialogues\n",
    "def sequence_f1(\n",
    "    y_true: pd.DataFrame,\n",
    "    y_pred: np.ndarray,\n",
    "    target_column: str,\n",
    "    avg: bool = True,\n",
    ") -> Union[dict, float]:\n",
    "    assert len(y_pred) == len(y_true), \"y_pred and y_true must be of the same lenght\"\n",
    "    assert (\n",
    "        y_true[EPISODE].is_monotonic_increasing\n",
    "        or y_true[EPISODE].is_monotonic_decreasing\n",
    "    ), \"utterances must be sorted over the episodes\"\n",
    "\n",
    "    res = {}\n",
    "    start = 0\n",
    "    stop_incl = 0\n",
    "    for i in range(1, len(y_pred)):\n",
    "        if y_true[EPISODE][i - 1] != y_true[EPISODE][i]:\n",
    "            stop_incl = i - 1\n",
    "            f1 = f1_score(\n",
    "                y_true=y_true[target_column][start : stop_incl + 1].to_list(),\n",
    "                y_pred=y_pred[start : stop_incl + 1],\n",
    "                average=\"micro\",\n",
    "            )\n",
    "            res.update({y_true[EPISODE][start]: f1})\n",
    "            start = i\n",
    "\n",
    "    # np.std(list(res.values))\n",
    "    return res if not avg else np.average(list(res.values()))\n",
    "\n",
    "\n",
    "### Compute F1 score for the unrolled sequence\n",
    "def unrolled_f1(\n",
    "    y_true: pd.DataFrame,\n",
    "    y_pred: np.ndarray,\n",
    "    target_column: str,\n",
    ") -> float:\n",
    "    return f1_score(y_true[target_column].to_list(), y_pred, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create baseline models\n",
    "\n",
    "\n",
    "# TODO do we still need the class?\n",
    "class SequenceDummyClassifier(DummyClassifier):\n",
    "    def __init__(self, strategy: str, seed: int = 42) -> None:\n",
    "        self.seed = seed\n",
    "        if not strategy.lower() in (\"random\", \"majority\"):\n",
    "            raise ValueError(\"strategy must be in [random, majority]\")\n",
    "        sklearn_strategy = \"uniform\" if strategy == \"random\" else \"most_frequent\"\n",
    "        super().__init__(strategy=sklearn_strategy, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_baseline(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    seed: int = 42,\n",
    ") -> dict:\n",
    "\n",
    "    baseline_f1s = {}\n",
    "    baseline_results = {}\n",
    "\n",
    "    for strategy in (\"Random\", \"Majority\"):\n",
    "        for target in (\"emotion\", \"trigger\"):\n",
    "            clf = SequenceDummyClassifier(strategy=strategy, seed=seed)\n",
    "            clf.fit(X=df_train[\"hist_curr\"], y=df_train[target])\n",
    "\n",
    "            res = clf.predict(X=df_test[\"hist_curr\"])\n",
    "            baseline_results.update({f\"{target}_{strategy}\": res})\n",
    "\n",
    "            seq_f1 = sequence_f1(y_true=df_test, y_pred=res, target_column=target)\n",
    "            baseline_f1s.update({f\"sequence_f1({target}_{strategy})\": seq_f1})\n",
    "\n",
    "            unr_f1 = unrolled_f1(y_true=df_test, y_pred=res, target_column=target)\n",
    "            baseline_f1s.update({f\"unrolled_f1({target}_{strategy})\": unr_f1})\n",
    "\n",
    "    return baseline_f1s, baseline_results\n",
    "\n",
    "\n",
    "f1s, results = experiment_baseline(df_train, df_test)\n",
    "for k, v in f1s.items():\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_pandas(df_train)\n",
    "ds_val = Dataset.from_pandas(df_val)\n",
    "ds_test = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove\n",
    "def encode(tokenizer, df, labels_emo, labels_tri):\n",
    "    \"\"\"\n",
    "    Encodes the dataset according to the BERT specifications.\n",
    "    The method encodes the data in the following way:\n",
    "      [CLS] current [SEP] future [SEP]\n",
    "      where -current- is the concatenation of history+utterance\n",
    "      and -future- is the sentence following the utterance\n",
    "    \"\"\"\n",
    "    # concatenate the history and the utterance\n",
    "    his_cur = df[\"history\"].str.cat(df[\"utterance\"])\n",
    "    # encoding version without future feature\n",
    "    \"\"\"encodings = tokenizer(list(df['history']),\n",
    "                        list(df['utterance']),\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          max_length = 512,\n",
    "                          )\"\"\"\n",
    "    encodings = tokenizer(\n",
    "        list(his_cur),\n",
    "        list(df[\"future\"]),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    input_ids, input_attention_mask = (\n",
    "        encodings[\"input_ids\"],\n",
    "        encodings[\"attention_mask\"],\n",
    "    )\n",
    "    print(f\"input shape: {np.shape(input_ids)}\")\n",
    "    encodings.update({\"labels_emo\": torch.LongTensor(labels_emo.values)})\n",
    "    labels_tri = np.expand_dims(labels_tri.values, axis=1)\n",
    "    encodings.update({\"labels_tri\": torch.FloatTensor(labels_tri)})\n",
    "    encodings.pop(\"token_type_ids\")\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(ds_row, tokenizer):\n",
    "    if type(ds_row[\"emotion\"]) != str:  ### batchsize > 1\n",
    "        emotion_encoding = []\n",
    "        emotion_encoding = [emotion_mapping[e] for e in ds_row[\"emotion\"]]\n",
    "    else:  ### batchsize == 1\n",
    "        emotion_encoding = emotion_mapping[ds_row[\"emotion\"]]\n",
    "\n",
    "    encoded_ds_row = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"emotions\": emotion_encoding,\n",
    "        \"triggers\": ds_row[\n",
    "            \"trigger\"\n",
    "        ],  # TODO non è necessario metterlo in un tensore, ci pensa ds.set_format(type='torch')\n",
    "    }\n",
    "\n",
    "    tokenized_context = tokenizer(\n",
    "        ds_row[\"hist_curr\"],\n",
    "        ds_row[\"next\"] if ds_row[\"next\"] != [] else None,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length // 2,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    encoded_ds_row[\"input_ids\"] = tokenized_context[\"input_ids\"]\n",
    "    encoded_ds_row[\"token_type_ids\"] = tokenized_context[\"token_type_ids\"]\n",
    "\n",
    "    encoded_ds_row[\"attention_mask\"] = tokenized_context[\"attention_mask\"]\n",
    "\n",
    "    return encoded_ds_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST\n",
    "ds_short = Dataset.from_pandas(df_train.iloc[0:4])\n",
    "ds_short_tokenized = ds_short.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=False,\n",
    "    remove_columns=[\"hist_curr\", \"next\"],\n",
    ")\n",
    "ds_short_tokenized_batched = ds_short.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=True,\n",
    "    remove_columns=[\"hist_curr\", \"next\"],\n",
    ")\n",
    "\n",
    "idx = 3\n",
    "\n",
    "ds_short_tokenized.set_format(type=\"torch\")\n",
    "ds_short_tokenized_batched.set_format(type=\"torch\")\n",
    "\n",
    "ids = ds_short_tokenized[\"input_ids\"][idx]\n",
    "ids_batch = ds_short_tokenized_batched[\"input_ids\"][idx]\n",
    "print(ds_short_tokenized[\"emotion\"][idx])\n",
    "print(ids)\n",
    "print(len(ids))\n",
    "print(len(ids[0]))\n",
    "print(ds_short_tokenized_batched[\"emotion\"][idx])\n",
    "print(ids_batch)\n",
    "print(len(ids_batch))\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids[0])\n",
    "tokens_batched = tokenizer.convert_ids_to_tokens(ids_batch)\n",
    "original_string = tokenizer.convert_tokens_to_string(tokens)\n",
    "original_string_batched = tokenizer.convert_tokens_to_string(tokens_batched)\n",
    "print(original_string)\n",
    "print(original_string_batched)\n",
    "print(ds_short[idx])\n",
    "\n",
    "# OSSERVAZIONI:\n",
    "# ds.set_format(type='torch') mette trigger ed emotion dentro a dei tensori e fa qualcosa a input_ids per cui viene stampato come matrice e non come riga\n",
    "# a prescindere da set_format:  input_ids è una [[]] se tokeniziamo senza batch, è una [] se tokenizziamo con il batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove\n",
    "def encode_emotion(ds_row, mapping):\n",
    "    if type(ds_row[\"emotion\"]) != str:  ### batchsize > 1\n",
    "        emotion_encoding = []\n",
    "        emotion_encoding = [mapping[e] for e in ds_row[\"emotion\"]]\n",
    "    else:  ### batchsize == 1\n",
    "        emotion_encoding = mapping[ds_row[\"emotion\"]]\n",
    "    ds_row[\"emotion\"] = emotion_encoding\n",
    "    return ds_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply tokenization\n",
    "\n",
    "batched = True\n",
    "cols_to_drop = [\"hist_curr\", \"next\"]\n",
    "\n",
    "# TODO remove\n",
    "# ds_train = ds_train.map(\n",
    "#    function=encode_emotion,\n",
    "#    fn_kwargs={\"mapping\": emotion_mapping},\n",
    "#    batched=batched,\n",
    "# )\n",
    "ds_train_tokenized = ds_train.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=batched,\n",
    "    remove_columns=cols_to_drop,\n",
    ")\n",
    "ds_train_tokenized.set_format(type=\"torch\")\n",
    "\n",
    "ds_test_tokenized = ds_test.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=batched,\n",
    "    remove_columns=cols_to_drop,\n",
    ")\n",
    "ds_test_tokenized.set_format(type=\"torch\")\n",
    "\n",
    "ds_val_tokenized = ds_val.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=batched,\n",
    "    remove_columns=cols_to_drop,\n",
    ")\n",
    "ds_val_tokenized.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization test\n",
    "tokens = tokenizer.convert_ids_to_tokens(ds_train_tokenized[\"input_ids\"][0])\n",
    "string = tokenizer.convert_tokens_to_string(tokens)\n",
    "original_string = ds_train[\"hist_curr\"][0] + ds_train[\"next\"][0]\n",
    "print(original_string)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class weighting algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(df: pd.Series) -> torch.tensor:\n",
    "    labels, n_ones = np.unique(df.to_numpy, return_counts=True)\n",
    "    l = len(df)\n",
    "    n_zeroes = np.array([l - n for n in n_ones])\n",
    "\n",
    "    weights = np.empty_like(n_ones)\n",
    "    for class_num, (ones, zeroes) in enumerate(zip(n_ones, n_zeroes)):\n",
    "        # TODO choose one\n",
    "        weights[class_num] = zeroes / (ones + 1e-4)\n",
    "        # weights[class_num] = np.sqrt(zeroes / (ones + 1e-4))\n",
    "\n",
    "    print(f\"weigts = {weights}\")\n",
    "    return torch.as_tensor(weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, num_emotions=7):\n",
    "        super(BERTClass, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        # self.lstm = torch.nn.LSTM()\n",
    "        # classifiers\n",
    "        self.l_emotions = torch.nn.Linear(self.bert.config.hidden_size, num_emotions)\n",
    "        self.l_triggers = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        output = output.pooler_output\n",
    "        output_emotions = self.dropout(output)\n",
    "        output_triggers = self.dropout(output)\n",
    "\n",
    "        # output_triggers = self.lstm(output_triggers)\n",
    "\n",
    "        output_emotions = self.l_emotions(output_emotions)\n",
    "        output_triggers = torch.squeeze(self.l_triggers(output_triggers))\n",
    "\n",
    "        return output_emotions, output_triggers\n",
    "\n",
    "    def freeze_params(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_emotions = len(uniq_emotions)\n",
    "\n",
    "model_frozen = BERTClass(num_emotions)\n",
    "model_full = BERTClass(num_emotions)\n",
    "\n",
    "model_frozen.to(device)\n",
    "model_full.to(device)\n",
    "model_frozen.freeze_params()\n",
    "\n",
    "# Verifying that the params are actually frozen\n",
    "# for name, param in model_frozen.named_parameters():\n",
    "#     print(name, param.requires_grad)\n",
    "#\n",
    "# for name, param in model_full.named_parameters():\n",
    "#     print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [model_full, model_frozen]\n",
    "num_epochs = 5\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "for model in model_list:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    # Tokenizer initiation\n",
    "    # TODO Check tokenizer parameters\n",
    "    # TODO A Dataloader seems to be commonly used for this use cases\n",
    "    # encoding = tokenizer(df_train, truncation=False, padding='max_length', return_tensors='pt')\n",
    "    # input_ids = encoding['input_ids']\n",
    "    # attention_mask = encoding['attention_mask']\n",
    "\n",
    "    # Training loop\n",
    "    # for epoch in range(num_epochs):\n",
    "    #\n",
    "    #    for idx in range(len(df_test)):\n",
    "\n",
    "    #        text = df_train[idx].drop(TRIGGERS)\n",
    "    #        label = df_train[idx][TRIGGERS]\n",
    "\n",
    "    #        optimizer.zero_grad()\n",
    "\n",
    "    #        logits = model(batch_data)\n",
    "    #        loss = loss_fn(logits, batch_labels)\n",
    "    #        loss.backward()\n",
    "    #        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs_emotions, outputs_triggers, emotions_labels, triggers_labels):\n",
    "    L1 = torch.nn.CrossEntropyLoss()\n",
    "    L2 = torch.nn.BCEWithLogitsLoss()\n",
    "    # print(f\"output_triggers shape = {outputs_triggers.shape}\")\n",
    "    # print(f\"triggers labels shape {triggers_labels.shape}\")\n",
    "    return L1(outputs_emotions, emotions_labels) + L2(outputs_triggers, triggers_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training of the model\n",
    "def train_model(train_dl, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions_emotions = 0\n",
    "    correct_predictions_triggers = 0\n",
    "    num_samples_emotions = 0\n",
    "    num_samples_triggers = 0\n",
    "\n",
    "    ### activate dropout, batch norm\n",
    "    model.train()\n",
    "\n",
    "    ### initialize progress bar\n",
    "    batches = tq.tqdm(\n",
    "        enumerate(train_dl), total=len(train_dl), leave=True, colour=\"steelblue\"\n",
    "    )\n",
    "\n",
    "    for batch_idx, data in batches:\n",
    "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "        emotions_labels = data[\"emotions\"].to(device, dtype=torch.float)\n",
    "        triggers_labels = data[\"triggers\"].to(device, dtype=torch.float)\n",
    "\n",
    "        ### Forward pass\n",
    "        outputs_emotions, outputs_triggers = model(ids, mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(\n",
    "            outputs_emotions, outputs_triggers, emotions_labels, triggers_labels\n",
    "        )\n",
    "        losses.append(loss.cpu().detach().numpy().item())\n",
    "\n",
    "        ### apply thresh 0.5\n",
    "        outputs_emotions = (\n",
    "            torch.sigmoid(outputs_emotions).cpu().detach().numpy().round()\n",
    "        )\n",
    "        outputs_triggers = (\n",
    "            torch.sigmoid(outputs_triggers).cpu().detach().numpy().round()\n",
    "        )\n",
    "\n",
    "        emotions_labels = emotions_labels.cpu().detach().numpy()\n",
    "        triggers_labels = triggers_labels.cpu().detach().numpy()\n",
    "\n",
    "        correct_predictions_emotions += np.sum(outputs_emotions == emotions_labels)\n",
    "        correct_predictions_triggers += np.sum(outputs_triggers == triggers_labels)\n",
    "\n",
    "        num_samples_emotions += emotions_labels.size\n",
    "        num_samples_triggers += triggers_labels.size\n",
    "\n",
    "        ### Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        ### Grad descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        ### Update progress bar\n",
    "        batches.set_description(f\"\")\n",
    "        batches.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "    # Si potrebbe fare una singola accuracy come media delle due, magari fuori dal training\n",
    "    accuracy_emotions = float(correct_predictions_emotions) / num_samples_emotions\n",
    "    accuracy_triggers = float(correct_predictions_triggers) / num_samples_triggers\n",
    "\n",
    "    return model, accuracy_emotions, accuracy_triggers, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(validation_dl, model):\n",
    "    losses = []\n",
    "    correct_predictions_emotions = 0\n",
    "    correct_predictions_triggers = 0\n",
    "    num_samples_emotions = 0\n",
    "    num_samples_triggers = 0\n",
    "    #num_categories = next(iter(validation_dl))[\"labels\"].shape[1]\n",
    "\n",
    "    ### accumulate data over each batch to compute the f1\n",
    "    #true_positives = np.array([0 for _ in range(num_categories)])\n",
    "    #false_positives = np.array([0 for _ in range(num_categories)])\n",
    "    #false_negatives = np.array([0 for _ in range(num_categories)])\n",
    "\n",
    "    ### turn off dropout, fix batch norm\n",
    "    model.eval()\n",
    "\n",
    "    ### show progress bar\n",
    "    batches = tq.tqdm(\n",
    "        enumerate(validation_dl),\n",
    "        total=len(validation_dl),\n",
    "        leave=True,\n",
    "        colour=\"steelblue\",\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in batches:\n",
    "            ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "            mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            emotions_labels = data[\"emotions\"].to(device, dtype=torch.float)\n",
    "            triggers_labels = data[\"triggers\"].to(device, dtype=torch.float)\n",
    "            outputs_emotions, outputs_triggers = model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss = loss_fn(\n",
    "                outputs_emotions, outputs_triggers, emotions_labels, triggers_labels\n",
    "            )\n",
    "            losses.append(loss.cpu().detach().numpy().item())\n",
    "\n",
    "            ### validation accuracy\n",
    "            ### training sigmoid is in BCEWithLogitsLoss\n",
    "            outputs_emotions = (\n",
    "                torch.sigmoid(outputs_emotions).cpu().detach().numpy().round()\n",
    "            )\n",
    "            outputs_triggers = (\n",
    "                torch.sigmoid(outputs_triggers).cpu().detach().numpy().round()\n",
    "            )\n",
    "\n",
    "            emotions_labels = emotions_labels.cpu().detach().numpy()\n",
    "            triggers_labels = triggers_labels.cpu().detach().numpy()\n",
    "            correct_predictions_emotions += np.sum(outputs_emotions == emotions_labels)\n",
    "            correct_predictions_triggers += np.sum(outputs_triggers == triggers_labels)\n",
    "\n",
    "            num_samples_emotions += emotions_labels.size\n",
    "            num_samples_triggers += triggers_labels.size\n",
    "\n",
    "        accuracy_emotions = float(correct_predictions_emotions) / num_samples_emotions\n",
    "        accuracy_triggers = float(correct_predictions_triggers) / num_samples_triggers\n",
    "        # precision = true_positives / (true_positives + false_positives)\n",
    "        # recall = true_positives / (true_positives + false_negatives)\n",
    "        # f1_per_cat = 2 * (precision * recall) / (precision + recall)\n",
    "        # f1_overall = np.mean(f1_per_cat)\n",
    "\n",
    "    return accuracy_emotions, accuracy_triggers, np.mean(losses)  # , f1_overall, f1_per_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(\n",
    "    train_dl,\n",
    "    validation_dl,\n",
    "    model,\n",
    "    optimizer,\n",
    "    n_epochs=1,\n",
    "    save_name=\"0\",\n",
    "    train_model_f=train_model,\n",
    "    eval_model_f=eval_model,\n",
    "):\n",
    "    model_folder = Path.cwd().joinpath(\"models\")\n",
    "    if not model_folder.exists():\n",
    "        model_folder.mkdir(parents=True)\n",
    "\n",
    "    history = {}\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "        model, accuracy_emotions, accuracy_triggers, train_loss = train_model_f(\n",
    "            train_dl, model, optimizer\n",
    "        )\n",
    "        val_accuracy_emotions, val_accuracy_triggers, val_loss = eval_model_f(\n",
    "            validation_dl, model\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"train_loss={np.mean(train_loss):.4f}, val_loss={np.mean(val_loss):.4f}, \"\n",
    "            f\"train_acc_emo={accuracy_emotions:.4f}, train_acc_emo={accuracy_triggers:.4f},\"\n",
    "            f\"val_acc_emo={val_accuracy_emotions:.4f}, val_acc_tri={val_accuracy_triggers:.4f}, \"\n",
    "        )\n",
    "\n",
    "        history.update({\"train_acc_emo\": accuracy_emotions})\n",
    "        history.update({\"train_acc_tri\": accuracy_triggers})\n",
    "        history.update({\"train_losses\": train_loss})\n",
    "        history.update({\"val_acc_emo\": val_accuracy_emotions})\n",
    "        history.update({\"val_acc_emo\": val_accuracy_triggers})\n",
    "        history.update({\"val_losses\": val_loss})\n",
    "        # history.update({\"f1_overall\": f1_overall})\n",
    "        # history.update({\"f1_per_cat\": f1_per_cat})\n",
    "\n",
    "        ### save the best model\n",
    "        # if f1_overall > best_f1:\n",
    "        #   torch.save(\n",
    "        #      model.state_dict(),\n",
    "        #      Path.joinpath(model_folder, f\"model_{save_name}.bin\"),\n",
    "        # )\n",
    "        # best_f1 = f1_overall\n",
    "\n",
    "    return history  # (history[\"f1_overall\"], history[\"f1_per_cat\"], history[\"train_losses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(tokenized_datasets, batch_size):\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"train\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    validation_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"validation\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    test_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"test\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return train_dl, validation_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    ds_train_tokenized,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "validation_dl = torch.utils.data.DataLoader(\n",
    "    ds_val_tokenized,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Custom loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_eval(\n",
    "    train_dl,\n",
    "    validation_dl,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    save_name=f\"test_model\",\n",
    ")\n",
    "\n",
    "# Reshape the target tensor to have the same size as the input tensor\n",
    "target_size = history[\"train_losses\"].size()\n",
    "input_size = history[\"train_acc_emo\"].size()\n",
    "if target_size != input_size:\n",
    "    history[\"train_losses\"] = history[\"train_losses\"].view(*input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor [[utt + pad], [utt + pad], [utt + pad]]\n",
    "tensor [CLS + utt + SEP + utt + SEP + utt + SEP + pad]\n",
    "[ tensor[cls utt padd] tensor [cls utt padd]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training con Classe Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainer(Trainer):\n",
    "    def __init__(self, model, training_args, train_ds, eval_ds, metrics):\n",
    "        super().__init__(model, training_args, train_ds, eval_ds, metrics)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        emotions_labels = inputs[\"emotions\"]\n",
    "        triggers_labels = inputs[\"triggers\"]\n",
    "        output_emotions, output_triggers = model(ids=ids, mask=mask)\n",
    "\n",
    "        custom_loss = self.loss_fn(\n",
    "            output_emotions, output_triggers, emotions_labels, triggers_labels\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            (custom_loss, output_emotions, output_triggers)\n",
    "            if return_outputs\n",
    "            else custom_loss\n",
    "        )\n",
    "\n",
    "    def loss_fn(outputs_emotions, outputs_triggers, emotions_labels, triggers_labels):\n",
    "        return torch.nn.CrossEntropyLoss(\n",
    "            outputs_emotions, emotions_labels\n",
    "        ) + torch.nn.BCELoss(outputs_triggers, triggers_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"\\\\test\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    # evaluate_during_training=True,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=8,\n",
    "    # seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_full,\n",
    "    args=training_args,\n",
    "    # data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    train_dataset=ds_train_tokenized,\n",
    "    # eval_dataset=ds_val_tokenized,\n",
    "    compute_metrics=sequence_f1,\n",
    "    # tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
