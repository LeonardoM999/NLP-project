{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm.notebook as tq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load corpus\n",
    "\n",
    "data_path = Path(\"data/MELD_train_efr.json\")\n",
    "assert data_path.exists(), \"Data file is not present\"\n",
    "# TODO download from GDrive?\n",
    "df = pd.read_json(data_path, dtype={\"speakers\": np.array, \"triggers\": np.array})\n",
    "EPISODE, SPEAKERS, EMOTIONS, UTTERANCES, TRIGGERS = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Look for how many groups of episodes with the same first utterance there are and their lenghts\n",
    "\n",
    "df.sort_values(by=UTTERANCES, inplace=True)\n",
    "groups = np.zeros((850,), dtype=int)\n",
    "\n",
    "index = 0\n",
    "count = 1\n",
    "for i in range(1, len(df[UTTERANCES])):\n",
    "    if df[UTTERANCES][i][0] == df[UTTERANCES][i - 1][0]:\n",
    "        ### still in the same group\n",
    "        count += 1\n",
    "    else:\n",
    "        ### found new group\n",
    "        groups[index] = count\n",
    "        index += 1\n",
    "        count = 1\n",
    "\n",
    "groups = groups[groups != 0]\n",
    "\n",
    "print(f\"Number of groups: {len(groups)}\")\n",
    "print(f\"Avg group len: {np.average(groups):.1f}\")\n",
    "print(f\"Longest group: {np.max(groups)}\")\n",
    "print(f\"Episodes not in a group: {groups[groups == 1].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count how many speakers there are in each episode\n",
    "\n",
    "speakers_count = df[SPEAKERS].apply(lambda arr: np.unique(arr).shape[0]).to_numpy()\n",
    "min_sp = np.min(speakers_count)\n",
    "max_sp = np.max(speakers_count)\n",
    "print(\"Distribution of number of speakers:\")\n",
    "for count in range(min_sp, max_sp + 1):\n",
    "    print(f\"{count} speakers:  {np.sum(speakers_count == count)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop not useful column\n",
    "\n",
    "df.drop(columns=[EPISODE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove Nones from the triggers\n",
    "\n",
    "# TODO no conversion to int, we may need float laters for label smoothing or so\n",
    "df[TRIGGERS] = df[TRIGGERS].apply(\n",
    "    lambda trig_seq: np.array([0.0 if t is None else t for t in trig_seq])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing:\n",
    "If an episode contains the same utterances of the previous and a few more then the triggers from the previous episode are replicated in the current episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{df[TRIGGERS][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(1, len(df)):\n",
    "    # TODO discuss: the version that does all the checks is faster\n",
    "    is_continuation = np.all([u in df[UTTERANCES][i] for u in df[UTTERANCES][i - 1]])\n",
    "    # is_continuation = True\n",
    "    # j = 0\n",
    "    # while is_continuation and j < len(df[UTTERANCES][i - 1]):\n",
    "    #     is_continuation = df[UTTERANCES][i - 1][j] in df[UTTERANCES][i]\n",
    "    #     j += 1\n",
    "    if is_continuation:\n",
    "        count += 1\n",
    "        for k, t in enumerate(df[TRIGGERS][i - 1]):\n",
    "            df[TRIGGERS][i][k] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{df[TRIGGERS][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Validation/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Val Test split: 80/10/10\n",
    "\n",
    "\n",
    "def split_data(df: pd.DataFrame, seed: int = 42):\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, test_size=0.2, train_size=0.8, random_state=seed\n",
    "    )\n",
    "\n",
    "    df_val, df_test = train_test_split(\n",
    "        df_test, test_size=0.5, train_size=0.5, random_state=seed\n",
    "    )\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check\n",
    "df_train, df_val, df_test = split_data(df)\n",
    "print(f\"df_train len: {len(df_train)}\")\n",
    "print(f\"df_val len: {len(df_val)}\")\n",
    "print(f\"df_test len: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class imbalance check\n",
    "classes_count= {}\n",
    "for emotions in df_train[\"emotions\"]:\n",
    "    for emotion in emotions:\n",
    "        if emotion in classes_count:\n",
    "            classes_count[emotion] += 1\n",
    "        else:\n",
    "            classes_count[emotion] = 1\n",
    "\n",
    "# then we sort the dictionary by occurences\n",
    "emotions_dict = {k: v for k, v in sorted(classes_count.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(\"Classes values:\")\n",
    "print(emotions_dict)\n",
    "\n",
    "#Class imbalance abbastanza alto, potremmo usare dei weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: give 2 series of sequences of triggers/emotions compute F1 inside each sequence and return avg, flatten out and compute F1\n",
    "###\n",
    "def sequence_f1(y_true, y_pred, avg: bool = True):\n",
    "    res = [\n",
    "        f1_score(y_true=y_t, y_pred=y_p, average=\"micro\")\n",
    "        for y_t, y_p in zip(y_true, y_pred)\n",
    "    ]\n",
    "    return np.average(res) if avg else res\n",
    "\n",
    "\n",
    "def unrolled_f1(y_true, y_pred):\n",
    "    y_t_flat = []\n",
    "    for l in y_true:\n",
    "        for e in l:\n",
    "            y_t_flat.append(e)\n",
    "\n",
    "    y_p_flat = []\n",
    "    for l in y_pred:\n",
    "        for e in l:\n",
    "            y_p_flat.append(e)\n",
    "\n",
    "    return f1_score(y_true=y_t_flat, y_pred=y_p_flat, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create baseline models\n",
    "\n",
    "\n",
    "class SequenceDummyClassifier(DummyClassifier):\n",
    "    def __init__(self, strategy: str, seed: int = 42) -> None:\n",
    "        self.seed = seed\n",
    "        # TODO proper exception\n",
    "        if not strategy.lower() in (\"random\", \"majority\"):\n",
    "            raise ValueError(\"strategy must be in [random, majority]\")\n",
    "        sklearn_strategy = \"uniform\" if strategy == \"random\" else \"most_frequent\"\n",
    "        super().__init__(strategy=sklearn_strategy, random_state=seed)\n",
    "\n",
    "    ### TODO discuss: problem = flattening sequences of != len from the df\n",
    "    ### sol1 = iterate over df and collect 1by1: bad for memory allocation\n",
    "    ### sol2 = pad the sequences in the df, create array, remove padding: can it be more efficient?\n",
    "    # np.array(df[UTTERANCES].tolist()).flatten() does not work because of the != len of the sequences\n",
    "\n",
    "    def _flatten_seq(self, df: pd.Series):\n",
    "        res = []\n",
    "        for l in df:\n",
    "            for e in l:\n",
    "                res.append(e)\n",
    "        return res\n",
    "\n",
    "    def _flatten_seq_(self, df: pd.Series):\n",
    "        max_len = np.max(df.apply(lambda s: len(s)).to_numpy())\n",
    "        dtype = type(df[0][0])\n",
    "        pad_element = dtype(999999)\n",
    "        ### Pad utterances with 0, flatten array\n",
    "        df = np.array(\n",
    "            df.apply(\n",
    "                lambda s: np.hstack(\n",
    "                    (\n",
    "                        s,\n",
    "                        np.repeat(\n",
    "                            [pad_element],\n",
    "                            repeats=(max_len - len(s)),\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "            ).to_list()\n",
    "        ).flatten()\n",
    "        ### Remove padding\n",
    "        return (\n",
    "            df[np.char.not_equal(df, pad_element)]\n",
    "            if dtype == str\n",
    "            else df[df != dtype(pad_element)]\n",
    "        )\n",
    "\n",
    "    def _deflatten_seq(self, seq, shape_like: pd.Series):\n",
    "        ### TODO discuss: we may think to use np.reshape but again the row len is not homogeneous!\n",
    "        data = iter(seq)\n",
    "        result = [[next(data) for _ in s] for s in shape_like]\n",
    "        return result\n",
    "\n",
    "    def fit(self, X: pd.Series, y: pd.Series):\n",
    "        X_flat = self._flatten_seq(X)\n",
    "        y_flat = self._flatten_seq(y)\n",
    "        super().fit(X=X_flat, y=y_flat)\n",
    "\n",
    "    def predict(self, X: pd.Series, return_flat: bool = False):\n",
    "        X_flat = self._flatten_seq(X)\n",
    "        y_flat = super().predict(X_flat)\n",
    "        return y_flat if return_flat else self._deflatten_seq(seq=y_flat, shape_like=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_baseline(df_train: pd.DataFrame, df_test: pd.DataFrame, seed: int = 42):\n",
    "\n",
    "    baseline_f1s = {}\n",
    "    baseline_results = {}\n",
    "\n",
    "    for strategy in (\"Random\", \"Majority\"):\n",
    "        for target in (EMOTIONS, TRIGGERS):\n",
    "            clf = SequenceDummyClassifier(strategy=strategy, seed=seed)\n",
    "            clf.fit(X=df_train[UTTERANCES], y=df_train[target])\n",
    "\n",
    "            res = clf.predict(X=df_test[UTTERANCES], return_flat=False)\n",
    "            baseline_results.update({f\"{target}_{strategy}\": res})\n",
    "\n",
    "            seq_f1 = sequence_f1(y_true=df_test[target], y_pred=res)\n",
    "            baseline_f1s.update({f\"sequence_f1({target}_{strategy})\": seq_f1})\n",
    "\n",
    "            unr_f1 = unrolled_f1(y_true=df_test[target], y_pred=res)\n",
    "            baseline_f1s.update({f\"unrolled_f1({target}_{strategy})\": unr_f1})\n",
    "\n",
    "    return baseline_f1s, baseline_results\n",
    "\n",
    "\n",
    "f1s, results = experiment_baseline(df_train, df_test)\n",
    "for k, v in f1s.items():\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(utterances, tokenizer):\n",
    "\n",
    "    text_tokens = []\n",
    "\n",
    "    for sentence in utterances:\n",
    "        tokenized_sentence = tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        encoded_ds_row = {\n",
    "        \"input_ids\": tokenized_sentence[\"input_ids\"],\n",
    "        \"token_type_ids\": tokenized_sentence[\"token_type_ids\"], # https://huggingface.co/transformers/v3.2.0/glossary.html#token-type-ids\n",
    "        \"attention_mask\": tokenized_sentence[\"attention_mask\"],\n",
    "        }\n",
    "        #Vanno aggiunte le labels (?)\n",
    "        # encoded_ds_row[\"labels\"] = add_labels(sentence,emotions,triggers)\n",
    "        \n",
    "        text_tokens.append(encoded_ds_row)\n",
    "\n",
    "    return encoded_ds_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test della funzione\n",
    "tokens = tokenize(df_train[\"utterances\"][0], tokenizer)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_emotions):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, num_emotions)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "    def freeze_params(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "\n",
    "        self.bert= BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.lstm = torch.nn.LSTM()\n",
    "        #classifiers\n",
    "        self.l_emotions = torch.nn.Linear(self.bert.config.hidden_size, num_emotions=7) \n",
    "        self.l_triggers = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        output= self.bert(ids, attention_mask = mask)\n",
    "\n",
    "        output_emotions = self.dropout(output)\n",
    "        output_triggers = self.dropout(output)\n",
    "\n",
    "        output_triggers = self.lstm(output_triggers)\n",
    "\n",
    "        output_emotions = self.l_emotions(output_emotions)\n",
    "        output_triggers = self.l_triggers(output_triggers)\n",
    "        \n",
    "        return output_emotions, output_triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Emotions and triggers tuning of class definiton\n",
    "model_frozen = BERTClassifier(3)\n",
    "model_full = BERTClassifier(3)\n",
    "\n",
    "model_frozen.freeze_params()\n",
    "\n",
    "#Verifying that the params are actually frozen\n",
    "for name, param in model_frozen.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "\n",
    "for name, param in model_full.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [model_full, model_frozen]\n",
    "num_epochs = 5\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "for model in model_list:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    #Tokenizer initiation\n",
    "    # TODO Check tokenizer parameters\n",
    "    # TODO A Dataloader seems to be commonly used for this use cases\n",
    "    #encoding = tokenizer(df_train, truncation=False, padding='max_length', return_tensors='pt')\n",
    "    #input_ids = encoding['input_ids']\n",
    "    #attention_mask = encoding['attention_mask']\n",
    "\n",
    "    #Training loop\n",
    "    #for epoch in range(num_epochs):\n",
    "    #    \n",
    "    #    for idx in range(len(df_test)):\n",
    "\n",
    "    #        text = df_train[idx].drop(TRIGGERS)\n",
    "    #        label = df_train[idx][TRIGGERS]\n",
    "\n",
    "    #        optimizer.zero_grad() \n",
    "\n",
    "    #        logits = model(batch_data)\n",
    "    #        loss = loss_fn(logits, batch_labels)\n",
    "    #        loss.backward()\n",
    "    #        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs_emotions, outputs_triggers, emotions_labels, triggers_labels):\n",
    "    return torch.nn.CrossEntropyLoss(outputs_emotions, emotions_labels) + torch.nn.BCELoss(outputs_triggers, triggers_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training of the model\n",
    "def train_model(train_dl, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions_emotions = 0\n",
    "    correct_predictions_triggers = 0\n",
    "    num_samples_emotions = 0\n",
    "    num_samples_triggers = 0\n",
    "\n",
    "    ### activate dropout, batch norm\n",
    "    model.train()\n",
    "\n",
    "    ### initialize progress bar\n",
    "    batches = tq.tqdm(\n",
    "        enumerate(train_dl), total=len(train_dl), leave=True, colour=\"steelblue\"\n",
    "    )\n",
    "\n",
    "    for batch_idx, data in batches:\n",
    "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        emotions_labels = data[\"emotions\"].to(device, dtype=torch.float)\n",
    "        triggers_labels = data[\"triggers\"].to(device, dtype=torch.float)\n",
    "        outputs_emotions,outputs_triggers = model(ids, mask)  ### Forward\n",
    "\n",
    "        loss = loss_fn(outputs_emotions, outputs_triggers, emotions_labels, triggers_labels)\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        ### apply thresh 0.5\n",
    "        outputs_emotions = torch.sigmoid(outputs_emotions).cpu().detach().numpy().round()\n",
    "        outputs_triggers = torch.sigmoid(outputs_triggers).cpu().detach().numpy().round()\n",
    "\n",
    "        emotions_labels = emotions_labels.cpu().detach().numpy()\n",
    "        triggers_labels = triggers_labels.cpu().detach().numpy()\n",
    "\n",
    "        correct_predictions_emotions += np.sum(outputs_emotions == emotions_labels)\n",
    "        correct_predictions_triggers += np.sum(outputs_triggers == triggers_labels)\n",
    "\n",
    "        num_samples_emotions += emotions_labels.size\n",
    "        num_samples_triggers += triggers_labels.size\n",
    "\n",
    "\n",
    "        ### Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        ### Grad descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        ### Update progress bar\n",
    "        batches.set_description(f\"\")\n",
    "        batches.set_postfix(batch_loss=loss)\n",
    "\n",
    "    #Si potrebbe fare una singola accuracy come media delle due, magari fuori dal training\n",
    "    accuracy_emotions = float(correct_predictions_emotions) / num_samples_emotions\n",
    "    accuracy_triggers = float(correct_predictions_triggers) / num_samples_triggers\n",
    "\n",
    "\n",
    "    return model, accuracy_emotions, accuracy_triggers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval model, setup e train_eval da definire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(tokenized_datasets, batch_size):\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"train\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    validation_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"validation\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    test_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"test\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return train_dl, validation_dl, test_dl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
