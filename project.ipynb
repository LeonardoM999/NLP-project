{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    # DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "import tqdm.notebook as tq\n",
    "from datasets import Dataset\n",
    "from typing import Tuple, Union\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load corpus\n",
    "\n",
    "data_path = Path(\"data/MELD_train_efr.json\")\n",
    "assert data_path.exists(), \"Data file is not present\"\n",
    "raw_df = pd.read_json(\n",
    "    data_path, dtype={\"speakers\": np.array}\n",
    ")  # , \"triggers\": np.array})\n",
    "EPISODE, SPEAKERS, EMOTIONS, UTTERANCES, TRIGGERS = raw_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>speakers</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>utterance_0</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise]</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>utterance_1</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>utterance_2</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>utterance_3</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utterance_4</td>\n",
       "      <td>[Joey, Rachel, Joey, Rachel]</td>\n",
       "      <td>[surprise, sadness, surprise, fear]</td>\n",
       "      <td>[But then who? The waitress I went out with la...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       episode                                           speakers  \\\n",
       "0  utterance_0  [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "1  utterance_1  [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "2  utterance_2  [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "3  utterance_3  [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "4  utterance_4                       [Joey, Rachel, Joey, Rachel]   \n",
       "\n",
       "                                            emotions  \\\n",
       "0     [neutral, neutral, neutral, neutral, surprise]   \n",
       "1  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "2  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "3  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "4                [surprise, sadness, surprise, fear]   \n",
       "\n",
       "                                          utterances  \\\n",
       "0  [also I was the point person on my company's t...   \n",
       "1  [also I was the point person on my company's t...   \n",
       "2  [also I was the point person on my company's t...   \n",
       "3  [also I was the point person on my company's t...   \n",
       "4  [But then who? The waitress I went out with la...   \n",
       "\n",
       "                                            triggers  \n",
       "0                          [0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4                               [0.0, 0.0, 1.0, 0.0]  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>speakers</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4000</td>\n",
       "      <td>3350</td>\n",
       "      <td>3427</td>\n",
       "      <td>3998</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>utterance_0</td>\n",
       "      <td>[Monica, Chandler, Monica]</td>\n",
       "      <td>[neutral, neutral, joy]</td>\n",
       "      <td>[Happy?! Is that what I'm supposed to be Vic? ...</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            episode                    speakers                 emotions  \\\n",
       "count          4000                        4000                     4000   \n",
       "unique         4000                        3350                     3427   \n",
       "top     utterance_0  [Monica, Chandler, Monica]  [neutral, neutral, joy]   \n",
       "freq              1                          15                       30   \n",
       "\n",
       "                                               utterances         triggers  \n",
       "count                                                4000             4000  \n",
       "unique                                               3998              523  \n",
       "top     [Happy?! Is that what I'm supposed to be Vic? ...  [0.0, 1.0, 0.0]  \n",
       "freq                                                    2              191  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups: 832\n",
      "Avg group len: 4.8\n",
      "Longest group: 16\n",
      "Episodes not in a group: 128\n"
     ]
    }
   ],
   "source": [
    "### Look for how many groups of episodes with the same first utterance there are and their lenghts\n",
    "\n",
    "raw_df.sort_values(by=UTTERANCES, inplace=True)\n",
    "groups = np.zeros((850,), dtype=int)\n",
    "\n",
    "index = 0\n",
    "count = 1\n",
    "for i in range(1, len(raw_df[UTTERANCES])):\n",
    "    if raw_df[UTTERANCES][i][0] == raw_df[UTTERANCES][i - 1][0]:\n",
    "        ### still in the same group\n",
    "        count += 1\n",
    "    else:\n",
    "        ### found new group\n",
    "        groups[index] = count\n",
    "        index += 1\n",
    "        count = 1\n",
    "\n",
    "groups = groups[groups != 0]\n",
    "\n",
    "print(f\"Number of groups: {len(groups)}\")\n",
    "print(f\"Avg group len: {np.average(groups):.1f}\")\n",
    "print(f\"Longest group: {np.max(groups)}\")\n",
    "print(f\"Episodes not in a group: {groups[groups == 1].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of number of speakers:\n",
      "1 speakers:  214\n",
      "2 speakers:  2105\n",
      "3 speakers:  1030\n",
      "4 speakers:  405\n",
      "5 speakers:  161\n",
      "6 speakers:  74\n",
      "7 speakers:  10\n",
      "8 speakers:  1\n"
     ]
    }
   ],
   "source": [
    "### Count how many speakers there are in each episode\n",
    "\n",
    "speakers_count = raw_df[SPEAKERS].apply(lambda arr: np.unique(arr).shape[0]).to_numpy()\n",
    "min_sp = np.min(speakers_count)\n",
    "max_sp = np.max(speakers_count)\n",
    "print(\"Distribution of number of speakers:\")\n",
    "for count in range(min_sp, max_sp + 1):\n",
    "    print(f\"{count} speakers:  {np.sum(speakers_count == count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes values:\n",
      "{'neutral': 15263, 'joy': 6317, 'surprise': 4645, 'anger': 3964, 'sadness': 2648, 'fear': 1114, 'disgust': 1049}\n"
     ]
    }
   ],
   "source": [
    "### Class imbalance check\n",
    "classes_count = {}\n",
    "for emotions in raw_df[\"emotions\"]:\n",
    "    for emotion in emotions:\n",
    "        if emotion in classes_count:\n",
    "            classes_count[emotion] += 1\n",
    "        else:\n",
    "            classes_count[emotion] = 1\n",
    "\n",
    "### then we sort the dictionary by occurences\n",
    "emotions_dict = {\n",
    "    k: v\n",
    "    for k, v in sorted(classes_count.items(), key=lambda item: item[1], reverse=True)\n",
    "}\n",
    "print(\"Classes values:\")\n",
    "print(emotions_dict)\n",
    "\n",
    "### Classes counts are not balanced: the use of weights is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop not useful column\n",
    "\n",
    "raw_df.drop(columns=[SPEAKERS], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove Nones from the triggers\n",
    "\n",
    "raw_df[TRIGGERS] = raw_df[TRIGGERS].apply(\n",
    "    lambda trig_seq: np.array([0.0 if t is None else t for t in trig_seq])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>episode_1061</td>\n",
       "      <td>[joy, neutral, surprise]</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>episode_1062</td>\n",
       "      <td>[joy, neutral, surprise, surprise]</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>episode_1063</td>\n",
       "      <td>[joy, neutral, surprise, surprise, neutral]</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>episode_1064</td>\n",
       "      <td>[joy, neutral, surprise, surprise, neutral, ne...</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>episode_1065</td>\n",
       "      <td>[joy, neutral, surprise, surprise, neutral, ne...</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           episode                                           emotions  \\\n",
       "1061  episode_1061                           [joy, neutral, surprise]   \n",
       "1062  episode_1062                 [joy, neutral, surprise, surprise]   \n",
       "1063  episode_1063        [joy, neutral, surprise, surprise, neutral]   \n",
       "1064  episode_1064  [joy, neutral, surprise, surprise, neutral, ne...   \n",
       "1065  episode_1065  [joy, neutral, surprise, surprise, neutral, ne...   \n",
       "\n",
       "                                             utterances  \\\n",
       "1061  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1062  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1063  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1064  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1065  [\"Happy birthday to you!\", You're paying for t...   \n",
       "\n",
       "                                 triggers  \n",
       "1061                      [0.0, 1.0, 0.0]  \n",
       "1062                 [0.0, 0.0, 1.0, 0.0]  \n",
       "1063            [0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1064       [0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1065  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Change column \"episode\" from utterance_xyz to episode_xyz\n",
    "for i in range(len(raw_df)):\n",
    "    raw_df[EPISODE][i] = f\"episode_{raw_df[EPISODE][i][10:]}\"\n",
    "\n",
    "clean_df = raw_df\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If an episode contains the same utterances of the previous and a few more then the triggers from the previous episode are replicated in the current episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 0. 1.]\n",
      "[0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{raw_df[TRIGGERS][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replicate triggers\n",
    "\n",
    "count = 0\n",
    "for i in range(1, len(clean_df)):\n",
    "    is_continuation = np.all(\n",
    "        [u in clean_df[UTTERANCES][i] for u in clean_df[UTTERANCES][i - 1]]\n",
    "    )\n",
    "    if is_continuation:\n",
    "        count += 1\n",
    "        for k, t in enumerate(clean_df[TRIGGERS][i - 1]):\n",
    "            clean_df[TRIGGERS][i][k] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      "[0. 0. 1. 0.]\n",
      "[0. 0. 1. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0. 1.]\n",
      "[0. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{raw_df[TRIGGERS][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Val Test split: 80/10/10\n",
    "\n",
    "\n",
    "def split_data(\n",
    "    df: pd.DataFrame, seed: int = 42\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, test_size=0.2, train_size=0.8, random_state=seed\n",
    "    )\n",
    "\n",
    "    df_val, df_test = train_test_split(\n",
    "        df_test, test_size=0.5, train_size=0.5, random_state=seed\n",
    "    )\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train len: 3200\n",
      "df_val len: 400\n",
      "df_test len: 400\n"
     ]
    }
   ],
   "source": [
    "### Check\n",
    "df_train_t, df_val_t, df_test_t = split_data(clean_df)\n",
    "print(f\"df_train len: {len(df_train_t)}\")\n",
    "print(f\"df_val len: {len(df_val_t)}\")\n",
    "print(f\"df_test len: {len(df_test_t)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explode the dataframe: <br>\n",
    "each rows contains: previous utterance, target utterance, next utterance (for context). <br>\n",
    "Except for the first and lat utterance of each episode that have no previous and next utterance, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_add_context(df: pd.DataFrame, ctxt_win_len: int = 1) -> pd.DataFrame:\n",
    "    # TODO ok che context_window sia il numero di utt future == numero utt passate invece che la len di tutta la window?\n",
    "\n",
    "    ### Flatten the lists of utterances,triggers,emotions into new rows of the dataframe\n",
    "    exploded_df = df.explode([UTTERANCES, TRIGGERS, EMOTIONS], ignore_index=True)\n",
    "    exploded_df.rename(columns={UTTERANCES: \"current\"}, inplace=True)\n",
    "    exploded_df.head(10)\n",
    "\n",
    "    ### Pair shifted columns of utterances to the exploded df to make previous and next\n",
    "    for i in range(1, ctxt_win_len + 1):\n",
    "        padding_cells = pd.Series([\" \" for _ in range(i)])\n",
    "\n",
    "        previous_col = pd.concat(\n",
    "            (padding_cells, exploded_df[\"current\"][:-i]), copy=False\n",
    "        ).to_list()\n",
    "        exploded_df.insert(loc=2, column=f\"previous_-{i}\", value=previous_col)\n",
    "\n",
    "        next_col = pd.concat(\n",
    "            (exploded_df[\"current\"][i:], padding_cells), copy=False\n",
    "        ).to_list()\n",
    "        exploded_df.insert(loc=2 + 2 * i, column=f\"next_{i}\", value=next_col)\n",
    "\n",
    "    ### Remove the previous of the first utterance and the next of the last utterance of each episode\n",
    "    for i in range(1, len(exploded_df) - 1):\n",
    "        for j in range(1, ctxt_win_len + 1):\n",
    "            if exploded_df[EPISODE][i] != exploded_df[EPISODE][i - 1]:\n",
    "                exploded_df[f\"next_{j}\"][i - j] = \" \"\n",
    "                exploded_df[f\"previous_-{j}\"][i] = \" \"\n",
    "\n",
    "    exploded_df.sort_values(by=EPISODE, inplace=True)\n",
    "    # TODO sortare prima su episode poi sull'indice\n",
    "    return exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "also I was the point person on my company's transition from the KL-5 to GR-6 system.\n",
      "\n",
      "You must've had your hands full.\n",
      "\n",
      "That I did. That I did.\n",
      "\n",
      "So let's talk a little bit about your duties.\n",
      "\n",
      "My duties?  All right.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = df_train_t[df_train_t[\"episode\"] == \"episode_0\"][\"utterances\"]\n",
    "[print(f\"{e}\\n\") for e in l[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>emotions</th>\n",
       "      <th>previous_-1</th>\n",
       "      <th>current</th>\n",
       "      <th>next_1</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td></td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>surprise</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24390</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24389</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td></td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24391</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24395</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Now you'll be heading a whole division, so you...</td>\n",
       "      <td>I see.</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24392</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         episode  emotions                                        previous_-1  \\\n",
       "864    episode_0   neutral                                                      \n",
       "868    episode_0  surprise      So let's talk a little bit about your duties.   \n",
       "867    episode_0   neutral                            That I did. That I did.   \n",
       "866    episode_0   neutral                   You must've had your hands full.   \n",
       "865    episode_0   neutral  also I was the point person on my company's tr...   \n",
       "24390  episode_1   neutral  also I was the point person on my company's tr...   \n",
       "24389  episode_1   neutral                                                      \n",
       "24391  episode_1   neutral                   You must've had your hands full.   \n",
       "24395  episode_1   neutral  Now you'll be heading a whole division, so you...   \n",
       "24392  episode_1   neutral                            That I did. That I did.   \n",
       "\n",
       "                                                 current  \\\n",
       "864    also I was the point person on my company's tr...   \n",
       "868                               My duties?  All right.   \n",
       "867        So let's talk a little bit about your duties.   \n",
       "866                              That I did. That I did.   \n",
       "865                     You must've had your hands full.   \n",
       "24390                   You must've had your hands full.   \n",
       "24389  also I was the point person on my company's tr...   \n",
       "24391                            That I did. That I did.   \n",
       "24395                                             I see.   \n",
       "24392      So let's talk a little bit about your duties.   \n",
       "\n",
       "                                              next_1 triggers  \n",
       "864                 You must've had your hands full.      0.0  \n",
       "868                                                       0.0  \n",
       "867                           My duties?  All right.      1.0  \n",
       "866    So let's talk a little bit about your duties.      0.0  \n",
       "865                          That I did. That I did.      0.0  \n",
       "24390                        That I did. That I did.      0.0  \n",
       "24389               You must've had your hands full.      0.0  \n",
       "24391  So let's talk a little bit about your duties.      0.0  \n",
       "24395                                                     0.0  \n",
       "24392                         My duties?  All right.      1.0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = explode_add_context(df_train_t, 1)\n",
    "df_val = explode_add_context(df_val_t, 1)\n",
    "df_test = explode_add_context(df_test_t, 1)\n",
    "\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>emotions</th>\n",
       "      <th>previous_-2</th>\n",
       "      <th>previous_-1</th>\n",
       "      <th>current</th>\n",
       "      <th>next_1</th>\n",
       "      <th>next_2</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>surprise</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td></td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24390</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Hello, Joey.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24389</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24391</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24395</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Now you'll be heading a whole division, so you...</td>\n",
       "      <td>I see.</td>\n",
       "      <td></td>\n",
       "      <td>Listen to the plinky-plunky music.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24392</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Now you'll be heading a whole division, so you...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         episode  emotions                                        previous_-2  \\\n",
       "864    episode_0   neutral                                                      \n",
       "868    episode_0  surprise                            That I did. That I did.   \n",
       "867    episode_0   neutral                   You must've had your hands full.   \n",
       "866    episode_0   neutral  also I was the point person on my company's tr...   \n",
       "865    episode_0   neutral  You-you-you didn't know that.  Well, I guess m...   \n",
       "24390  episode_1   neutral                                       Hello, Joey.   \n",
       "24389  episode_1   neutral                                                      \n",
       "24391  episode_1   neutral  also I was the point person on my company's tr...   \n",
       "24395  episode_1   neutral                             My duties?  All right.   \n",
       "24392  episode_1   neutral                   You must've had your hands full.   \n",
       "\n",
       "                                             previous_-1  \\\n",
       "864                                                        \n",
       "868        So let's talk a little bit about your duties.   \n",
       "867                              That I did. That I did.   \n",
       "866                     You must've had your hands full.   \n",
       "865    also I was the point person on my company's tr...   \n",
       "24390  also I was the point person on my company's tr...   \n",
       "24389                                                      \n",
       "24391                   You must've had your hands full.   \n",
       "24395  Now you'll be heading a whole division, so you...   \n",
       "24392                            That I did. That I did.   \n",
       "\n",
       "                                                 current  \\\n",
       "864    also I was the point person on my company's tr...   \n",
       "868                               My duties?  All right.   \n",
       "867        So let's talk a little bit about your duties.   \n",
       "866                              That I did. That I did.   \n",
       "865                     You must've had your hands full.   \n",
       "24390                   You must've had your hands full.   \n",
       "24389  also I was the point person on my company's tr...   \n",
       "24391                            That I did. That I did.   \n",
       "24395                                             I see.   \n",
       "24392      So let's talk a little bit about your duties.   \n",
       "\n",
       "                                              next_1  \\\n",
       "864                 You must've had your hands full.   \n",
       "868                                                    \n",
       "867                           My duties?  All right.   \n",
       "866    So let's talk a little bit about your duties.   \n",
       "865                          That I did. That I did.   \n",
       "24390                        That I did. That I did.   \n",
       "24389               You must've had your hands full.   \n",
       "24391  So let's talk a little bit about your duties.   \n",
       "24395                                                  \n",
       "24392                         My duties?  All right.   \n",
       "\n",
       "                                                  next_2 triggers  \n",
       "864                              That I did. That I did.      0.0  \n",
       "868    Yeah, it kinda grows on you.  Actually, I want...      0.0  \n",
       "867                                                           1.0  \n",
       "866                               My duties?  All right.      0.0  \n",
       "865        So let's talk a little bit about your duties.      0.0  \n",
       "24390      So let's talk a little bit about your duties.      0.0  \n",
       "24389                            That I did. That I did.      0.0  \n",
       "24391                             My duties?  All right.      0.0  \n",
       "24395                 Listen to the plinky-plunky music.      0.0  \n",
       "24392  Now you'll be heading a whole division, so you...      1.0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = explode_add_context(df_train_t, 2)\n",
    "dff.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>emotions</th>\n",
       "      <th>previous_-3</th>\n",
       "      <th>previous_-2</th>\n",
       "      <th>previous_-1</th>\n",
       "      <th>current</th>\n",
       "      <th>next_1</th>\n",
       "      <th>next_2</th>\n",
       "      <th>next_3</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>surprise</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td></td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td>Oh good.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td></td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>That's why you broke up with me?</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24390</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Sorry. Wrong boobies.</td>\n",
       "      <td>Hello, Joey.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         episode  emotions                                        previous_-3  \\\n",
       "864    episode_0   neutral                                                      \n",
       "868    episode_0  surprise                   You must've had your hands full.   \n",
       "867    episode_0   neutral  also I was the point person on my company's tr...   \n",
       "866    episode_0   neutral  You-you-you didn't know that.  Well, I guess m...   \n",
       "865    episode_0   neutral                   That's why you broke up with me?   \n",
       "24390  episode_1   neutral                              Sorry. Wrong boobies.   \n",
       "\n",
       "                                             previous_-2  \\\n",
       "864                                                        \n",
       "868                              That I did. That I did.   \n",
       "867                     You must've had your hands full.   \n",
       "866    also I was the point person on my company's tr...   \n",
       "865    You-you-you didn't know that.  Well, I guess m...   \n",
       "24390                                       Hello, Joey.   \n",
       "\n",
       "                                             previous_-1  \\\n",
       "864                                                        \n",
       "868        So let's talk a little bit about your duties.   \n",
       "867                              That I did. That I did.   \n",
       "866                     You must've had your hands full.   \n",
       "865    also I was the point person on my company's tr...   \n",
       "24390  also I was the point person on my company's tr...   \n",
       "\n",
       "                                                 current  \\\n",
       "864    also I was the point person on my company's tr...   \n",
       "868                               My duties?  All right.   \n",
       "867        So let's talk a little bit about your duties.   \n",
       "866                              That I did. That I did.   \n",
       "865                     You must've had your hands full.   \n",
       "24390                   You must've had your hands full.   \n",
       "\n",
       "                                              next_1  \\\n",
       "864                 You must've had your hands full.   \n",
       "868                                                    \n",
       "867                           My duties?  All right.   \n",
       "866    So let's talk a little bit about your duties.   \n",
       "865                          That I did. That I did.   \n",
       "24390                        That I did. That I did.   \n",
       "\n",
       "                                                  next_2  \\\n",
       "864                              That I did. That I did.   \n",
       "868    Yeah, it kinda grows on you.  Actually, I want...   \n",
       "867                                                        \n",
       "866                               My duties?  All right.   \n",
       "865        So let's talk a little bit about your duties.   \n",
       "24390      So let's talk a little bit about your duties.   \n",
       "\n",
       "                                                  next_3 triggers  \n",
       "864        So let's talk a little bit about your duties.      0.0  \n",
       "868                                             Oh good.      0.0  \n",
       "867    Yeah, it kinda grows on you.  Actually, I want...      1.0  \n",
       "866                                                           0.0  \n",
       "865                               My duties?  All right.      0.0  \n",
       "24390                             My duties?  All right.      0.0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = explode_add_context(df_train_t, 3)\n",
    "dff.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>emotions</th>\n",
       "      <th>previous_-5</th>\n",
       "      <th>previous_-4</th>\n",
       "      <th>previous_-3</th>\n",
       "      <th>previous_-2</th>\n",
       "      <th>previous_-1</th>\n",
       "      <th>current</th>\n",
       "      <th>next_1</th>\n",
       "      <th>next_2</th>\n",
       "      <th>next_3</th>\n",
       "      <th>next_4</th>\n",
       "      <th>next_5</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>surprise</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td></td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td>Oh good.</td>\n",
       "      <td>Look, I  Look, I'm having a great time with y...</td>\n",
       "      <td>So, I'm sorry I just don't think we should go ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>That's why you broke up with me?</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td></td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td>Oh good.</td>\n",
       "      <td>Look, I  Look, I'm having a great time with y...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Ah, uh, I owe you a long overdue apology. I ne...</td>\n",
       "      <td>That's why you broke up with me?</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td></td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td>Oh good.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>It's pretty clear.</td>\n",
       "      <td>Ah, uh, I owe you a long overdue apology. I ne...</td>\n",
       "      <td>That's why you broke up with me?</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td></td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24390</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Clear the tracks for the boobie payback expres...</td>\n",
       "      <td>Joey!! What the hell were you doing?!</td>\n",
       "      <td>Sorry. Wrong boobies.</td>\n",
       "      <td>Hello, Joey.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Now you'll be heading a whole division, so you...</td>\n",
       "      <td>I see.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         episode  emotions                                        previous_-5  \\\n",
       "864    episode_0   neutral                                                      \n",
       "868    episode_0  surprise  You-you-you didn't know that.  Well, I guess m...   \n",
       "867    episode_0   neutral                   That's why you broke up with me?   \n",
       "866    episode_0   neutral  Ah, uh, I owe you a long overdue apology. I ne...   \n",
       "865    episode_0   neutral                                 It's pretty clear.   \n",
       "24390  episode_1   neutral  Clear the tracks for the boobie payback expres...   \n",
       "\n",
       "                                             previous_-4  \\\n",
       "864                                                        \n",
       "868    also I was the point person on my company's tr...   \n",
       "867    You-you-you didn't know that.  Well, I guess m...   \n",
       "866                     That's why you broke up with me?   \n",
       "865    Ah, uh, I owe you a long overdue apology. I ne...   \n",
       "24390              Joey!! What the hell were you doing?!   \n",
       "\n",
       "                                             previous_-3  \\\n",
       "864                                                        \n",
       "868                     You must've had your hands full.   \n",
       "867    also I was the point person on my company's tr...   \n",
       "866    You-you-you didn't know that.  Well, I guess m...   \n",
       "865                     That's why you broke up with me?   \n",
       "24390                              Sorry. Wrong boobies.   \n",
       "\n",
       "                                             previous_-2  \\\n",
       "864                                                        \n",
       "868                              That I did. That I did.   \n",
       "867                     You must've had your hands full.   \n",
       "866    also I was the point person on my company's tr...   \n",
       "865    You-you-you didn't know that.  Well, I guess m...   \n",
       "24390                                       Hello, Joey.   \n",
       "\n",
       "                                             previous_-1  \\\n",
       "864                                                        \n",
       "868        So let's talk a little bit about your duties.   \n",
       "867                              That I did. That I did.   \n",
       "866                     You must've had your hands full.   \n",
       "865    also I was the point person on my company's tr...   \n",
       "24390  also I was the point person on my company's tr...   \n",
       "\n",
       "                                                 current  \\\n",
       "864    also I was the point person on my company's tr...   \n",
       "868                               My duties?  All right.   \n",
       "867        So let's talk a little bit about your duties.   \n",
       "866                              That I did. That I did.   \n",
       "865                     You must've had your hands full.   \n",
       "24390                   You must've had your hands full.   \n",
       "\n",
       "                                              next_1  \\\n",
       "864                 You must've had your hands full.   \n",
       "868                                                    \n",
       "867                           My duties?  All right.   \n",
       "866    So let's talk a little bit about your duties.   \n",
       "865                          That I did. That I did.   \n",
       "24390                        That I did. That I did.   \n",
       "\n",
       "                                                  next_2  \\\n",
       "864                              That I did. That I did.   \n",
       "868    Yeah, it kinda grows on you.  Actually, I want...   \n",
       "867                                                        \n",
       "866                               My duties?  All right.   \n",
       "865        So let's talk a little bit about your duties.   \n",
       "24390      So let's talk a little bit about your duties.   \n",
       "\n",
       "                                                  next_3  \\\n",
       "864        So let's talk a little bit about your duties.   \n",
       "868                                             Oh good.   \n",
       "867    Yeah, it kinda grows on you.  Actually, I want...   \n",
       "866                                                        \n",
       "865                               My duties?  All right.   \n",
       "24390                             My duties?  All right.   \n",
       "\n",
       "                                                  next_4  \\\n",
       "864                               My duties?  All right.   \n",
       "868    Look, I\n",
       "  Look, I'm having a great time with y...   \n",
       "867                                             Oh good.   \n",
       "866    Yeah, it kinda grows on you.  Actually, I want...   \n",
       "865                                                        \n",
       "24390  Now you'll be heading a whole division, so you...   \n",
       "\n",
       "                                                  next_5 triggers  \n",
       "864                                                           0.0  \n",
       "868    So, I'm sorry I just don't think we should go ...      0.0  \n",
       "867    Look, I\n",
       "  Look, I'm having a great time with y...      1.0  \n",
       "866                                             Oh good.      0.0  \n",
       "865    Yeah, it kinda grows on you.  Actually, I want...      0.0  \n",
       "24390                                             I see.      0.0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = explode_add_context(df_train_t, 5)\n",
    "dff.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Given 2 series of sequences of triggers/emotions compute F1 inside each sequence and return avg,\n",
    "\n",
    "# TODO remove\n",
    "# def sequence_f1(y_true, y_pred, avg: bool = True):\n",
    "#     res = [\n",
    "#         f1_score(y_true=y_t, y_pred=y_p, average=\"micro\")\n",
    "#         for y_t, y_p in zip(y_true, y_pred)\n",
    "#     ]\n",
    "#     return np.average(res) if avg else res\n",
    "\n",
    "\n",
    "### Compute F1 score for each flattened dialogue and return avg over dialogues\n",
    "def sequence_f1(\n",
    "    y_true: pd.DataFrame,\n",
    "    y_pred: np.ndarray,\n",
    "    target_column: str,\n",
    "    avg: bool = True,\n",
    ") -> Union[dict, float]:\n",
    "    assert len(y_pred) == len(y_true), \"y_pred and y_true must be of the same lenght\"\n",
    "    assert (\n",
    "        y_true[EPISODE].is_monotonic_increasing\n",
    "        or y_true[EPISODE].is_monotonic_decreasing\n",
    "    ), \"utterances must be sorted over the episodes\"\n",
    "\n",
    "    res = {}\n",
    "    start = 0\n",
    "    stop_incl = 0\n",
    "    for i in range(1, len(y_pred)):\n",
    "        if y_true[EPISODE][i - 1] != y_true[EPISODE][i]:\n",
    "            stop_incl = i - 1\n",
    "            f1 = f1_score(\n",
    "                y_true=y_true[target_column][start : stop_incl + 1].to_list(),\n",
    "                y_pred=y_pred[start : stop_incl + 1],\n",
    "                average=\"micro\",\n",
    "            )\n",
    "            res.update({y_true[EPISODE][start]: f1})\n",
    "            start = i\n",
    "\n",
    "    # np.std(list(res.values))\n",
    "    return res if not avg else np.average(list(res.values()))\n",
    "\n",
    "\n",
    "### Compute F1 score for the unrolled sequence\n",
    "def unrolled_f1(\n",
    "    y_true: pd.DataFrame,\n",
    "    y_pred: np.ndarray,\n",
    "    target_column: str,\n",
    ") -> float:\n",
    "    return f1_score(y_true[target_column].to_list(), y_pred, average=\"micro\")\n",
    "\n",
    "\n",
    "# TODO remove\n",
    "# def unrolled_f1(y_true, y_pred):\n",
    "#     y_t_flat = []\n",
    "#     for l in y_true:\n",
    "#         for e in l:\n",
    "#             y_t_flat.append(e)\n",
    "#\n",
    "#     y_p_flat = []\n",
    "#     for l in y_pred:\n",
    "#         for e in l:\n",
    "#             y_p_flat.append(e)\n",
    "#\n",
    "#     return f1_score(y_true=y_t_flat, y_pred=y_p_flat, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create baseline models\n",
    "\n",
    "\n",
    "# TODO do we still need the class?\n",
    "class SequenceDummyClassifier(DummyClassifier):\n",
    "    def __init__(self, strategy: str, seed: int = 42) -> None:\n",
    "        self.seed = seed\n",
    "        if not strategy.lower() in (\"random\", \"majority\"):\n",
    "            raise ValueError(\"strategy must be in [random, majority]\")\n",
    "        sklearn_strategy = \"uniform\" if strategy == \"random\" else \"most_frequent\"\n",
    "        super().__init__(strategy=sklearn_strategy, random_state=seed)\n",
    "\n",
    "    # TODO remove\n",
    "    # def _flatten_seq(self, df: pd.Series):\n",
    "    #     res = []\n",
    "    #     for l in df:\n",
    "    #         for e in l:\n",
    "    #             res.append(e)\n",
    "    #     return res\n",
    "\n",
    "    # TODO remove\n",
    "    # def _deflatten_seq(self, seq, shape_like: pd.Series):\n",
    "    #     data = iter(seq)\n",
    "    #     result = [[next(data) for _ in s] for s in shape_like]\n",
    "    #     return result\n",
    "\n",
    "    # TODO remove\n",
    "    # def fit(self, X: pd.Series, y: pd.Series):\n",
    "    #     X_flat = self._flatten_seq(X)\n",
    "    #     y_flat = self._flatten_seq(y)\n",
    "    #     super().fit(X=X_flat, y=y_flat)\n",
    "\n",
    "    # TODO remove\n",
    "    # def predict(self, X: pd.Series, return_flat: bool = False):\n",
    "    #     X_flat = self._flatten_seq(X)\n",
    "    #     y_flat = super().predict(X_flat)\n",
    "    #     return y_flat if return_flat else self._deflatten_seq(seq=y_flat, shape_like=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_f1(emotions_Random) : 0.44657433160540777\n",
      "unrolled_f1(emotions_Random) : 0.4330935251798561\n",
      "sequence_f1(triggers_Random) : 0.6522845341422358\n",
      "unrolled_f1(triggers_Random) : 0.6515107913669065\n",
      "sequence_f1(emotions_Majority) : 0.44657433160540777\n",
      "unrolled_f1(emotions_Majority) : 0.4330935251798561\n",
      "sequence_f1(triggers_Majority) : 0.6522845341422358\n",
      "unrolled_f1(triggers_Majority) : 0.6515107913669065\n"
     ]
    }
   ],
   "source": [
    "def experiment_baseline(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    seed: int = 42,\n",
    ") -> dict:\n",
    "\n",
    "    baseline_f1s = {}\n",
    "    baseline_results = {}\n",
    "\n",
    "    for strategy in (\"Random\", \"Majority\"):\n",
    "        for target in (EMOTIONS, TRIGGERS):\n",
    "            clf = SequenceDummyClassifier(strategy=strategy, seed=seed)\n",
    "            clf.fit(X=df_train[\"current\"], y=df_train[target])\n",
    "\n",
    "            res = clf.predict(X=df_test[\"current\"])\n",
    "            baseline_results.update({f\"{target}_{strategy}\": res})\n",
    "\n",
    "            seq_f1 = sequence_f1(y_true=df_test, y_pred=res, target_column=target)\n",
    "            baseline_f1s.update({f\"sequence_f1({target}_{strategy})\": seq_f1})\n",
    "\n",
    "            unr_f1 = unrolled_f1(y_true=df_test, y_pred=res, target_column=target)\n",
    "            baseline_f1s.update({f\"unrolled_f1({target}_{strategy})\": unr_f1})\n",
    "\n",
    "    return baseline_f1s, baseline_results\n",
    "\n",
    "\n",
    "f1s, results = experiment_baseline(df_train, df_test)\n",
    "for k, v in f1s.items():\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': array([1., 0., 0., 0., 0., 0., 0.]), 'disgust': array([0., 1., 0., 0., 0., 0., 0.]), 'fear': array([0., 0., 1., 0., 0., 0., 0.]), 'joy': array([0., 0., 0., 1., 0., 0., 0.]), 'neutral': array([0., 0., 0., 0., 1., 0., 0.]), 'sadness': array([0., 0., 0., 0., 0., 1., 0.]), 'surprise': array([0., 0., 0., 0., 0., 0., 1.])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# TODO see class weights for obtaining all_emotions (unique values)\n",
    "all_emotions = []\n",
    "for e in df_train[EMOTIONS]:\n",
    "    all_emotions.append(e)\n",
    "uniq_emotions = np.sort(np.unique(all_emotions))\n",
    "one_hot = np.identity(len(uniq_emotions))\n",
    "emotion_mapping = {e: one_hot[i] for i, e in enumerate(uniq_emotions)}\n",
    "print(emotion_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e']\n"
     ]
    }
   ],
   "source": [
    "l1 = [\"a\", \"b\"]\n",
    "l2 = [\"d\", \"e\"]\n",
    "l = l1 + [\"c\"] + l2\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP = \"[SEP]\"\n",
    "\n",
    "\n",
    "def tokenize(ds_row, tokenizer=tokenizer):\n",
    "\n",
    "    if type(ds_row[\"emotions\"]) != str:  ### batchsize > 1\n",
    "        emotion_encoding = []\n",
    "        emotion_encoding = [emotion_mapping[e] for e in ds_row[\"emotions\"]]\n",
    "    else:  ### batchsize == 1\n",
    "        emotion_encoding = emotion_mapping[ds_row[\"emotions\"]]\n",
    "\n",
    "    encoded_ds_row = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        # \"emotions\": ds_row[\"emotions\"],\n",
    "        \"emotions\": emotion_encoding,\n",
    "        \"triggers\": ds_row[\"triggers\"],\n",
    "    }\n",
    "\n",
    "    # next_1 next_2 next_3\n",
    "    # context = [previous_-2,-1,... current, next1,2,...]\n",
    "    # TODO: pass as param\n",
    "    col_names = list(ds_row.keys())\n",
    "    prev_names = [c for c in col_names if \"prev\" in c]\n",
    "    prev_names.sort(\n",
    "        reverse=True\n",
    "    )  ### descending (numbers are -1,-2.. but sorted lexicographically)\n",
    "    next_names = [c for c in col_names if \"next\" in c]\n",
    "    next_names.sort()\n",
    "    context_names = prev_names + [\"current\"] + next_names\n",
    "    context = [ds_row[name] for name in context_names if ds_row[name] != \" \"]\n",
    "\n",
    "    # print(context)\n",
    "\n",
    "    sentence = \"\"\n",
    "    for sent in context:\n",
    "        sentence += sent\n",
    "        sentence += SEP\n",
    "    sentence = sentence[:-5]\n",
    "\n",
    "    tokenized_context = tokenizer(\n",
    "        sentence,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length // 4,\n",
    "        return_tensors=\"pt\",\n",
    "        # is_split_into_words=True,  # TODO remove\n",
    "    )\n",
    "\n",
    "    encoded_ds_row[\"input_ids\"] = tokenized_context[\"input_ids\"].squeeze(dim=0)\n",
    "    encoded_ds_row[\"token_type_ids\"] = tokenized_context[\"token_type_ids\"].squeeze(dim=0)\n",
    "    encoded_ds_row[\"attention_mask\"] = tokenized_context[\"attention_mask\"].squeeze(dim=0)\n",
    "\n",
    "    # for sentence in ds_row[\"utterances\"]:\n",
    "    #\n",
    "    #    tokenized_sentence = tokenizer(\n",
    "    #        sentence,\n",
    "    #        truncation=True,\n",
    "    #        padding=\"max_length\",\n",
    "    #        max_length=tokenizer.model_max_length // 4,\n",
    "    #        return_tensors=\"pt\",\n",
    "    #        # is_split_into_words=True,\n",
    "    #    )\n",
    "    #    encoded_ds_row[\"input_ids\"].append(tokenized_sentence[\"input_ids\"])\n",
    "    #    encoded_ds_row[\"token_type_ids\"].append(tokenized_sentence[\"token_type_ids\"])\n",
    "    #    encoded_ds_row[\"attention_mask\"].append(tokenized_sentence[\"attention_mask\"])\n",
    "\n",
    "    return encoded_ds_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_pandas(df_train)\n",
    "ds_val = Dataset.from_pandas(df_val)\n",
    "ds_test = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(ds_row, tokenizer=tokenizer):\n",
    "    print(dir(ds_row))\n",
    "    # len\n",
    "    # data\n",
    "\n",
    "    1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['previous_-1', 'current', 'next_1']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_train.columns[2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1a990da3a14dfd9ca68a9373f22bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1b53f4c6fd43aa85174d4640fdfe38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38eef09fccfa4d6696ede047fffb8cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3475 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Apply tokenization\n",
    "batched = False  # TODO sistemare per True\n",
    "cols_to_drop = list(df_train.columns[2:-1])\n",
    "# TODO check che non ci sia una colonna '__index_level_0__' da droppare o evitare proprio che ci sia dal principio\n",
    "ds_train_tokenized = ds_train.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=batched,\n",
    "    remove_columns=cols_to_drop,\n",
    ")\n",
    "ds_train_tokenized.set_format(type=\"torch\")\n",
    "ds_val_tokenized = ds_val.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=batched,\n",
    "    remove_columns=cols_to_drop,\n",
    ")\n",
    "ds_val_tokenized.set_format(type=\"torch\")\n",
    "ds_test_tokenized = ds_test.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=batched,\n",
    "    remove_columns=cols_to_drop,\n",
    ")\n",
    "ds_test_tokenized.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] so let ' s talk a little bit about your duties . [SEP] my duties ? all right . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "ii = ds_train_tokenized[\"input_ids\"][1]\n",
    "tokens = tokenizer.convert_ids_to_tokens(ii)\n",
    "original_string = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(original_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28034, 128])\n"
     ]
    }
   ],
   "source": [
    "print(ds_train_tokenized[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ C L S ]', 'a l s o', 'i', 'w a s', 't h e', 'p o i n t', 'p e r s o n', 'o n', 'm y', 'c o m p a n y', \"'\", 's', 't r a n s i t i o n', 'f r o m', 't h e', 'k', '# # l', '-', '5', 't o', 'g r', '-', '6', 's y s t e m', '.', '[ S E P ]', 'y o u', 'm u s t', \"'\", 'v e', 'h a d', 'y o u r', 'h a n d s', 'f u l l', '.', '[ S E P ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]']\n",
      "['[ C L S ]', 'a l s o', 'i', 'w a s', 't h e', 'p o i n t', 'p e r s o n', 'o n', 'm y', 'c o m p a n y', \"'\", 's', 't r a n s i t i o n', 'f r o m', 't h e', 'k', '# # l', '-', '5', 't o', 'g r', '-', '6', 's y s t e m', '.', '[ S E P ]', 'y o u', 'm u s t', \"'\", 'v e', 'h a d', 'y o u r', 'h a n d s', 'f u l l', '.', '[ S E P ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_row = tokenize(ds_train[0])\n",
    "# print(tokenized_row[\"input_ids\"][0])\n",
    "print(tokenizer.batch_decode(tokenized_row[\"input_ids\"]))\n",
    "print(tokenizer.batch_decode(tokenized_row[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,  2036,  1045,  2001,  1996,  2391,  2711,  2006,  2026,  2194,\n",
      "         1005,  1055,  6653,  2013,  1996,  1047,  2140,  1011,  1019,  2000,\n",
      "        24665,  1011,  1020,  2291,  1012,   102,  2017,  2442,  1005,  2310,\n",
      "         2018,  2115,  2398,  2440,  1012,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'emotions': array([0., 0., 0., 0., 1., 0., 0.]), 'triggers': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Test della funzione\n",
    "tokens = tokenize(ds_train[0], tokenizer)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class weighting algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(df: pd.Series) -> torch.tensor:\n",
    "    labels, n_ones = np.unique(df.to_numpy, return_counts=True)\n",
    "    l = len(df)\n",
    "    n_zeroes = np.array([l - n for n in n_ones])\n",
    "\n",
    "    weights = np.empty_like(n_ones)\n",
    "    for class_num, (ones, zeroes) in enumerate(zip(n_ones, n_zeroes)):\n",
    "        # TODO choose one\n",
    "        weights[class_num] = zeroes / (ones + 1e-4)\n",
    "        # weights[class_num] = np.sqrt(zeroes / (ones + 1e-4))\n",
    "\n",
    "    print(f\"weigts = {weights}\")\n",
    "    return torch.as_tensor(weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, num_emotions=7):\n",
    "        super(BERTClass, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        # self.lstm = torch.nn.LSTM()\n",
    "        # classifiers\n",
    "        self.l_emotions = torch.nn.Linear(self.bert.config.hidden_size, num_emotions)\n",
    "        self.l_triggers = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "\n",
    "        print(ids.shape)\n",
    "        print(mask.shape)\n",
    "        print(token_type_ids.shape)\n",
    "\n",
    "        # print(ids.reshape(-1, ids.size(-1)))\n",
    "        output = self.bert(\n",
    "            ids, attention_mask=mask, token_type_ids=token_type_ids\n",
    "        )  # , token_type_ids=token_type_ids)\n",
    "\n",
    "        output = output.pooler_output\n",
    "        output_emotions = self.dropout(output)\n",
    "        output_triggers = self.dropout(output)\n",
    "\n",
    "        # output_triggers = self.lstm(output_triggers)\n",
    "\n",
    "        output_emotions = self.l_emotions(output_emotions)\n",
    "        output_triggers = self.l_triggers(output_triggers)\n",
    "\n",
    "        return output_emotions, output_triggers\n",
    "\n",
    "    def freeze_params(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_emotions = df_train[\"emotions\"].nunique()\n",
    "\n",
    "model_frozen = BERTClass(num_emotions)\n",
    "model_full = BERTClass(num_emotions)\n",
    "\n",
    "model_frozen.to(device)\n",
    "model_full.to(device)\n",
    "model_frozen.freeze_params()\n",
    "\n",
    "# Verifying that the params are actually frozen\n",
    "# for name, param in model_frozen.named_parameters():\n",
    "#     print(name, param.requires_grad)\n",
    "#\n",
    "# for name, param in model_full.named_parameters():\n",
    "#     print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [model_full, model_frozen]\n",
    "num_epochs = 5\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "for model in model_list:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    # Tokenizer initiation\n",
    "    # TODO Check tokenizer parameters\n",
    "    # TODO A Dataloader seems to be commonly used for this use cases\n",
    "    # encoding = tokenizer(df_train, truncation=False, padding='max_length', return_tensors='pt')\n",
    "    # input_ids = encoding['input_ids']\n",
    "    # attention_mask = encoding['attention_mask']\n",
    "\n",
    "    # Training loop\n",
    "    # for epoch in range(num_epochs):\n",
    "    #\n",
    "    #    for idx in range(len(df_test)):\n",
    "\n",
    "    #        text = df_train[idx].drop(TRIGGERS)\n",
    "    #        label = df_train[idx][TRIGGERS]\n",
    "\n",
    "    #        optimizer.zero_grad()\n",
    "\n",
    "    #        logits = model(batch_data)\n",
    "    #        loss = loss_fn(logits, batch_labels)\n",
    "    #        loss.backward()\n",
    "    #        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs_emotions, outputs_triggers, emotions_labels, triggers_labels):\n",
    "    L1 = torch.nn.CrossEntropyLoss()\n",
    "    L2 = torch.nn.BCELoss()\n",
    "    print(outputs_triggers.shape)\n",
    "    print(triggers_labels.shape)\n",
    "    return L1(outputs_emotions, emotions_labels) + L2(outputs_triggers, triggers_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training of the model\n",
    "def train_model(train_dl, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions_emotions = 0\n",
    "    correct_predictions_triggers = 0\n",
    "    num_samples_emotions = 0\n",
    "    num_samples_triggers = 0\n",
    "\n",
    "    ### activate dropout, batch norm\n",
    "    model.train()\n",
    "\n",
    "    ### initialize progress bar\n",
    "    batches = tq.tqdm(\n",
    "        enumerate(train_dl), total=len(train_dl), leave=True, colour=\"steelblue\"\n",
    "    )\n",
    "\n",
    "    for batch_idx, data in batches:\n",
    "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "        emotions_labels = data[\"emotions\"].to(device, dtype=torch.float)\n",
    "        triggers_labels = data[\"triggers\"].to(device, dtype=torch.float)\n",
    "\n",
    "\n",
    "        outputs_emotions, outputs_triggers = model(\n",
    "            ids, mask, token_type_ids\n",
    "        )  ### Forward\n",
    "\n",
    "\n",
    "        #outputs_emotions = outputs_emotions.squeeze(dim=1)\n",
    "        outputs_triggers = outputs_triggers.squeeze(dim=1)\n",
    "\n",
    "        print(emotions_labels, outputs_emotions)\n",
    "        loss = loss_fn(\n",
    "            outputs_emotions, outputs_triggers, emotions_labels, triggers_labels\n",
    "        )\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        ### apply thresh 0.5\n",
    "        outputs_emotions = (\n",
    "            torch.sigmoid(outputs_emotions).cpu().detach().numpy().round()\n",
    "        )\n",
    "        outputs_triggers = (\n",
    "            torch.sigmoid(outputs_triggers).cpu().detach().numpy().round()\n",
    "        )\n",
    "\n",
    "        emotions_labels = emotions_labels.cpu().detach().numpy()\n",
    "        triggers_labels = triggers_labels.cpu().detach().numpy()\n",
    "\n",
    "        correct_predictions_emotions += np.sum(outputs_emotions == emotions_labels)\n",
    "        correct_predictions_triggers += np.sum(outputs_triggers == triggers_labels)\n",
    "\n",
    "        num_samples_emotions += emotions_labels.size\n",
    "        num_samples_triggers += triggers_labels.size\n",
    "\n",
    "        ### Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        ### Grad descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        ### Update progress bar\n",
    "        batches.set_description(f\"\")\n",
    "        batches.set_postfix(batch_loss=loss)\n",
    "\n",
    "    # Si potrebbe fare una singola accuracy come media delle due, magari fuori dal training\n",
    "    accuracy_emotions = float(correct_predictions_emotions) / num_samples_emotions\n",
    "    accuracy_triggers = float(correct_predictions_triggers) / num_samples_triggers\n",
    "\n",
    "    return model, accuracy_emotions, accuracy_triggers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval model, setup e train_eval da definire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(validation_dl, model):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    num_categories = next(iter(validation_dl))[\"labels\"].shape[1]\n",
    "\n",
    "    ### accumulate data over each batch to compute the f1\n",
    "    true_positives = np.array([0 for _ in range(num_categories)])\n",
    "    false_positives = np.array([0 for _ in range(num_categories)])\n",
    "    false_negatives = np.array([0 for _ in range(num_categories)])\n",
    "\n",
    "    ### turn off dropout, fix batch norm\n",
    "    model.eval()\n",
    "\n",
    "    ### show progress bar\n",
    "    batches = tq.tqdm(\n",
    "        enumerate(validation_dl),\n",
    "        total=len(validation_dl),\n",
    "        leave=True,\n",
    "        colour=\"steelblue\",\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in batches:\n",
    "            ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "            mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            emotions_labels = data[\"emotions\"].to(device, dtype=torch.float)\n",
    "            triggers_labels = data[\"triggers\"].to(device, dtype=torch.float)\n",
    "            outputs_emotions, outputs_triggers = model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss = loss_fn(\n",
    "                outputs_emotions, outputs_triggers, emotions_labels, triggers_labels\n",
    "            )\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "            ### validation accuracy\n",
    "            ### training sigmoid is in BCEWithLogitsLoss\n",
    "            outputs_emotions = (\n",
    "                torch.sigmoid(outputs_emotions).cpu().detach().numpy().round()\n",
    "            )\n",
    "            outputs_triggers = (\n",
    "                torch.sigmoid(outputs_triggers).cpu().detach().numpy().round()\n",
    "            )\n",
    "\n",
    "            emotions_labels = emotions_labels.cpu().detach().numpy()\n",
    "            triggers_labels = triggers_labels.cpu().detach().numpy()\n",
    "            correct_predictions_emotions += np.sum(outputs_emotions == emotions_labels)\n",
    "            correct_predictions_triggers += np.sum(outputs_triggers == triggers_labels)\n",
    "\n",
    "            num_samples_emotions += emotions_labels.size\n",
    "            num_samples_triggers += triggers_labels.size\n",
    "\n",
    "        accuracy_emotions = float(correct_predictions_emotions) / num_samples_emotions\n",
    "        accuracy_triggers = float(correct_predictions_triggers) / num_samples_triggers\n",
    "        # precision = true_positives / (true_positives + false_positives)\n",
    "        # recall = true_positives / (true_positives + false_negatives)\n",
    "        # f1_per_cat = 2 * (precision * recall) / (precision + recall)\n",
    "        # f1_overall = np.mean(f1_per_cat)\n",
    "\n",
    "    return accuracy_emotions, accuracy_triggers, losses  # , f1_overall, f1_per_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(\n",
    "    train_dl,\n",
    "    validation_dl,\n",
    "    model,\n",
    "    optimizer,\n",
    "    n_epochs=1,\n",
    "    save_name=\"0\",\n",
    "    train_model_f=train_model,\n",
    "    eval_model_f=eval_model,\n",
    "):\n",
    "    model_folder = Path.cwd().joinpath(\"models\")\n",
    "    if not model_folder.exists():\n",
    "        model_folder.mkdir(parents=True)\n",
    "\n",
    "    history = {}\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "        model, accuracy_emotions, accuracy_triggers, train_loss = train_model_f(\n",
    "            train_dl, model, optimizer\n",
    "        )\n",
    "        val_accuracy_emotions, val_accuracy_triggers, val_loss = eval_model_f(\n",
    "            validation_dl, model\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"train_loss={np.mean(train_loss):.4f}, val_loss={np.mean(val_loss):.4f}, \"\n",
    "            f\"train_acc_emo={accuracy_emotions:.4f}, train_acc_emo={accuracy_triggers:.4f},\"\n",
    "            f\"val_acc_emo={val_accuracy_emotions:.4f}, val_acc_tri={val_accuracy_triggers:.4f}, \"\n",
    "        )\n",
    "\n",
    "        history.update({\"train_acc_emo\": accuracy_emotions})\n",
    "        history.update({\"train_acc_tri\": accuracy_triggers})\n",
    "        history.update({\"train_losses\": train_loss})\n",
    "        history.update({\"val_acc_emo\": val_accuracy_emotions})\n",
    "        history.update({\"val_acc_emo\": val_accuracy_triggers})\n",
    "        history.update({\"val_losses\": val_loss})\n",
    "        # history.update({\"f1_overall\": f1_overall})\n",
    "        # history.update({\"f1_per_cat\": f1_per_cat})\n",
    "\n",
    "        ### save the best model\n",
    "        # if f1_overall > best_f1:\n",
    "        #   torch.save(\n",
    "        #      model.state_dict(),\n",
    "        #      Path.joinpath(model_folder, f\"model_{save_name}.bin\"),\n",
    "        # )\n",
    "        # best_f1 = f1_overall\n",
    "\n",
    "    return history  # (history[\"f1_overall\"], history[\"f1_per_cat\"], history[\"train_losses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(tokenized_datasets, batch_size):\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"train\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    validation_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"validation\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    test_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"test\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return train_dl, validation_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    ds_train_tokenized,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "validation_dl = torch.utils.data.DataLoader(\n",
    "    ds_val_tokenized,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Custom loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e021352fbd444848bd7af0b07c567377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 128])\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0.]], device='cuda:0') tensor([[-0.0049,  0.0021, -0.3460, -0.3165, -0.3444, -0.4918, -0.1588]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Reshape the target tensor to have the same size as the input tensor\u001b[39;00m\n\u001b[0;32m     11\u001b[0m target_size \u001b[38;5;241m=\u001b[39m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize()\n",
      "Cell \u001b[1;32mIn[124], line 20\u001b[0m, in \u001b[0;36mtrain_eval\u001b[1;34m(train_dl, validation_dl, model, optimizer, n_epochs, save_name, train_model_f, eval_model_f)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     model, accuracy_emotions, accuracy_triggers, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_f\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     val_accuracy_emotions, val_accuracy_triggers, val_loss \u001b[38;5;241m=\u001b[39m eval_model_f(\n\u001b[0;32m     24\u001b[0m         validation_dl, model\n\u001b[0;32m     25\u001b[0m     )\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(train_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(val_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc_emo=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_emotions\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train_acc_emo=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_triggers\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc_emo=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy_emotions\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val_acc_tri=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy_triggers\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[121], line 37\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_dl, model, optimizer)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(emotions_labels, outputs_emotions)\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(\n\u001b[0;32m     35\u001b[0m     outputs_emotions, outputs_triggers, emotions_labels, triggers_labels\n\u001b[0;32m     36\u001b[0m )\n\u001b[1;32m---> 37\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m### apply thresh 0.5\u001b[39;00m\n\u001b[0;32m     40\u001b[0m outputs_emotions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     41\u001b[0m     torch\u001b[38;5;241m.\u001b[39msigmoid(outputs_emotions)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mround()\n\u001b[0;32m     42\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "history = train_eval(\n",
    "    train_dl,\n",
    "    validation_dl,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    save_name=f\"test_model\",\n",
    ")\n",
    "\n",
    "# Reshape the target tensor to have the same size as the input tensor\n",
    "target_size = history[\"train_losses\"].size()\n",
    "input_size = history[\"train_acc_emo\"].size()\n",
    "if target_size != input_size:\n",
    "    history[\"train_losses\"] = history[\"train_losses\"].view(*input_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor [[utt + pad], [utt + pad], [utt + pad]]\n",
    "tensor [CLS + utt + SEP + utt + SEP + utt + SEP + pad]\n",
    "[ tensor[cls utt padd] tensor [cls utt padd]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training con Classe Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainer(Trainer):\n",
    "    def __init__(self, model, training_args, train_ds, eval_ds, metrics):\n",
    "        super().__init__(model, training_args, train_ds, eval_ds, metrics)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        emotions_labels = inputs[\"emotions\"]\n",
    "        triggers_labels = inputs[\"triggers\"]\n",
    "        output_emotions, output_triggers = model(ids, mask)\n",
    "\n",
    "        custom_loss = self.loss_fn(\n",
    "            output_emotions, output_triggers, emotions_labels, triggers_labels\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            (custom_loss, output_emotions, output_triggers)\n",
    "            if return_outputs\n",
    "            else custom_loss\n",
    "        )\n",
    "\n",
    "    def loss_fn(outputs_emotions, outputs_triggers, emotions_labels, triggers_labels):\n",
    "        return torch.nn.CrossEntropyLoss(\n",
    "            outputs_emotions, emotions_labels\n",
    "        ) + torch.nn.BCELoss(outputs_triggers, triggers_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"\\\\test\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    # evaluate_during_training=True,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=8,\n",
    "    #seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "false INTERNAL ASSERT FAILED at \"C:\\\\actions-runner\\\\_work\\\\pytorch\\\\pytorch\\\\builder\\\\windows\\\\pytorch\\\\c10/cuda/CUDAGraphsC10Utils.h\":74, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus-1851167152",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_train_tokenized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_val_tokenized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_f1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# tokenizer=tokenizer,\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gianl\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:337\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gianl\\anaconda3\\lib\\site-packages\\transformers\\trainer_utils.py:95\u001b[0m, in \u001b[0;36mset_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     93\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gianl\\anaconda3\\lib\\site-packages\\torch\\random.py:40\u001b[0m, in \u001b[0;36mmanual_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[1;32m---> 40\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n",
      "File \u001b[1;32mc:\\Users\\gianl\\anaconda3\\lib\\site-packages\\torch\\cuda\\random.py:113\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m    110\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m    111\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m--> 113\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gianl\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:165\u001b[0m, in \u001b[0;36m_lazy_call\u001b[1;34m(callable, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 165\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[1;32mc:\\Users\\gianl\\anaconda3\\lib\\site-packages\\torch\\cuda\\random.py:111\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[0;32m    110\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m--> 111\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: false INTERNAL ASSERT FAILED at \"C:\\\\actions-runner\\\\_work\\\\pytorch\\\\pytorch\\\\builder\\\\windows\\\\pytorch\\\\c10/cuda/CUDAGraphsC10Utils.h\":74, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus-1851167152"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_full,\n",
    "    args=training_args,\n",
    "    # data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    train_dataset=ds_train_tokenized,\n",
    "    eval_dataset=ds_val_tokenized,\n",
    "    compute_metrics=sequence_f1,\n",
    "    # tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
