{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/Projects/NLP_PROJECT/NLP-project/.venv/lib/python3.8/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/leonardo/Projects/NLP_PROJECT/NLP-project/.venv/lib/python3.8/site-packages/torch/cuda/__init__.py:758: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    # DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "import tqdm.notebook as tq\n",
    "from datasets import Dataset\n",
    "from typing import Tuple\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load corpus\n",
    "\n",
    "data_path = Path(\"data/MELD_train_efr.json\")\n",
    "assert data_path.exists(), \"Data file is not present\"\n",
    "raw_df = pd.read_json(\n",
    "    data_path, dtype={\"speakers\": np.array}\n",
    ")  # , \"triggers\": np.array})\n",
    "EPISODE, SPEAKERS, EMOTIONS, UTTERANCES, TRIGGERS = raw_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>speakers</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>utterance_0</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise]</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>utterance_1</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>utterance_2</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>utterance_3</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utterance_4</td>\n",
       "      <td>[Joey, Rachel, Joey, Rachel]</td>\n",
       "      <td>[surprise, sadness, surprise, fear]</td>\n",
       "      <td>[But then who? The waitress I went out with la...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       episode                                           speakers  \\\n",
       "0  utterance_0  [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "1  utterance_1  [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "2  utterance_2  [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "3  utterance_3  [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "4  utterance_4                       [Joey, Rachel, Joey, Rachel]   \n",
       "\n",
       "                                            emotions  \\\n",
       "0     [neutral, neutral, neutral, neutral, surprise]   \n",
       "1  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "2  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "3  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "4                [surprise, sadness, surprise, fear]   \n",
       "\n",
       "                                          utterances  \\\n",
       "0  [also I was the point person on my company's t...   \n",
       "1  [also I was the point person on my company's t...   \n",
       "2  [also I was the point person on my company's t...   \n",
       "3  [also I was the point person on my company's t...   \n",
       "4  [But then who? The waitress I went out with la...   \n",
       "\n",
       "                                            triggers  \n",
       "0                          [0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4                               [0.0, 0.0, 1.0, 0.0]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>speakers</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4000</td>\n",
       "      <td>3350</td>\n",
       "      <td>3427</td>\n",
       "      <td>3998</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>utterance_0</td>\n",
       "      <td>[Monica, Chandler, Monica]</td>\n",
       "      <td>[neutral, neutral, joy]</td>\n",
       "      <td>[Happy?! Is that what I'm supposed to be Vic? ...</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            episode                    speakers                 emotions  \\\n",
       "count          4000                        4000                     4000   \n",
       "unique         4000                        3350                     3427   \n",
       "top     utterance_0  [Monica, Chandler, Monica]  [neutral, neutral, joy]   \n",
       "freq              1                          15                       30   \n",
       "\n",
       "                                               utterances         triggers  \n",
       "count                                                4000             4000  \n",
       "unique                                               3998              523  \n",
       "top     [Happy?! Is that what I'm supposed to be Vic? ...  [0.0, 1.0, 0.0]  \n",
       "freq                                                    2              191  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups: 832\n",
      "Avg group len: 4.8\n",
      "Longest group: 16\n",
      "Episodes not in a group: 128\n"
     ]
    }
   ],
   "source": [
    "### Look for how many groups of episodes with the same first utterance there are and their lenghts\n",
    "\n",
    "raw_df.sort_values(by=UTTERANCES, inplace=True)\n",
    "groups = np.zeros((850,), dtype=int)\n",
    "\n",
    "index = 0\n",
    "count = 1\n",
    "for i in range(1, len(raw_df[UTTERANCES])):\n",
    "    if raw_df[UTTERANCES][i][0] == raw_df[UTTERANCES][i - 1][0]:\n",
    "        ### still in the same group\n",
    "        count += 1\n",
    "    else:\n",
    "        ### found new group\n",
    "        groups[index] = count\n",
    "        index += 1\n",
    "        count = 1\n",
    "\n",
    "groups = groups[groups != 0]\n",
    "\n",
    "print(f\"Number of groups: {len(groups)}\")\n",
    "print(f\"Avg group len: {np.average(groups):.1f}\")\n",
    "print(f\"Longest group: {np.max(groups)}\")\n",
    "print(f\"Episodes not in a group: {groups[groups == 1].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of number of speakers:\n",
      "1 speakers:  214\n",
      "2 speakers:  2105\n",
      "3 speakers:  1030\n",
      "4 speakers:  405\n",
      "5 speakers:  161\n",
      "6 speakers:  74\n",
      "7 speakers:  10\n",
      "8 speakers:  1\n"
     ]
    }
   ],
   "source": [
    "### Count how many speakers there are in each episode\n",
    "\n",
    "speakers_count = raw_df[SPEAKERS].apply(lambda arr: np.unique(arr).shape[0]).to_numpy()\n",
    "min_sp = np.min(speakers_count)\n",
    "max_sp = np.max(speakers_count)\n",
    "print(\"Distribution of number of speakers:\")\n",
    "for count in range(min_sp, max_sp + 1):\n",
    "    print(f\"{count} speakers:  {np.sum(speakers_count == count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes values:\n",
      "{'neutral': 15263, 'joy': 6317, 'surprise': 4645, 'anger': 3964, 'sadness': 2648, 'fear': 1114, 'disgust': 1049}\n"
     ]
    }
   ],
   "source": [
    "### Class imbalance check\n",
    "classes_count = {}\n",
    "for emotions in raw_df[\"emotions\"]:\n",
    "    for emotion in emotions:\n",
    "        if emotion in classes_count:\n",
    "            classes_count[emotion] += 1\n",
    "        else:\n",
    "            classes_count[emotion] = 1\n",
    "\n",
    "### then we sort the dictionary by occurences\n",
    "emotions_dict = {\n",
    "    k: v\n",
    "    for k, v in sorted(classes_count.items(), key=lambda item: item[1], reverse=True)\n",
    "}\n",
    "print(\"Classes values:\")\n",
    "print(emotions_dict)\n",
    "\n",
    "### Classes counts are not balanced: the use of weights is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop not useful column\n",
    "\n",
    "raw_df.drop(columns=[SPEAKERS], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove Nones from the triggers\n",
    "\n",
    "raw_df[TRIGGERS] = raw_df[TRIGGERS].apply(\n",
    "    lambda trig_seq: np.array([0.0 if t is None else t for t in trig_seq])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>episode_1061</td>\n",
       "      <td>[joy, neutral, surprise]</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>episode_1062</td>\n",
       "      <td>[joy, neutral, surprise, surprise]</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>episode_1063</td>\n",
       "      <td>[joy, neutral, surprise, surprise, neutral]</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>episode_1064</td>\n",
       "      <td>[joy, neutral, surprise, surprise, neutral, ne...</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>episode_1065</td>\n",
       "      <td>[joy, neutral, surprise, surprise, neutral, ne...</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           episode                                           emotions  \\\n",
       "1061  episode_1061                           [joy, neutral, surprise]   \n",
       "1062  episode_1062                 [joy, neutral, surprise, surprise]   \n",
       "1063  episode_1063        [joy, neutral, surprise, surprise, neutral]   \n",
       "1064  episode_1064  [joy, neutral, surprise, surprise, neutral, ne...   \n",
       "1065  episode_1065  [joy, neutral, surprise, surprise, neutral, ne...   \n",
       "\n",
       "                                             utterances  \\\n",
       "1061  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1062  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1063  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1064  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1065  [\"Happy birthday to you!\", You're paying for t...   \n",
       "\n",
       "                                 triggers  \n",
       "1061                      [0.0, 1.0, 0.0]  \n",
       "1062                 [0.0, 0.0, 1.0, 0.0]  \n",
       "1063            [0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1064       [0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1065  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Change column \"episode\" from utterance_xyz to episode_xyz\n",
    "for i in range(len(raw_df)):\n",
    "    raw_df[EPISODE][i] = f\"episode_{raw_df[EPISODE][i][10:]}\"\n",
    "\n",
    "clean_df = raw_df\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If an episode contains the same utterances of the previous and a few more then the triggers from the previous episode are replicated in the current episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 0. 1.]\n",
      "[0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{raw_df[TRIGGERS][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replicate triggers\n",
    "\n",
    "count = 0\n",
    "for i in range(1, len(clean_df)):\n",
    "    is_continuation = np.all(\n",
    "        [u in clean_df[UTTERANCES][i] for u in clean_df[UTTERANCES][i - 1]]\n",
    "    )\n",
    "    if is_continuation:\n",
    "        count += 1\n",
    "        for k, t in enumerate(clean_df[TRIGGERS][i - 1]):\n",
    "            clean_df[TRIGGERS][i][k] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      "[0. 0. 1. 0.]\n",
      "[0. 0. 1. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0. 1.]\n",
      "[0. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{raw_df[TRIGGERS][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Val Test split: 80/10/10\n",
    "\n",
    "\n",
    "def split_data(\n",
    "    df: pd.DataFrame, seed: int = 42\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, test_size=0.2, train_size=0.8, random_state=seed\n",
    "    )\n",
    "\n",
    "    df_val, df_test = train_test_split(\n",
    "        df_test, test_size=0.5, train_size=0.5, random_state=seed\n",
    "    )\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train len: 3200\n",
      "df_val len: 400\n",
      "df_test len: 400\n"
     ]
    }
   ],
   "source": [
    "### Check\n",
    "df_train_t, df_val_t, df_test_t = split_data(clean_df)\n",
    "print(f\"df_train len: {len(df_train_t)}\")\n",
    "print(f\"df_val len: {len(df_val_t)}\")\n",
    "print(f\"df_test len: {len(df_test_t)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explode the dataframe: <br>\n",
    "each rows contains: previous utterance, target utterance, next utterance (for context). <br>\n",
    "Except for the first and lat utterance of each episode that have no previous and next utterance, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_add_context(df: pd.DataFrame, ctxt_win_len: int = 1) -> pd.DataFrame:\n",
    "    # TODO ok che context_window sia il numero di utt future == numero utt passate invece che la len di tutta la window?\n",
    "\n",
    "    ### Flatten the lists of utterances,triggers,emotions into new rows of the dataframe\n",
    "    exploded_df = df.explode([UTTERANCES, TRIGGERS, EMOTIONS], ignore_index=True)\n",
    "    exploded_df.rename(columns={UTTERANCES: \"current\"}, inplace=True)\n",
    "    exploded_df.head(10)\n",
    "\n",
    "    ### Pair shifted columns of utterances to the exploded df to make previous and next\n",
    "    for i in range(1, ctxt_win_len + 1):\n",
    "        padding_cells = pd.Series([\" \" for _ in range(i)])\n",
    "\n",
    "        next_col = pd.concat(\n",
    "            (exploded_df[\"current\"][i:], padding_cells), copy=False\n",
    "        ).to_list()\n",
    "        exploded_df.insert(loc=2, column=f\"next_{i}\", value=next_col)\n",
    "\n",
    "        previous_col = pd.concat(\n",
    "            (padding_cells, exploded_df[\"current\"][:-i]), copy=False\n",
    "        ).to_list()\n",
    "        exploded_df.insert(loc=2 + 2 * i, column=f\"previous_{i}\", value=previous_col)\n",
    "\n",
    "    ### Remove the previous of the first utterance and the next of the last utterance of each episode\n",
    "    for i in range(1, len(exploded_df) - 1):\n",
    "        for j in range(1, ctxt_win_len + 1):\n",
    "            if exploded_df[EPISODE][i] != exploded_df[EPISODE][i - 1]:\n",
    "                exploded_df[f\"next_{j}\"][i - j] = \" \"\n",
    "                exploded_df[f\"previous_{j}\"][i] = \" \"\n",
    "\n",
    "    exploded_df.sort_values(by=EPISODE, inplace=True)\n",
    "    return exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>emotions</th>\n",
       "      <th>next_1</th>\n",
       "      <th>current</th>\n",
       "      <th>previous_1</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>surprise</td>\n",
       "      <td></td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24390</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24389</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24391</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24395</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td></td>\n",
       "      <td>I see.</td>\n",
       "      <td>Now you'll be heading a whole division, so you...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24392</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         episode  emotions                                         next_1  \\\n",
       "864    episode_0   neutral               You must've had your hands full.   \n",
       "868    episode_0  surprise                                                  \n",
       "867    episode_0   neutral                         My duties?  All right.   \n",
       "866    episode_0   neutral  So let's talk a little bit about your duties.   \n",
       "865    episode_0   neutral                        That I did. That I did.   \n",
       "24390  episode_1   neutral                        That I did. That I did.   \n",
       "24389  episode_1   neutral               You must've had your hands full.   \n",
       "24391  episode_1   neutral  So let's talk a little bit about your duties.   \n",
       "24395  episode_1   neutral                                                  \n",
       "24392  episode_1   neutral                         My duties?  All right.   \n",
       "\n",
       "                                                 current  \\\n",
       "864    also I was the point person on my company's tr...   \n",
       "868                               My duties?  All right.   \n",
       "867        So let's talk a little bit about your duties.   \n",
       "866                              That I did. That I did.   \n",
       "865                     You must've had your hands full.   \n",
       "24390                   You must've had your hands full.   \n",
       "24389  also I was the point person on my company's tr...   \n",
       "24391                            That I did. That I did.   \n",
       "24395                                             I see.   \n",
       "24392      So let's talk a little bit about your duties.   \n",
       "\n",
       "                                              previous_1 triggers  \n",
       "864                                                           0.0  \n",
       "868        So let's talk a little bit about your duties.      0.0  \n",
       "867                              That I did. That I did.      1.0  \n",
       "866                     You must've had your hands full.      0.0  \n",
       "865    also I was the point person on my company's tr...      0.0  \n",
       "24390  also I was the point person on my company's tr...      0.0  \n",
       "24389                                                         0.0  \n",
       "24391                   You must've had your hands full.      0.0  \n",
       "24395  Now you'll be heading a whole division, so you...      0.0  \n",
       "24392                            That I did. That I did.      1.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = explode_add_context(df_train_t, 1)\n",
    "df_val = explode_add_context(df_val_t, 1)\n",
    "df_test = explode_add_context(df_test_t, 1)\n",
    "\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>emotions</th>\n",
       "      <th>next_2</th>\n",
       "      <th>next_1</th>\n",
       "      <th>current</th>\n",
       "      <th>previous_1</th>\n",
       "      <th>previous_2</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td></td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td></td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24390</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>Hello, Joey.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24389</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24391</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24395</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Listen to the plinky-plunky music.</td>\n",
       "      <td></td>\n",
       "      <td>I see.</td>\n",
       "      <td>Now you'll be heading a whole division, so you...</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24392</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Now you'll be heading a whole division, so you...</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         episode  emotions                                             next_2  \\\n",
       "864    episode_0   neutral                            That I did. That I did.   \n",
       "868    episode_0  surprise  Yeah, it kinda grows on you.  Actually, I want...   \n",
       "867    episode_0   neutral                                                      \n",
       "866    episode_0   neutral                             My duties?  All right.   \n",
       "865    episode_0   neutral      So let's talk a little bit about your duties.   \n",
       "24390  episode_1   neutral      So let's talk a little bit about your duties.   \n",
       "24389  episode_1   neutral                            That I did. That I did.   \n",
       "24391  episode_1   neutral                             My duties?  All right.   \n",
       "24395  episode_1   neutral                 Listen to the plinky-plunky music.   \n",
       "24392  episode_1   neutral  Now you'll be heading a whole division, so you...   \n",
       "\n",
       "                                              next_1  \\\n",
       "864                 You must've had your hands full.   \n",
       "868                                                    \n",
       "867                           My duties?  All right.   \n",
       "866    So let's talk a little bit about your duties.   \n",
       "865                          That I did. That I did.   \n",
       "24390                        That I did. That I did.   \n",
       "24389               You must've had your hands full.   \n",
       "24391  So let's talk a little bit about your duties.   \n",
       "24395                                                  \n",
       "24392                         My duties?  All right.   \n",
       "\n",
       "                                                 current  \\\n",
       "864    also I was the point person on my company's tr...   \n",
       "868                               My duties?  All right.   \n",
       "867        So let's talk a little bit about your duties.   \n",
       "866                              That I did. That I did.   \n",
       "865                     You must've had your hands full.   \n",
       "24390                   You must've had your hands full.   \n",
       "24389  also I was the point person on my company's tr...   \n",
       "24391                            That I did. That I did.   \n",
       "24395                                             I see.   \n",
       "24392      So let's talk a little bit about your duties.   \n",
       "\n",
       "                                              previous_1  \\\n",
       "864                                                        \n",
       "868        So let's talk a little bit about your duties.   \n",
       "867                              That I did. That I did.   \n",
       "866                     You must've had your hands full.   \n",
       "865    also I was the point person on my company's tr...   \n",
       "24390  also I was the point person on my company's tr...   \n",
       "24389                                                      \n",
       "24391                   You must've had your hands full.   \n",
       "24395  Now you'll be heading a whole division, so you...   \n",
       "24392                            That I did. That I did.   \n",
       "\n",
       "                                              previous_2 triggers  \n",
       "864                                                           0.0  \n",
       "868                              That I did. That I did.      0.0  \n",
       "867                     You must've had your hands full.      1.0  \n",
       "866    also I was the point person on my company's tr...      0.0  \n",
       "865    You-you-you didn't know that.  Well, I guess m...      0.0  \n",
       "24390                                       Hello, Joey.      0.0  \n",
       "24389                                                         0.0  \n",
       "24391  also I was the point person on my company's tr...      0.0  \n",
       "24395                             My duties?  All right.      0.0  \n",
       "24392                   You must've had your hands full.      1.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = explode_add_context(df_train_t, 2)\n",
    "dff.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>emotions</th>\n",
       "      <th>next_3</th>\n",
       "      <th>next_2</th>\n",
       "      <th>next_1</th>\n",
       "      <th>current</th>\n",
       "      <th>previous_1</th>\n",
       "      <th>previous_2</th>\n",
       "      <th>previous_3</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Oh good.</td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td></td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td></td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td></td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>That's why you broke up with me?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24390</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>Hello, Joey.</td>\n",
       "      <td>Sorry. Wrong boobies.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         episode  emotions                                             next_3  \\\n",
       "864    episode_0   neutral      So let's talk a little bit about your duties.   \n",
       "868    episode_0  surprise                                           Oh good.   \n",
       "867    episode_0   neutral  Yeah, it kinda grows on you.  Actually, I want...   \n",
       "866    episode_0   neutral                                                      \n",
       "865    episode_0   neutral                             My duties?  All right.   \n",
       "24390  episode_1   neutral                             My duties?  All right.   \n",
       "\n",
       "                                                  next_2  \\\n",
       "864                              That I did. That I did.   \n",
       "868    Yeah, it kinda grows on you.  Actually, I want...   \n",
       "867                                                        \n",
       "866                               My duties?  All right.   \n",
       "865        So let's talk a little bit about your duties.   \n",
       "24390      So let's talk a little bit about your duties.   \n",
       "\n",
       "                                              next_1  \\\n",
       "864                 You must've had your hands full.   \n",
       "868                                                    \n",
       "867                           My duties?  All right.   \n",
       "866    So let's talk a little bit about your duties.   \n",
       "865                          That I did. That I did.   \n",
       "24390                        That I did. That I did.   \n",
       "\n",
       "                                                 current  \\\n",
       "864    also I was the point person on my company's tr...   \n",
       "868                               My duties?  All right.   \n",
       "867        So let's talk a little bit about your duties.   \n",
       "866                              That I did. That I did.   \n",
       "865                     You must've had your hands full.   \n",
       "24390                   You must've had your hands full.   \n",
       "\n",
       "                                              previous_1  \\\n",
       "864                                                        \n",
       "868        So let's talk a little bit about your duties.   \n",
       "867                              That I did. That I did.   \n",
       "866                     You must've had your hands full.   \n",
       "865    also I was the point person on my company's tr...   \n",
       "24390  also I was the point person on my company's tr...   \n",
       "\n",
       "                                              previous_2  \\\n",
       "864                                                        \n",
       "868                              That I did. That I did.   \n",
       "867                     You must've had your hands full.   \n",
       "866    also I was the point person on my company's tr...   \n",
       "865    You-you-you didn't know that.  Well, I guess m...   \n",
       "24390                                       Hello, Joey.   \n",
       "\n",
       "                                              previous_3 triggers  \n",
       "864                                                           0.0  \n",
       "868                     You must've had your hands full.      0.0  \n",
       "867    also I was the point person on my company's tr...      1.0  \n",
       "866    You-you-you didn't know that.  Well, I guess m...      0.0  \n",
       "865                     That's why you broke up with me?      0.0  \n",
       "24390                              Sorry. Wrong boobies.      0.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = explode_add_context(df_train_t, 3)\n",
    "dff.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>emotions</th>\n",
       "      <th>next_5</th>\n",
       "      <th>next_4</th>\n",
       "      <th>next_3</th>\n",
       "      <th>next_2</th>\n",
       "      <th>next_1</th>\n",
       "      <th>current</th>\n",
       "      <th>previous_1</th>\n",
       "      <th>previous_2</th>\n",
       "      <th>previous_3</th>\n",
       "      <th>previous_4</th>\n",
       "      <th>previous_5</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td></td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>surprise</td>\n",
       "      <td>So, I'm sorry I just don't think we should go ...</td>\n",
       "      <td>Look, I  Look, I'm having a great time with y...</td>\n",
       "      <td>Oh good.</td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td></td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Look, I  Look, I'm having a great time with y...</td>\n",
       "      <td>Oh good.</td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td></td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>That's why you broke up with me?</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Oh good.</td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td></td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>That's why you broke up with me?</td>\n",
       "      <td>Ah, uh, I owe you a long overdue apology. I ne...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>episode_0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Yeah, it kinda grows on you.  Actually, I want...</td>\n",
       "      <td></td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>You-you-you didn't know that.  Well, I guess m...</td>\n",
       "      <td>That's why you broke up with me?</td>\n",
       "      <td>Ah, uh, I owe you a long overdue apology. I ne...</td>\n",
       "      <td>It's pretty clear.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24390</th>\n",
       "      <td>episode_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I see.</td>\n",
       "      <td>Now you'll be heading a whole division, so you...</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>So let's talk a little bit about your duties.</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>You must've had your hands full.</td>\n",
       "      <td>also I was the point person on my company's tr...</td>\n",
       "      <td>Hello, Joey.</td>\n",
       "      <td>Sorry. Wrong boobies.</td>\n",
       "      <td>Joey!! What the hell were you doing?!</td>\n",
       "      <td>Clear the tracks for the boobie payback expres...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         episode  emotions                                             next_5  \\\n",
       "864    episode_0   neutral                                                      \n",
       "868    episode_0  surprise  So, I'm sorry I just don't think we should go ...   \n",
       "867    episode_0   neutral  Look, I\n",
       "  Look, I'm having a great time with y...   \n",
       "866    episode_0   neutral                                           Oh good.   \n",
       "865    episode_0   neutral  Yeah, it kinda grows on you.  Actually, I want...   \n",
       "24390  episode_1   neutral                                             I see.   \n",
       "\n",
       "                                                  next_4  \\\n",
       "864                               My duties?  All right.   \n",
       "868    Look, I\n",
       "  Look, I'm having a great time with y...   \n",
       "867                                             Oh good.   \n",
       "866    Yeah, it kinda grows on you.  Actually, I want...   \n",
       "865                                                        \n",
       "24390  Now you'll be heading a whole division, so you...   \n",
       "\n",
       "                                                  next_3  \\\n",
       "864        So let's talk a little bit about your duties.   \n",
       "868                                             Oh good.   \n",
       "867    Yeah, it kinda grows on you.  Actually, I want...   \n",
       "866                                                        \n",
       "865                               My duties?  All right.   \n",
       "24390                             My duties?  All right.   \n",
       "\n",
       "                                                  next_2  \\\n",
       "864                              That I did. That I did.   \n",
       "868    Yeah, it kinda grows on you.  Actually, I want...   \n",
       "867                                                        \n",
       "866                               My duties?  All right.   \n",
       "865        So let's talk a little bit about your duties.   \n",
       "24390      So let's talk a little bit about your duties.   \n",
       "\n",
       "                                              next_1  \\\n",
       "864                 You must've had your hands full.   \n",
       "868                                                    \n",
       "867                           My duties?  All right.   \n",
       "866    So let's talk a little bit about your duties.   \n",
       "865                          That I did. That I did.   \n",
       "24390                        That I did. That I did.   \n",
       "\n",
       "                                                 current  \\\n",
       "864    also I was the point person on my company's tr...   \n",
       "868                               My duties?  All right.   \n",
       "867        So let's talk a little bit about your duties.   \n",
       "866                              That I did. That I did.   \n",
       "865                     You must've had your hands full.   \n",
       "24390                   You must've had your hands full.   \n",
       "\n",
       "                                              previous_1  \\\n",
       "864                                                        \n",
       "868        So let's talk a little bit about your duties.   \n",
       "867                              That I did. That I did.   \n",
       "866                     You must've had your hands full.   \n",
       "865    also I was the point person on my company's tr...   \n",
       "24390  also I was the point person on my company's tr...   \n",
       "\n",
       "                                              previous_2  \\\n",
       "864                                                        \n",
       "868                              That I did. That I did.   \n",
       "867                     You must've had your hands full.   \n",
       "866    also I was the point person on my company's tr...   \n",
       "865    You-you-you didn't know that.  Well, I guess m...   \n",
       "24390                                       Hello, Joey.   \n",
       "\n",
       "                                              previous_3  \\\n",
       "864                                                        \n",
       "868                     You must've had your hands full.   \n",
       "867    also I was the point person on my company's tr...   \n",
       "866    You-you-you didn't know that.  Well, I guess m...   \n",
       "865                     That's why you broke up with me?   \n",
       "24390                              Sorry. Wrong boobies.   \n",
       "\n",
       "                                              previous_4  \\\n",
       "864                                                        \n",
       "868    also I was the point person on my company's tr...   \n",
       "867    You-you-you didn't know that.  Well, I guess m...   \n",
       "866                     That's why you broke up with me?   \n",
       "865    Ah, uh, I owe you a long overdue apology. I ne...   \n",
       "24390              Joey!! What the hell were you doing?!   \n",
       "\n",
       "                                              previous_5 triggers  \n",
       "864                                                           0.0  \n",
       "868    You-you-you didn't know that.  Well, I guess m...      0.0  \n",
       "867                     That's why you broke up with me?      1.0  \n",
       "866    Ah, uh, I owe you a long overdue apology. I ne...      0.0  \n",
       "865                                   It's pretty clear.      0.0  \n",
       "24390  Clear the tracks for the boobie payback expres...      0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = explode_add_context(df_train_t, 5)\n",
    "dff.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Given 2 series of sequences of triggers/emotions compute F1 inside each sequence and return avg,\n",
    "\n",
    "# TODO remove\n",
    "# def sequence_f1(y_true, y_pred, avg: bool = True):\n",
    "#     res = [\n",
    "#         f1_score(y_true=y_t, y_pred=y_p, average=\"micro\")\n",
    "#         for y_t, y_p in zip(y_true, y_pred)\n",
    "#     ]\n",
    "#     return np.average(res) if avg else res\n",
    "\n",
    "\n",
    "### Compute F1 score for each flattened dialogue and return avg over dialogues\n",
    "def sequence_f1(\n",
    "    y_true: pd.DataFrame,\n",
    "    y_pred: np.ndarray,\n",
    "    target_column: str,\n",
    "    avg: bool = True,\n",
    "):\n",
    "    assert len(y_pred) == len(y_true), \"y_pred and y_true must be of the same lenght\"\n",
    "    assert (\n",
    "        y_true[EPISODE].is_monotonic_increasing\n",
    "        or y_true[EPISODE].is_monotonic_decreasing\n",
    "    ), \"utterances must be sorted over the episodes\"\n",
    "\n",
    "    res = {}\n",
    "    start = 0\n",
    "    stop_incl = 0\n",
    "    for i in range(1, len(y_pred)):\n",
    "        if y_true[EPISODE][i - 1] != y_true[EPISODE][i]:\n",
    "            stop_incl = i - 1\n",
    "            f1 = f1_score(\n",
    "                y_true=y_true[target_column][start : stop_incl + 1].to_list(),\n",
    "                y_pred=y_pred[start : stop_incl + 1],\n",
    "                average=\"micro\",\n",
    "            )\n",
    "            res.update({y_true[EPISODE][start]: f1})\n",
    "            start = i\n",
    "\n",
    "    # np.std(list(res.values))\n",
    "    return res if not avg else np.average(list(res.values()))\n",
    "\n",
    "\n",
    "### Compute F1 score for the unrolled sequence\n",
    "def unrolled_f1(\n",
    "    y_true: pd.DataFrame,\n",
    "    y_pred: np.ndarray,\n",
    "    target_column: str,\n",
    "):\n",
    "    return f1_score(y_true[target_column].to_list(), y_pred, average=\"micro\")\n",
    "\n",
    "\n",
    "# TODO remove\n",
    "# def unrolled_f1(y_true, y_pred):\n",
    "#     y_t_flat = []\n",
    "#     for l in y_true:\n",
    "#         for e in l:\n",
    "#             y_t_flat.append(e)\n",
    "#\n",
    "#     y_p_flat = []\n",
    "#     for l in y_pred:\n",
    "#         for e in l:\n",
    "#             y_p_flat.append(e)\n",
    "#\n",
    "#     return f1_score(y_true=y_t_flat, y_pred=y_p_flat, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create baseline models\n",
    "\n",
    "\n",
    "# TODO do we still need the class?\n",
    "class SequenceDummyClassifier(DummyClassifier):\n",
    "    def __init__(self, strategy: str, seed: int = 42) -> None:\n",
    "        self.seed = seed\n",
    "        if not strategy.lower() in (\"random\", \"majority\"):\n",
    "            raise ValueError(\"strategy must be in [random, majority]\")\n",
    "        sklearn_strategy = \"uniform\" if strategy == \"random\" else \"most_frequent\"\n",
    "        super().__init__(strategy=sklearn_strategy, random_state=seed)\n",
    "\n",
    "    # TODO remove\n",
    "    # def _flatten_seq(self, df: pd.Series):\n",
    "    #     res = []\n",
    "    #     for l in df:\n",
    "    #         for e in l:\n",
    "    #             res.append(e)\n",
    "    #     return res\n",
    "\n",
    "    # TODO remove\n",
    "    # def _deflatten_seq(self, seq, shape_like: pd.Series):\n",
    "    #     data = iter(seq)\n",
    "    #     result = [[next(data) for _ in s] for s in shape_like]\n",
    "    #     return result\n",
    "\n",
    "    # TODO remove\n",
    "    # def fit(self, X: pd.Series, y: pd.Series):\n",
    "    #     X_flat = self._flatten_seq(X)\n",
    "    #     y_flat = self._flatten_seq(y)\n",
    "    #     super().fit(X=X_flat, y=y_flat)\n",
    "\n",
    "    # TODO remove\n",
    "    # def predict(self, X: pd.Series, return_flat: bool = False):\n",
    "    #     X_flat = self._flatten_seq(X)\n",
    "    #     y_flat = super().predict(X_flat)\n",
    "    #     return y_flat if return_flat else self._deflatten_seq(seq=y_flat, shape_like=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_f1(emotions_Random) : 0.44657433160540777\n",
      "unrolled_f1(emotions_Random) : 0.4330935251798561\n",
      "sequence_f1(triggers_Random) : 0.6522845341422358\n",
      "unrolled_f1(triggers_Random) : 0.6515107913669065\n",
      "sequence_f1(emotions_Majority) : 0.44657433160540777\n",
      "unrolled_f1(emotions_Majority) : 0.4330935251798561\n",
      "sequence_f1(triggers_Majority) : 0.6522845341422358\n",
      "unrolled_f1(triggers_Majority) : 0.6515107913669065\n"
     ]
    }
   ],
   "source": [
    "def experiment_baseline(df_train: pd.DataFrame, df_test: pd.DataFrame, seed: int = 42):\n",
    "\n",
    "    baseline_f1s = {}\n",
    "    baseline_results = {}\n",
    "\n",
    "    for strategy in (\"Random\", \"Majority\"):\n",
    "        for target in (EMOTIONS, TRIGGERS):\n",
    "            clf = SequenceDummyClassifier(strategy=strategy, seed=seed)\n",
    "            clf.fit(X=df_train[\"current\"], y=df_train[target])\n",
    "\n",
    "            res = clf.predict(X=df_test[\"current\"])\n",
    "            baseline_results.update({f\"{target}_{strategy}\": res})\n",
    "\n",
    "            seq_f1 = sequence_f1(y_true=df_test, y_pred=res, target_column=target)\n",
    "            baseline_f1s.update({f\"sequence_f1({target}_{strategy})\": seq_f1})\n",
    "\n",
    "            unr_f1 = unrolled_f1(y_true=df_test, y_pred=res, target_column=target)\n",
    "            baseline_f1s.update({f\"unrolled_f1({target}_{strategy})\": unr_f1})\n",
    "\n",
    "    return baseline_f1s, baseline_results\n",
    "\n",
    "\n",
    "f1s, results = experiment_baseline(df_train, df_test)\n",
    "for k, v in f1s.items():\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'd': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'e': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'f': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'g': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'i': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'j': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'l': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), 'n': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]), 'o': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]), 'p': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]), 'r': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), 's': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), 't': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), 'u': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), 'y': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "all_emotions = []\n",
    "for seq in df_train[EMOTIONS]:\n",
    "    for e in seq:\n",
    "        all_emotions.append(e)\n",
    "uniq_emotions = np.sort(np.unique(all_emotions))\n",
    "one_hot = np.identity(len(uniq_emotions))\n",
    "emotion_mapping = {e: one_hot[i] for i, e in enumerate(uniq_emotions)}\n",
    "print(emotion_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(ds_row, tokenizer=tokenizer):\n",
    "\n",
    "    if type(ds_row[\"emotions\"][0]) != str:  ### batchsize > 1\n",
    "        emotion_encoding = []\n",
    "        emotion_encoding = [\n",
    "            [emotion_mapping[e] for e in emotions_of_one_utterance]\n",
    "            for emotions_of_one_utterance in ds_row[\"emotions\"]\n",
    "        ]\n",
    "    else:  ### batchsize == 1\n",
    "        emotion_encoding = [emotion_mapping[e] for e in ds_row[\"emotions\"]]\n",
    "\n",
    "    encoded_ds_row = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        # \"emotions\": ds_row[\"emotions\"],\n",
    "        \"emotions\": emotion_encoding,\n",
    "        \"triggers\": ds_row[\"triggers\"],\n",
    "    }\n",
    "\n",
    "    for sentence in ds_row[\"utterances\"]:\n",
    "\n",
    "        tokenized_sentence = tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length // 4,\n",
    "            return_tensors=\"pt\",\n",
    "            # is_split_into_words=True,\n",
    "        )\n",
    "        encoded_ds_row[\"input_ids\"].append(tokenized_sentence[\"input_ids\"])\n",
    "        encoded_ds_row[\"token_type_ids\"].append(tokenized_sentence[\"token_type_ids\"])\n",
    "        encoded_ds_row[\"attention_mask\"].append(tokenized_sentence[\"attention_mask\"])\n",
    "\n",
    "    return encoded_ds_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_pandas(df_train)\n",
    "ds_val = Dataset.from_pandas(df_val)\n",
    "ds_test = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column to remove ['utterances'] not in the dataset. Current columns in the dataset: ['episode', 'emotions', 'next_1', 'current', 'previous_1', 'triggers', '__index_level_0__']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Apply tokenization\u001b[39;00m\n\u001b[1;32m      2\u001b[0m batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ds_train_tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mds_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mUTTERANCES\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m ds_train_tokenized\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m ds_val_tokenized \u001b[38;5;241m=\u001b[39m ds_val\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     11\u001b[0m     function\u001b[38;5;241m=\u001b[39mtokenize,\n\u001b[1;32m     12\u001b[0m     fn_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokenizer},\n\u001b[1;32m     13\u001b[0m     batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m     14\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m[UTTERANCES],\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/NLP_PROJECT/NLP-project/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/NLP_PROJECT/NLP-project/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/NLP_PROJECT/NLP-project/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:3025\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3023\u001b[0m     missing_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(remove_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m   3024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[0;32m-> 3025\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3026\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn to remove \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3027\u001b[0m         )\n\u001b[1;32m   3029\u001b[0m load_from_cache_file \u001b[38;5;241m=\u001b[39m load_from_cache_file \u001b[38;5;28;01mif\u001b[39;00m load_from_cache_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_caching_enabled()\n\u001b[1;32m   3031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Column to remove ['utterances'] not in the dataset. Current columns in the dataset: ['episode', 'emotions', 'next_1', 'current', 'previous_1', 'triggers', '__index_level_0__']"
     ]
    }
   ],
   "source": [
    "### Apply tokenization\n",
    "batched = True\n",
    "ds_train_tokenized = ds_train.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=batched,\n",
    "    remove_columns=[UTTERANCES],\n",
    ")\n",
    "ds_train_tokenized.set_format(type=\"torch\")\n",
    "ds_val_tokenized = ds_val.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=batched,\n",
    "    remove_columns=[UTTERANCES],\n",
    ")\n",
    "ds_val_tokenized.set_format(type=\"torch\")\n",
    "ds_test_tokenized = ds_test.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=batched,\n",
    "    remove_columns=[UTTERANCES],\n",
    ")\n",
    "ds_test_tokenized.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_train_tokenized[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_train_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_train[UTTERANCES][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_row = tokenize(ds_train[0])\n",
    "# print(tokenized_row[\"input_ids\"][0])\n",
    "print(tokenizer.batch_decode(tokenized_row[\"input_ids\"][0]))\n",
    "print(tokenizer.batch_decode(tokenized_row[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test della funzione\n",
    "tokens = tokenize(ds_train[0], tokenizer)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, num_emotions=7):\n",
    "        super(BERTClass, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        # self.lstm = torch.nn.LSTM()\n",
    "        # classifiers\n",
    "        self.l_emotions = torch.nn.Linear(self.bert.config.hidden_size, num_emotions)\n",
    "        self.l_triggers = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        print(ids.shape)\n",
    "        print(ids.reshape(-1, ids.size(-1)))\n",
    "        output = self.bert(ids, attention_mask=mask)  # , token_type_ids=token_type_ids)\n",
    "\n",
    "        output_emotions = self.dropout(output)\n",
    "        output_triggers = self.dropout(output)\n",
    "\n",
    "        # output_triggers = self.lstm(output_triggers)\n",
    "\n",
    "        output_emotions = self.l_emotions(output_emotions)\n",
    "        output_triggers = self.l_triggers(output_triggers)\n",
    "\n",
    "        return output_emotions, output_triggers\n",
    "\n",
    "    def freeze_params(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_emotions = df[\"emotions\"].explode().nunique()\n",
    "\n",
    "model_frozen = BERTClass(num_emotions)\n",
    "model_full = BERTClass(num_emotions)\n",
    "\n",
    "model_frozen.freeze_params()\n",
    "\n",
    "# Verifying that the params are actually frozen\n",
    "for name, param in model_frozen.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "\n",
    "for name, param in model_full.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [model_full, model_frozen]\n",
    "num_epochs = 5\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "for model in model_list:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    # Tokenizer initiation\n",
    "    # TODO Check tokenizer parameters\n",
    "    # TODO A Dataloader seems to be commonly used for this use cases\n",
    "    # encoding = tokenizer(df_train, truncation=False, padding='max_length', return_tensors='pt')\n",
    "    # input_ids = encoding['input_ids']\n",
    "    # attention_mask = encoding['attention_mask']\n",
    "\n",
    "    # Training loop\n",
    "    # for epoch in range(num_epochs):\n",
    "    #\n",
    "    #    for idx in range(len(df_test)):\n",
    "\n",
    "    #        text = df_train[idx].drop(TRIGGERS)\n",
    "    #        label = df_train[idx][TRIGGERS]\n",
    "\n",
    "    #        optimizer.zero_grad()\n",
    "\n",
    "    #        logits = model(batch_data)\n",
    "    #        loss = loss_fn(logits, batch_labels)\n",
    "    #        loss.backward()\n",
    "    #        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs_emotions, outputs_triggers, emotions_labels, triggers_labels):\n",
    "    return torch.nn.CrossEntropyLoss(\n",
    "        outputs_emotions, emotions_labels\n",
    "    ) + torch.nn.BCELoss(outputs_triggers, triggers_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training of the model\n",
    "def train_model(train_dl, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions_emotions = 0\n",
    "    correct_predictions_triggers = 0\n",
    "    num_samples_emotions = 0\n",
    "    num_samples_triggers = 0\n",
    "\n",
    "    ### activate dropout, batch norm\n",
    "    model.train()\n",
    "\n",
    "    ### initialize progress bar\n",
    "    batches = tq.tqdm(\n",
    "        enumerate(train_dl), total=len(train_dl), leave=True, colour=\"steelblue\"\n",
    "    )\n",
    "\n",
    "    for batch_idx, data in batches:\n",
    "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "        emotions_labels = data[\"emotions\"].to(device, dtype=torch.float)\n",
    "        triggers_labels = data[\"triggers\"].to(device, dtype=torch.float)\n",
    "        outputs_emotions, outputs_triggers = model(\n",
    "            ids, mask, token_type_ids\n",
    "        )  ### Forward\n",
    "\n",
    "        loss = loss_fn(\n",
    "            outputs_emotions, outputs_triggers, emotions_labels, triggers_labels\n",
    "        )\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        ### apply thresh 0.5\n",
    "        outputs_emotions = (\n",
    "            torch.sigmoid(outputs_emotions).cpu().detach().numpy().round()\n",
    "        )\n",
    "        outputs_triggers = (\n",
    "            torch.sigmoid(outputs_triggers).cpu().detach().numpy().round()\n",
    "        )\n",
    "\n",
    "        emotions_labels = emotions_labels.cpu().detach().numpy()\n",
    "        triggers_labels = triggers_labels.cpu().detach().numpy()\n",
    "\n",
    "        correct_predictions_emotions += np.sum(outputs_emotions == emotions_labels)\n",
    "        correct_predictions_triggers += np.sum(outputs_triggers == triggers_labels)\n",
    "\n",
    "        num_samples_emotions += emotions_labels.size\n",
    "        num_samples_triggers += triggers_labels.size\n",
    "\n",
    "        ### Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        ### Grad descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        ### Update progress bar\n",
    "        batches.set_description(f\"\")\n",
    "        batches.set_postfix(batch_loss=loss)\n",
    "\n",
    "    # Si potrebbe fare una singola accuracy come media delle due, magari fuori dal training\n",
    "    accuracy_emotions = float(correct_predictions_emotions) / num_samples_emotions\n",
    "    accuracy_triggers = float(correct_predictions_triggers) / num_samples_triggers\n",
    "\n",
    "    return model, accuracy_emotions, accuracy_triggers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval model, setup e train_eval da definire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(validation_dl, model):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    num_categories = next(iter(validation_dl))[\"labels\"].shape[1]\n",
    "\n",
    "    ### accumulate data over each batch to compute the f1\n",
    "    true_positives = np.array([0 for _ in range(num_categories)])\n",
    "    false_positives = np.array([0 for _ in range(num_categories)])\n",
    "    false_negatives = np.array([0 for _ in range(num_categories)])\n",
    "\n",
    "    ### turn off dropout, fix batch norm\n",
    "    model.eval()\n",
    "\n",
    "    ### show progress bar\n",
    "    batches = tq.tqdm(\n",
    "        enumerate(validation_dl),\n",
    "        total=len(validation_dl),\n",
    "        leave=True,\n",
    "        colour=\"steelblue\",\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in batches:\n",
    "            ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "            mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            emotions_labels = data[\"emotions\"].to(device, dtype=torch.float)\n",
    "            triggers_labels = data[\"triggers\"].to(device, dtype=torch.float)\n",
    "            outputs_emotions, outputs_triggers = model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss = loss_fn(\n",
    "                outputs_emotions, outputs_triggers, emotions_labels, triggers_labels\n",
    "            )\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "            ### validation accuracy\n",
    "            ### training sigmoid is in BCEWithLogitsLoss\n",
    "            outputs_emotions = (\n",
    "                torch.sigmoid(outputs_emotions).cpu().detach().numpy().round()\n",
    "            )\n",
    "            outputs_triggers = (\n",
    "                torch.sigmoid(outputs_triggers).cpu().detach().numpy().round()\n",
    "            )\n",
    "\n",
    "            emotions_labels = emotions_labels.cpu().detach().numpy()\n",
    "            triggers_labels = triggers_labels.cpu().detach().numpy()\n",
    "            correct_predictions_emotions += np.sum(outputs_emotions == emotions_labels)\n",
    "            correct_predictions_triggers += np.sum(outputs_triggers == triggers_labels)\n",
    "\n",
    "            num_samples_emotions += emotions_labels.size\n",
    "            num_samples_triggers += triggers_labels.size\n",
    "\n",
    "        accuracy_emotions = float(correct_predictions_emotions) / num_samples_emotions\n",
    "        accuracy_triggers = float(correct_predictions_triggers) / num_samples_triggers\n",
    "        # precision = true_positives / (true_positives + false_positives)\n",
    "        # recall = true_positives / (true_positives + false_negatives)\n",
    "        # f1_per_cat = 2 * (precision * recall) / (precision + recall)\n",
    "        # f1_overall = np.mean(f1_per_cat)\n",
    "\n",
    "    return accuracy_emotions, accuracy_triggers, losses  # , f1_overall, f1_per_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(\n",
    "    train_dl,\n",
    "    validation_dl,\n",
    "    model,\n",
    "    optimizer,\n",
    "    n_epochs=1,\n",
    "    save_name=\"0\",\n",
    "    train_model_f=train_model,\n",
    "    eval_model_f=eval_model,\n",
    "):\n",
    "    model_folder = Path.cwd().joinpath(\"models\")\n",
    "    if not model_folder.exists():\n",
    "        model_folder.mkdir(parents=True)\n",
    "\n",
    "    history = {}\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "        model, accuracy_emotions, accuracy_triggers, train_loss = train_model_f(\n",
    "            train_dl, model, optimizer\n",
    "        )\n",
    "        val_accuracy_emotions, val_accuracy_triggers, val_loss = eval_model_f(\n",
    "            validation_dl, model\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"train_loss={np.mean(train_loss):.4f}, val_loss={np.mean(val_loss):.4f}, \"\n",
    "            f\"train_acc_emo={accuracy_emotions:.4f}, train_acc_emo={accuracy_triggers:.4f},\"\n",
    "            f\"val_acc_emo={val_accuracy_emotions:.4f}, val_acc_tri={val_accuracy_triggers:.4f}, \"\n",
    "        )\n",
    "\n",
    "        history.update({\"train_acc_emo\": accuracy_emotions})\n",
    "        history.update({\"train_acc_tri\": accuracy_triggers})\n",
    "        history.update({\"train_losses\": train_loss})\n",
    "        history.update({\"val_acc_emo\": val_accuracy_emotions})\n",
    "        history.update({\"val_acc_emo\": val_accuracy_triggers})\n",
    "        history.update({\"val_losses\": val_loss})\n",
    "        # history.update({\"f1_overall\": f1_overall})\n",
    "        # history.update({\"f1_per_cat\": f1_per_cat})\n",
    "\n",
    "        ### save the best model\n",
    "        # if f1_overall > best_f1:\n",
    "        #   torch.save(\n",
    "        #      model.state_dict(),\n",
    "        #      Path.joinpath(model_folder, f\"model_{save_name}.bin\"),\n",
    "        # )\n",
    "        # best_f1 = f1_overall\n",
    "\n",
    "    return history  # (history[\"f1_overall\"], history[\"f1_per_cat\"], history[\"train_losses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(tokenized_datasets, batch_size):\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"train\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    validation_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"validation\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    test_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"test\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return train_dl, validation_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    ds_train_tokenized,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "validation_dl = torch.utils.data.DataLoader(\n",
    "    ds_val_tokenized,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Custom loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_eval(\n",
    "    train_dl,\n",
    "    validation_dl,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    save_name=f\"test_model\",\n",
    ")\n",
    "# history_list_c_lr3.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor [[utt + pad], [utt + pad], [utt + pad]]\n",
    "tensor [CLS + utt + SEP + utt + SEP + utt + SEP + pad]\n",
    "[ tensor[cls utt padd] tensor [cls utt padd]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training con Classe Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainer(Trainer):\n",
    "    def __init__(self, model, training_args, train_ds, eval_ds, metrics):\n",
    "        super().__init__(model, training_args, train_ds, eval_ds, metrics)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        emotions_labels = inputs[\"emotions\"]\n",
    "        triggers_labels = inputs[\"triggers\"]\n",
    "        output_emotions, output_triggers = model(ids, mask)\n",
    "\n",
    "        custom_loss = self.loss_fn(\n",
    "            output_emotions, output_triggers, emotions_labels, triggers_labels\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            (custom_loss, output_emotions, output_triggers)\n",
    "            if return_outputs\n",
    "            else custom_loss\n",
    "        )\n",
    "\n",
    "    def loss_fn(outputs_emotions, outputs_triggers, emotions_labels, triggers_labels):\n",
    "        return torch.nn.CrossEntropyLoss(\n",
    "            outputs_emotions, emotions_labels\n",
    "        ) + torch.nn.BCELoss(outputs_triggers, triggers_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"\\\\test\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    # evaluate_during_training=True,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=8,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_full,\n",
    "    args=training_args,\n",
    "    # data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    compute_metrics=sequence_f1,\n",
    "    # tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
