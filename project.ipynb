{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm.notebook as tq\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load corpus\n",
    "\n",
    "data_path = Path(\"data/MELD_train_efr.json\")\n",
    "assert data_path.exists(), \"Data file is not present\"\n",
    "df = pd.read_json(data_path, dtype={\"speakers\": np.array})  # , \"triggers\": np.array})\n",
    "EPISODE, SPEAKERS, EMOTIONS, UTTERANCES, TRIGGERS = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Look for how many groups of episodes with the same first utterance there are and their lenghts\n",
    "\n",
    "df.sort_values(by=UTTERANCES, inplace=True)\n",
    "groups = np.zeros((850,), dtype=int)\n",
    "\n",
    "index = 0\n",
    "count = 1\n",
    "for i in range(1, len(df[UTTERANCES])):\n",
    "    if df[UTTERANCES][i][0] == df[UTTERANCES][i - 1][0]:\n",
    "        ### still in the same group\n",
    "        count += 1\n",
    "    else:\n",
    "        ### found new group\n",
    "        groups[index] = count\n",
    "        index += 1\n",
    "        count = 1\n",
    "\n",
    "groups = groups[groups != 0]\n",
    "\n",
    "print(f\"Number of groups: {len(groups)}\")\n",
    "print(f\"Avg group len: {np.average(groups):.1f}\")\n",
    "print(f\"Longest group: {np.max(groups)}\")\n",
    "print(f\"Episodes not in a group: {groups[groups == 1].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count how many speakers there are in each episode\n",
    "\n",
    "speakers_count = df[SPEAKERS].apply(lambda arr: np.unique(arr).shape[0]).to_numpy()\n",
    "min_sp = np.min(speakers_count)\n",
    "max_sp = np.max(speakers_count)\n",
    "print(\"Distribution of number of speakers:\")\n",
    "for count in range(min_sp, max_sp + 1):\n",
    "    print(f\"{count} speakers:  {np.sum(speakers_count == count)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop not useful column\n",
    "\n",
    "df.drop(columns=[EPISODE, SPEAKERS], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove Nones from the triggers\n",
    "\n",
    "df[TRIGGERS] = df[TRIGGERS].apply(\n",
    "    lambda trig_seq: np.array([0.0 if t is None else t for t in trig_seq])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing:\n",
    "If an episode contains the same utterances of the previous and a few more then the triggers from the previous episode are replicated in the current episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{df[TRIGGERS][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(1, len(df)):\n",
    "    is_continuation = np.all([u in df[UTTERANCES][i] for u in df[UTTERANCES][i - 1]])\n",
    "    if is_continuation:\n",
    "        count += 1\n",
    "        for k, t in enumerate(df[TRIGGERS][i - 1]):\n",
    "            df[TRIGGERS][i][k] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{df[TRIGGERS][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Validation/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Val Test split: 80/10/10\n",
    "\n",
    "\n",
    "def split_data(df: pd.DataFrame, seed: int = 42):\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, test_size=0.2, train_size=0.8, random_state=seed\n",
    "    )\n",
    "\n",
    "    df_val, df_test = train_test_split(\n",
    "        df_test, test_size=0.5, train_size=0.5, random_state=seed\n",
    "    )\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check\n",
    "df_train, df_val, df_test = split_data(df)\n",
    "print(f\"df_train len: {len(df_train)}\")\n",
    "print(f\"df_val len: {len(df_val)}\")\n",
    "print(f\"df_test len: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class imbalance check\n",
    "classes_count = {}\n",
    "for emotions in df_train[\"emotions\"]:\n",
    "    for emotion in emotions:\n",
    "        if emotion in classes_count:\n",
    "            classes_count[emotion] += 1\n",
    "        else:\n",
    "            classes_count[emotion] = 1\n",
    "\n",
    "### then we sort the dictionary by occurences\n",
    "emotions_dict = {\n",
    "    k: v\n",
    "    for k, v in sorted(classes_count.items(), key=lambda item: item[1], reverse=True)\n",
    "}\n",
    "print(\"Classes values:\")\n",
    "print(emotions_dict)\n",
    "\n",
    "# Class imbalance abbastanza alto, potremmo usare dei weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### given 2 series of sequences of triggers/emotions compute F1 inside each sequence and return avg, flatten out and compute F1\n",
    "def sequence_f1(y_true, y_pred, avg: bool = True):\n",
    "    res = [\n",
    "        f1_score(y_true=y_t, y_pred=y_p, average=\"micro\")\n",
    "        for y_t, y_p in zip(y_true, y_pred)\n",
    "    ]\n",
    "    return np.average(res) if avg else res\n",
    "\n",
    "\n",
    "def unrolled_f1(y_true, y_pred):\n",
    "    y_t_flat = []\n",
    "    for l in y_true:\n",
    "        for e in l:\n",
    "            y_t_flat.append(e)\n",
    "\n",
    "    y_p_flat = []\n",
    "    for l in y_pred:\n",
    "        for e in l:\n",
    "            y_p_flat.append(e)\n",
    "\n",
    "    return f1_score(y_true=y_t_flat, y_pred=y_p_flat, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create baseline models\n",
    "\n",
    "\n",
    "class SequenceDummyClassifier(DummyClassifier):\n",
    "    def __init__(self, strategy: str, seed: int = 42) -> None:\n",
    "        self.seed = seed\n",
    "        if not strategy.lower() in (\"random\", \"majority\"):\n",
    "            raise ValueError(\"strategy must be in [random, majority]\")\n",
    "        sklearn_strategy = \"uniform\" if strategy == \"random\" else \"most_frequent\"\n",
    "        super().__init__(strategy=sklearn_strategy, random_state=seed)\n",
    "\n",
    "    def _flatten_seq_grouping(self, df: pd.Series):\n",
    "        lens = df.apply(lambda s: len(s)).to_numpy()\n",
    "        max_len = np.max(lens)\n",
    "        min_len = np.min(lens)\n",
    "        res = np.empty((0,))\n",
    "        for group_len in range(min_len, max_len + 1):\n",
    "            group = df.loc[lens == group_len]\n",
    "            np_group = np.array(group.tolist()).flatten()\n",
    "            res = np.hstack((res, np_group))\n",
    "        return res\n",
    "\n",
    "    def _flatten_seq(self, df: pd.Series):\n",
    "        res = []\n",
    "        for l in df:\n",
    "            for e in l:\n",
    "                res.append(e)\n",
    "        return res\n",
    "\n",
    "    def _deflatten_seq(self, seq, shape_like: pd.Series):\n",
    "        data = iter(seq)\n",
    "        result = [[next(data) for _ in s] for s in shape_like]\n",
    "        return result\n",
    "\n",
    "    def fit(self, X: pd.Series, y: pd.Series):\n",
    "        X_flat = self._flatten_seq(X)\n",
    "        y_flat = self._flatten_seq(y)\n",
    "        super().fit(X=X_flat, y=y_flat)\n",
    "\n",
    "    def predict(self, X: pd.Series, return_flat: bool = False):\n",
    "        X_flat = self._flatten_seq(X)\n",
    "        y_flat = super().predict(X_flat)\n",
    "        return y_flat if return_flat else self._deflatten_seq(seq=y_flat, shape_like=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_baseline(df_train: pd.DataFrame, df_test: pd.DataFrame, seed: int = 42):\n",
    "\n",
    "    baseline_f1s = {}\n",
    "    baseline_results = {}\n",
    "\n",
    "    for strategy in (\"Random\", \"Majority\"):\n",
    "        for target in (EMOTIONS, TRIGGERS):\n",
    "            clf = SequenceDummyClassifier(strategy=strategy, seed=seed)\n",
    "            clf.fit(X=df_train[UTTERANCES], y=df_train[target])\n",
    "\n",
    "            res = clf.predict(X=df_test[UTTERANCES], return_flat=False)\n",
    "            baseline_results.update({f\"{target}_{strategy}\": res})\n",
    "\n",
    "            seq_f1 = sequence_f1(y_true=df_test[target], y_pred=res)\n",
    "            baseline_f1s.update({f\"sequence_f1({target}_{strategy})\": seq_f1})\n",
    "\n",
    "            unr_f1 = unrolled_f1(y_true=df_test[target], y_pred=res)\n",
    "            baseline_f1s.update({f\"unrolled_f1({target}_{strategy})\": unr_f1})\n",
    "\n",
    "    return baseline_f1s, baseline_results\n",
    "\n",
    "\n",
    "f1s, results = experiment_baseline(df_train, df_test)\n",
    "for k, v in f1s.items():\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# TODO discuss: questo fa schifo\n",
    "# encoder = OneHotEncoder(handle_unknown=\"error\", sparse_output=False)\n",
    "all_emotions = []\n",
    "for seq in df_train[EMOTIONS]:\n",
    "    for e in seq:\n",
    "        all_emotions.append(e)\n",
    "uniq_emotions = np.sort(np.unique(all_emotions))\n",
    "one_hot = np.identity(len(uniq_emotions))\n",
    "emotion_mapping = {e: one_hot[i] for i, e in enumerate(uniq_emotions)}\n",
    "print(emotion_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(ds_row, tokenizer=tokenizer):\n",
    "\n",
    "    if type(ds_row[\"emotions\"][0]) != str:  ### batchsize > 1\n",
    "        emotion_encoding = []\n",
    "        emotion_encoding = [\n",
    "            [emotion_mapping[e] for e in emotions_of_one_utterance]\n",
    "            for emotions_of_one_utterance in ds_row[\"emotions\"]\n",
    "        ]\n",
    "    else:  ### batchsize == 1\n",
    "        emotion_encoding = [emotion_mapping[e] for e in ds_row[\"emotions\"]]\n",
    "\n",
    "    encoded_ds_row = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        # \"emotions\": ds_row[\"emotions\"],\n",
    "        \"emotions\": emotion_encoding,\n",
    "        \"triggers\": ds_row[\"triggers\"],\n",
    "    }\n",
    "\n",
    "    for sentence in ds_row[\"utterances\"]:\n",
    "\n",
    "        tokenized_sentence = tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        encoded_ds_row[\"input_ids\"].append(tokenized_sentence[\"input_ids\"])\n",
    "        encoded_ds_row[\"token_type_ids\"].append(tokenized_sentence[\"token_type_ids\"])\n",
    "        encoded_ds_row[\"attention_mask\"].append(tokenized_sentence[\"attention_mask\"])\n",
    "\n",
    "    return encoded_ds_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_pandas(df_train)\n",
    "ds_val = Dataset.from_pandas(df_val)\n",
    "ds_test = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply tokenization\n",
    "batched = True\n",
    "ds_train_tokenized = ds_train.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=batched,\n",
    "    remove_columns=[UTTERANCES],\n",
    ")\n",
    "ds_val_tokenized = ds_val.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=batched,\n",
    "    remove_columns=[UTTERANCES],\n",
    ")\n",
    "ds_test_tokenized = ds_test.map(\n",
    "    function=tokenize,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=batched,\n",
    "    remove_columns=[UTTERANCES],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test della funzione\n",
    "tokens = tokenize(ds_train[0], tokenizer)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, num_emotions=7):\n",
    "        super(BERTClass, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        # self.lstm = torch.nn.LSTM()\n",
    "        # classifiers\n",
    "        self.l_emotions = torch.nn.Linear(self.bert.config.hidden_size, num_emotions)\n",
    "        self.l_triggers = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        output_emotions = self.dropout(output)\n",
    "        output_triggers = self.dropout(output)\n",
    "\n",
    "        # output_triggers = self.lstm(output_triggers)\n",
    "\n",
    "        output_emotions = self.l_emotions(output_emotions)\n",
    "        output_triggers = self.l_triggers(output_triggers)\n",
    "\n",
    "        return output_emotions, output_triggers\n",
    "\n",
    "    def freeze_params(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_emotions = df[\"emotions\"].explode().nunique()\n",
    "\n",
    "model_frozen = BERTClass(num_emotions)\n",
    "model_full = BERTClass(num_emotions)\n",
    "\n",
    "model_frozen.freeze_params()\n",
    "\n",
    "# Verifying that the params are actually frozen\n",
    "for name, param in model_frozen.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "\n",
    "for name, param in model_full.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [model_full, model_frozen]\n",
    "num_epochs = 5\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "for model in model_list:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    # Tokenizer initiation\n",
    "    # TODO Check tokenizer parameters\n",
    "    # TODO A Dataloader seems to be commonly used for this use cases\n",
    "    # encoding = tokenizer(df_train, truncation=False, padding='max_length', return_tensors='pt')\n",
    "    # input_ids = encoding['input_ids']\n",
    "    # attention_mask = encoding['attention_mask']\n",
    "\n",
    "    # Training loop\n",
    "    # for epoch in range(num_epochs):\n",
    "    #\n",
    "    #    for idx in range(len(df_test)):\n",
    "\n",
    "    #        text = df_train[idx].drop(TRIGGERS)\n",
    "    #        label = df_train[idx][TRIGGERS]\n",
    "\n",
    "    #        optimizer.zero_grad()\n",
    "\n",
    "    #        logits = model(batch_data)\n",
    "    #        loss = loss_fn(logits, batch_labels)\n",
    "    #        loss.backward()\n",
    "    #        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs_emotions, outputs_triggers, emotions_labels, triggers_labels):\n",
    "    return torch.nn.CrossEntropyLoss(\n",
    "        outputs_emotions, emotions_labels\n",
    "    ) + torch.nn.BCELoss(outputs_triggers, triggers_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training of the model\n",
    "def train_model(train_dl, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions_emotions = 0\n",
    "    correct_predictions_triggers = 0\n",
    "    num_samples_emotions = 0\n",
    "    num_samples_triggers = 0\n",
    "\n",
    "    ### activate dropout, batch norm\n",
    "    model.train()\n",
    "\n",
    "    ### initialize progress bar\n",
    "    batches = tq.tqdm(\n",
    "        enumerate(train_dl), total=len(train_dl), leave=True, colour=\"steelblue\"\n",
    "    )\n",
    "\n",
    "    for batch_idx, data in batches:\n",
    "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        emotions_labels = data[\"emotions\"].to(device, dtype=torch.float)\n",
    "        triggers_labels = data[\"triggers\"].to(device, dtype=torch.float)\n",
    "        outputs_emotions, outputs_triggers = model(ids, mask)  ### Forward\n",
    "\n",
    "        loss = loss_fn(\n",
    "            outputs_emotions, outputs_triggers, emotions_labels, triggers_labels\n",
    "        )\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        ### apply thresh 0.5\n",
    "        outputs_emotions = (\n",
    "            torch.sigmoid(outputs_emotions).cpu().detach().numpy().round()\n",
    "        )\n",
    "        outputs_triggers = (\n",
    "            torch.sigmoid(outputs_triggers).cpu().detach().numpy().round()\n",
    "        )\n",
    "\n",
    "        emotions_labels = emotions_labels.cpu().detach().numpy()\n",
    "        triggers_labels = triggers_labels.cpu().detach().numpy()\n",
    "\n",
    "        correct_predictions_emotions += np.sum(outputs_emotions == emotions_labels)\n",
    "        correct_predictions_triggers += np.sum(outputs_triggers == triggers_labels)\n",
    "\n",
    "        num_samples_emotions += emotions_labels.size\n",
    "        num_samples_triggers += triggers_labels.size\n",
    "\n",
    "        ### Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        ### Grad descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        ### Update progress bar\n",
    "        batches.set_description(f\"\")\n",
    "        batches.set_postfix(batch_loss=loss)\n",
    "\n",
    "    # Si potrebbe fare una singola accuracy come media delle due, magari fuori dal training\n",
    "    accuracy_emotions = float(correct_predictions_emotions) / num_samples_emotions\n",
    "    accuracy_triggers = float(correct_predictions_triggers) / num_samples_triggers\n",
    "\n",
    "    return model, accuracy_emotions, accuracy_triggers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval model, setup e train_eval da definire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(tokenized_datasets, batch_size):\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"train\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    validation_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"validation\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    test_dl = torch.utils.data.DataLoader(\n",
    "        tokenized_datasets[\"test\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return train_dl, validation_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainer(Trainer):\n",
    "    def __init__(self, model, training_args, train_ds, eval_ds, metrics):\n",
    "        super().__init__(model, training_args, train_ds, eval_ds, metrics)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        emotions_labels = inputs[\"emotions\"]\n",
    "        triggers_labels = inputs[\"triggers\"]\n",
    "        output_emotions, output_triggers = model(ids, mask)\n",
    "\n",
    "        custom_loss = self.loss_fn(\n",
    "            output_emotions, output_triggers, emotions_labels, triggers_labels\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            (custom_loss, output_emotions, output_triggers)\n",
    "            if return_outputs\n",
    "            else custom_loss\n",
    "        )\n",
    "\n",
    "    def loss_fn(outputs_emotions, outputs_triggers, emotions_labels, triggers_labels):\n",
    "        return torch.nn.CrossEntropyLoss(\n",
    "            outputs_emotions, emotions_labels\n",
    "        ) + torch.nn.BCELoss(outputs_triggers, triggers_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    # evaluate_during_training=True,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=8,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_full,\n",
    "    args=training_args,\n",
    "    # data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    compute_metrics=sequence_f1,\n",
    "    # tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
