{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G85c0I_dfrel"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "[[ -z \"$COLAB_RELEASE_TAG\" ]] && exit\n",
        "\n",
        "pip install -U accelerate\n",
        "pip install -U transformers\n",
        "pip install datasets\n",
        "pip install torcheval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQauKapN_A-k"
      },
      "source": [
        "the previous cell is executed only in colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zsiwgDday5H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertModel,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "import tqdm.notebook as tq\n",
        "from datasets import Dataset\n",
        "from typing import Tuple, Union\n",
        "from torcheval.metrics.functional import multiclass_f1_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkl3MAJNay5K"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "\n",
        "device = \"cuda\" if cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGPHLj_l_A-n"
      },
      "source": [
        "## Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoIQQ-f8SCx-"
      },
      "outputs": [],
      "source": [
        "# The data is in the root of the GDrive storage if running colab or in the same folder if running locally\n",
        "try:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/drive\")\n",
        "    data_path = Path(\"/content/drive/MyDrive/MELD_train_efr.json\")\n",
        "except:\n",
        "    data_path = Path(\"data/MELD_train_efr.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PDr_k7zay5M"
      },
      "outputs": [],
      "source": [
        "# Load corpus\n",
        "\n",
        "assert data_path.exists(), \"Data file is not present\"\n",
        "raw_df = pd.read_json(data_path, dtype={\"speakers\": np.array})\n",
        "# , \"triggers\": np.array})\n",
        "EPISODE, SPEAKERS, EMOTIONS, UTTERANCES, TRIGGERS = raw_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrM0XGw7ay5N"
      },
      "source": [
        "#### Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj6YC-LYay5N"
      },
      "outputs": [],
      "source": [
        "raw_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF83yTf2ay5O"
      },
      "outputs": [],
      "source": [
        "raw_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhOaBCgmay5O"
      },
      "outputs": [],
      "source": [
        "# Look for how many groups of episodes with the same first utterance there are and their lenghts\n",
        "\n",
        "raw_df.sort_values(by=UTTERANCES, inplace=True)\n",
        "groups = np.zeros((850,), dtype=int)\n",
        "\n",
        "index = 0\n",
        "count = 1\n",
        "for i in range(1, len(raw_df[UTTERANCES])):\n",
        "    if raw_df[UTTERANCES][i][0] == raw_df[UTTERANCES][i - 1][0]:\n",
        "        # still in the same group\n",
        "        count += 1\n",
        "    else:\n",
        "        # found new group\n",
        "        groups[index] = count\n",
        "        index += 1\n",
        "        count = 1\n",
        "\n",
        "groups = groups[groups != 0]\n",
        "\n",
        "print(f\"Number of groups: {len(groups)}\")\n",
        "print(f\"Avg group len: {np.average(groups):.1f}\")\n",
        "print(f\"Longest group: {np.max(groups)}\")\n",
        "print(f\"Episodes not in a group: {groups[groups == 1].shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2WnNM71ay5O"
      },
      "outputs": [],
      "source": [
        "# Count how many speakers there are in each episode\n",
        "\n",
        "speakers_count = raw_df[SPEAKERS].apply(lambda arr: np.unique(arr).shape[0]).to_numpy()\n",
        "min_sp = np.min(speakers_count)\n",
        "max_sp = np.max(speakers_count)\n",
        "print(\"Distribution of number of speakers:\")\n",
        "for count in range(min_sp, max_sp + 1):\n",
        "    print(f\"{count} speakers:  {np.sum(speakers_count == count)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-xkU5zLay5P"
      },
      "outputs": [],
      "source": [
        "# Class imbalance check\n",
        "classes_count = {}\n",
        "for emotions in raw_df[\"emotions\"]:\n",
        "    for emotion in emotions:\n",
        "        if emotion in classes_count:\n",
        "            classes_count[emotion] += 1\n",
        "        else:\n",
        "            classes_count[emotion] = 1\n",
        "\n",
        "# then we sort the dictionary by occurences\n",
        "emotions_dict = {\n",
        "    k: v\n",
        "    for k, v in sorted(classes_count.items(), key=lambda item: item[1], reverse=True)\n",
        "}\n",
        "print(\"Classes values:\")\n",
        "print(emotions_dict)\n",
        "\n",
        "# Classes counts are not balanced: the use of weights is recommended"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mu98zcLay5Q"
      },
      "source": [
        "#### Data cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rapPklVSay5R"
      },
      "outputs": [],
      "source": [
        "# Drop not useful column\n",
        "\n",
        "raw_df.drop(columns=[SPEAKERS], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yICEGx_4ay5R"
      },
      "outputs": [],
      "source": [
        "# Remove Nones from the triggers\n",
        "\n",
        "raw_df[TRIGGERS] = raw_df[TRIGGERS].apply(\n",
        "    lambda trig_seq: np.array([0.0 if t is None else t for t in trig_seq])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DGMbo39ay5S"
      },
      "outputs": [],
      "source": [
        "# Change column \"episode\" from utterance_xyz to episode_xyz\n",
        "for i in range(len(raw_df)):\n",
        "    raw_df[EPISODE][i] = f\"episode_{raw_df[EPISODE][i][10:]}\"\n",
        "\n",
        "clean_df = raw_df\n",
        "clean_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8jHrTTyay5S"
      },
      "source": [
        "### Data preprocessing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMdYcBJAay5S"
      },
      "source": [
        "\n",
        "If an episode contains the same utterances of the previous and a few more then the triggers from the previous episode are replicated in the current episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPGeMQ3Way5S"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    print(f\"{raw_df[TRIGGERS][i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLqAP_IUay5S"
      },
      "outputs": [],
      "source": [
        "# Replicate triggers\n",
        "\n",
        "count = 0\n",
        "for i in range(1, len(clean_df)):\n",
        "    is_continuation = np.all(\n",
        "        [u in clean_df[UTTERANCES][i] for u in clean_df[UTTERANCES][i - 1]]\n",
        "    )\n",
        "    if is_continuation:\n",
        "        count += 1\n",
        "        for k, t in enumerate(clean_df[TRIGGERS][i - 1]):\n",
        "            clean_df[TRIGGERS][i][k] = t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wsvy-l1Qay5T"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    print(f\"{raw_df[TRIGGERS][i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SPyuwNuay5T"
      },
      "outputs": [],
      "source": [
        "# Train Val Test split: 80/10/10\n",
        "\n",
        "\n",
        "def split_data(\n",
        "    df: pd.DataFrame, seed: int = 42\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    df_train, df_test = train_test_split(\n",
        "        df, test_size=0.2, train_size=0.8, random_state=seed\n",
        "    )\n",
        "\n",
        "    df_val, df_test = train_test_split(\n",
        "        df_test, test_size=0.5, train_size=0.5, random_state=seed\n",
        "    )\n",
        "\n",
        "    return df_train, df_val, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojgAR5vjay5T"
      },
      "outputs": [],
      "source": [
        "# Check\n",
        "df_train_t, df_val_t, df_test_t = split_data(clean_df)\n",
        "print(f\"df_train len: {len(df_train_t)}\")\n",
        "print(f\"df_val len:   {len(df_val_t)}\")\n",
        "print(f\"df_test len:  {len(df_test_t)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSI7H2Znay5T"
      },
      "source": [
        "explode the dataframe: <br>\n",
        "each rows contains: previous utterance, target utterance, next utterance (for context). <br>\n",
        "Except for the first and lat utterance of each episode that have no previous and next utterance, respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaA7zvAKay5U"
      },
      "outputs": [],
      "source": [
        "def explode_add_context_old(df: pd.DataFrame, ctxt_win_len: int = 1) -> pd.DataFrame:\n",
        "    # context_window = number of utterances before and number of utterance after the current sentence.\n",
        "\n",
        "    # Flatten the lists of utterances,triggers,emotions into new rows of the dataframe\n",
        "    exploded_df = df.explode([UTTERANCES, TRIGGERS, EMOTIONS], ignore_index=True)\n",
        "    exploded_df.rename(columns={UTTERANCES: \"current\"}, inplace=True)\n",
        "    exploded_df.head(10)\n",
        "\n",
        "    # Pair shifted columns of utterances to the exploded df to make previous and next\n",
        "    for i in range(1, ctxt_win_len + 1):\n",
        "        padding_cells = pd.Series([\" \" for _ in range(i)])\n",
        "\n",
        "        previous_col = pd.concat(\n",
        "            (padding_cells, exploded_df[\"current\"][:-i]), copy=False\n",
        "        ).to_list()\n",
        "        exploded_df.insert(loc=2, column=f\"previous_-{i}\", value=previous_col)\n",
        "\n",
        "        next_col = pd.concat(\n",
        "            (exploded_df[\"current\"][i:], padding_cells), copy=False\n",
        "        ).to_list()\n",
        "        exploded_df.insert(loc=2 + 2 * i, column=f\"next_{i}\", value=next_col)\n",
        "\n",
        "    # Remove the previous of the first utterance and the next of the last utterance of each episode\n",
        "    for i in range(1, len(exploded_df) - 1):\n",
        "        for j in range(1, ctxt_win_len + 1):\n",
        "            if exploded_df[EPISODE][i] != exploded_df[EPISODE][i - 1]:\n",
        "                exploded_df[f\"next_{j}\"][i - j] = \" \"\n",
        "                exploded_df[f\"previous_-{j}\"][i] = \" \"\n",
        "\n",
        "    exploded_df.sort_values(by=EPISODE, inplace=True)\n",
        "    # TODO sortare prima su episode poi sull'indice\n",
        "    return exploded_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OToQ09Cjay5U"
      },
      "outputs": [],
      "source": [
        "all_emotions = df_train_t[EMOTIONS].explode().to_numpy()\n",
        "uniq_emotions = np.sort(np.unique(all_emotions))\n",
        "# uniq_emotions = [\"neutral\", \"joy\", \"surprise\", \"anger\", \"sadness\", \"disgust\", \"fear\"]\n",
        "one_hot = np.identity(len(uniq_emotions), dtype=int)\n",
        "emotion_mapping = {e: one_hot[i] for i, e in enumerate(uniq_emotions)}\n",
        "max_em_len = np.max([len(v) for v in emotion_mapping.values()]) + 1\n",
        "for k, e in emotion_mapping.items():\n",
        "    print(f\"{k}:{' '*(max_em_len - len(k))} {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2SmZxixay5U"
      },
      "outputs": [],
      "source": [
        "def explode_add_context(df: pd.DataFrame, ctxt_win_len: int = 1) -> pd.DataFrame:\n",
        "\n",
        "    new_df = {\"emotion\": [], \"trigger\": [], \"hist_curr\": [], \"next\": [], \"episode\": []}\n",
        "\n",
        "    utt_separator = \" \"  # TODO va bene ' ' come separatore?\n",
        "    # for each original row\n",
        "    for _, row in df.iterrows():\n",
        "        # create as many new rows as utterances in the original row\n",
        "        for utt_idx, (emo, trig) in enumerate(zip(row[EMOTIONS], row[TRIGGERS])):\n",
        "            new_df[\"emotion\"].append(emo)\n",
        "            new_df[\"trigger\"].append(trig)\n",
        "            new_df[\"episode\"].append(row[\"episode\"])\n",
        "\n",
        "            # previous utterances + current (same way triggers are defined)\n",
        "            start_hist = np.max((0, utt_idx - ctxt_win_len))\n",
        "            curr_incl = utt_idx + 1\n",
        "            hist_curr = utt_separator.join(row[UTTERANCES][start_hist:curr_incl])\n",
        "            new_df[\"hist_curr\"].append(hist_curr)\n",
        "\n",
        "            # next utterances\n",
        "            curr_excl = utt_idx + 1\n",
        "            last_next_excl = utt_idx + ctxt_win_len + 1\n",
        "            next = utt_separator.join(row[UTTERANCES][curr_excl:last_next_excl])\n",
        "            new_df[\"next\"].append(next)\n",
        "\n",
        "    new_df = pd.DataFrame(new_df)\n",
        "    # important: the new_df still has utterances grouped by episode (to make the metrics work), otherwise we sort\n",
        "    new_df.sort_values(by=EPISODE, inplace=True)\n",
        "\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdIfKbjQay5V"
      },
      "outputs": [],
      "source": [
        "# ctxt_win_len = 2\n",
        "# ctxt_win_len = 3\n",
        "ctxt_win_len = 2\n",
        "\n",
        "df_train = explode_add_context(df_train_t, ctxt_win_len)\n",
        "df_val = explode_add_context(df_val_t, ctxt_win_len)\n",
        "df_test = explode_add_context(df_test_t, ctxt_win_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9bBL5v0ay5V"
      },
      "outputs": [],
      "source": [
        "df_train.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H14Z-kC8ay5V"
      },
      "outputs": [],
      "source": [
        "for idx in range(10 if len(df_train) >= 10 else len(df_train)):\n",
        "    print(f\"{df_train['hist_curr'][idx]}  -->  {df_train['next'][idx]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2APG5E7Vay5e"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9czZlvHay5e"
      },
      "outputs": [],
      "source": [
        "# Compute F1 score for each dialogue and return avg over dialogues\n",
        "def sequence_f1(\n",
        "    y_true: pd.DataFrame,\n",
        "    y_pred: np.ndarray,\n",
        "    target_column: str,\n",
        "    avg: bool = True,\n",
        ") -> Union[dict, float]:\n",
        "    assert len(y_pred) == len(y_true), \"y_pred and y_true must be of the same lenght\"\n",
        "    # assert (\n",
        "    #    y_true[EPISODE].is_monotonic_increasing\n",
        "    #    or y_true[EPISODE].is_monotonic_decreasing\n",
        "    # ), \"utterances must be sorted over the episodes\"\n",
        "\n",
        "    res = {}\n",
        "    start = 0\n",
        "    stop_incl = 0\n",
        "    for i in range(1, len(y_pred)):\n",
        "        if y_true[EPISODE][i - 1] != y_true[EPISODE][i]:\n",
        "            stop_incl = i - 1\n",
        "            f1 = f1_score(\n",
        "                y_true=y_true[target_column][start : stop_incl + 1].to_list(),\n",
        "                y_pred=y_pred[start : stop_incl + 1],\n",
        "                average=\"micro\",\n",
        "            )\n",
        "            res.update({y_true[EPISODE][start]: f1})\n",
        "            start = i\n",
        "\n",
        "    # np.std(list(res.values))\n",
        "    return res if not avg else np.average(list(res.values()))\n",
        "\n",
        "\n",
        "# Compute F1 score for the unrolled sequence\n",
        "def unrolled_f1(\n",
        "    y_true: pd.DataFrame,\n",
        "    y_pred: np.ndarray,\n",
        "    target_column: str,\n",
        ") -> float:\n",
        "    return f1_score(y_true[target_column].to_list(), y_pred, average=\"micro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_zR-xLoay5f"
      },
      "source": [
        "## Baseline Models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbrQP_Ilay5f"
      },
      "outputs": [],
      "source": [
        "# Create baseline model\n",
        "\n",
        "\n",
        "class SequenceDummyClassifier(DummyClassifier):\n",
        "    def __init__(self, strategy: str, seed: int = 42) -> None:\n",
        "        self.seed = seed\n",
        "        if not strategy.lower() in (\"random\", \"majority\"):\n",
        "            raise ValueError(\"strategy must be in [random, majority]\")\n",
        "        sklearn_strategy = \"uniform\" if strategy == \"random\" else \"most_frequent\"\n",
        "        super().__init__(strategy=sklearn_strategy, random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsMoC-F7ay5f"
      },
      "outputs": [],
      "source": [
        "def experiment_baseline(\n",
        "    df_train: pd.DataFrame,\n",
        "    df_test: pd.DataFrame,\n",
        "    seed: int = 42,\n",
        ") -> dict:\n",
        "\n",
        "    baseline_f1s = {}\n",
        "    baseline_results = {}\n",
        "\n",
        "    for strategy in (\"Random\", \"Majority\"):\n",
        "        for target in (\"emotion\", \"trigger\"):\n",
        "            clf = SequenceDummyClassifier(strategy=strategy, seed=seed)\n",
        "            clf.fit(X=df_train[\"hist_curr\"], y=df_train[target])\n",
        "\n",
        "            res = clf.predict(X=df_test[\"hist_curr\"])\n",
        "            baseline_results.update({f\"{target}_{strategy}\": res})\n",
        "\n",
        "            seq_f1 = sequence_f1(y_true=df_test, y_pred=res, target_column=target)\n",
        "            baseline_f1s.update({f\"sequence_f1({target}_{strategy})\": seq_f1})\n",
        "\n",
        "            unr_f1 = unrolled_f1(y_true=df_test, y_pred=res, target_column=target)\n",
        "            baseline_f1s.update({f\"unrolled_f1({target}_{strategy})\": unr_f1})\n",
        "\n",
        "    return baseline_f1s, baseline_results\n",
        "\n",
        "\n",
        "f1s, results = experiment_baseline(df_train, df_test)\n",
        "for k, v in f1s.items():\n",
        "    print(f\"{k} : {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P_NI6Qeay5f"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQOz8hgHay5l"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GObQL5nHay5m"
      },
      "outputs": [],
      "source": [
        "ds_train = Dataset.from_pandas(df_train, preserve_index=False)\n",
        "ds_val = Dataset.from_pandas(df_val, preserve_index=False)\n",
        "ds_test = Dataset.from_pandas(df_test, preserve_index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtB_dXbPay5n"
      },
      "outputs": [],
      "source": [
        "def tokenize(ds_row, tokenizer):\n",
        "    if type(ds_row[\"emotion\"]) != str:  # batchsize > 1\n",
        "        emotion_encoding = []\n",
        "        emotion_encoding = [emotion_mapping[e] for e in ds_row[\"emotion\"]]\n",
        "        # trigger_encoding = []\n",
        "        # print(ds_row[\"trigger\"])\n",
        "        # print(ds_row[\"emotion\"])\n",
        "        trigger_encoding = [np.array([e]) for e in ds_row[\"trigger\"]]\n",
        "    else:  # batchsize == 1\n",
        "        emotion_encoding = emotion_mapping[ds_row[\"emotion\"]]\n",
        "        trigger_encoding = np.array([ds_row[\"trigger\"]])\n",
        "\n",
        "    encoded_ds_row = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"emotion\": torch.tensor(np.array(emotion_encoding), dtype=torch.float),\n",
        "        \"trigger\": torch.tensor(np.array(trigger_encoding), dtype=torch.float),\n",
        "    }\n",
        "\n",
        "    tokenized_context = tokenizer(\n",
        "        ds_row[\"hist_curr\"],\n",
        "        ds_row[\"next\"] if ds_row[\"next\"] != [] else None,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        # max_length=tokenizer.model_max_length // 2,\n",
        "        max_length=166,  # ot by experiment: check how long the tokenization is and set it to that\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    encoded_ds_row[\"input_ids\"] = tokenized_context[\"input_ids\"]\n",
        "    encoded_ds_row[\"token_type_ids\"] = tokenized_context[\"token_type_ids\"]\n",
        "\n",
        "    encoded_ds_row[\"attention_mask\"] = tokenized_context[\"attention_mask\"]\n",
        "\n",
        "    return encoded_ds_row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyasWFdLay5o"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "ds_short = Dataset.from_pandas(df_train.iloc[0:4], preserve_index=False)\n",
        "print(f\"cols before tokenization: {ds_short.column_names}\")\n",
        "\n",
        "ds_short_tokenized = ds_short.map(\n",
        "    function=tokenize,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    batched=False,\n",
        "    remove_columns=[\"hist_curr\", \"next\"],\n",
        ")\n",
        "ds_short_tokenized_batched = ds_short.map(\n",
        "    function=tokenize,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    batched=True,\n",
        "    remove_columns=[\"hist_curr\", \"next\"],\n",
        ")\n",
        "ds_short_tokenized.set_format(type=\"torch\")\n",
        "ds_short_tokenized_batched.set_format(type=\"torch\")\n",
        "\n",
        "print(f\"cols after tokenization: {ds_short_tokenized.column_names}\")\n",
        "\n",
        "idx = 3\n",
        "\n",
        "print(\"*** NO BATCH -> matrix ***\")\n",
        "emotion = ds_short_tokenized[\"emotion\"][idx]\n",
        "ids = ds_short_tokenized[\"input_ids\"][idx]\n",
        "print(f\"ds[emotion][{idx}] = {emotion}\")\n",
        "print(f\"type of emotion: {emotion.dtype}\")\n",
        "print(f\"ds[input_ids][{idx}] = {ids}\")\n",
        "print(f\"ds[input_ids][{idx}] shape = {ids.size()}\")\n",
        "\n",
        "\n",
        "print(\"\\n*** BATCH -> list ***\")\n",
        "emotion_batch = ds_short_tokenized_batched[\"emotion\"][idx]\n",
        "ids_batch = ds_short_tokenized_batched[\"input_ids\"][idx]\n",
        "print(f\"ds_batch[emotion][{idx}] = {emotion_batch}\")\n",
        "print(f\"ds_batch[input_ids][{idx}] = {ids_batch}\")\n",
        "print(f\"ds_batch[input_ids] shape = {ids_batch.size()}\")\n",
        "\n",
        "print(\"\\n*** RECONSTRUCTION TEST ***\")\n",
        "tokens = tokenizer.convert_ids_to_tokens(ids[0])\n",
        "tokens_batched = tokenizer.convert_ids_to_tokens(ids_batch)\n",
        "reconstructed_string = tokenizer.convert_tokens_to_string(tokens)\n",
        "reconstructed_string_batched = tokenizer.convert_tokens_to_string(\n",
        "    tokens_batched)\n",
        "print(f\"original string:         {reconstructed_string}\")\n",
        "print(f\"original string batched: {reconstructed_string_batched}\")\n",
        "\n",
        "print(\"___________\")\n",
        "print(ds_short_tokenized_batched[\"trigger\"][0])\n",
        "print(ds_short_tokenized_batched[\"trigger\"][0].size())\n",
        "# print(ds_short)\n",
        "\n",
        "# OSSERVAZIONI:\n",
        "# ds.set_format(type='torch') mette trigger ed emotion dentro a dei tensori e fa qualcosa a input_ids per cui viene stampato come matrice e non come riga\n",
        "# a prescindere da set_format:  input_ids è una [[]] se tokeniziamo senza batch, è una [] se tokenizziamo con il batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvfEGNlyay5o"
      },
      "outputs": [],
      "source": [
        "# Apply tokenization\n",
        "\n",
        "batched = True\n",
        "cols_to_drop = [\"hist_curr\", \"next\"]\n",
        "\n",
        "ds_train_tokenized = ds_train.map(\n",
        "    function=tokenize,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    batched=batched,\n",
        "    remove_columns=cols_to_drop,\n",
        ")\n",
        "ds_train_tokenized.set_format(type=\"torch\")\n",
        "\n",
        "ds_test_tokenized = ds_test.map(\n",
        "    function=tokenize,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    batched=batched,\n",
        "    remove_columns=cols_to_drop,\n",
        ")\n",
        "ds_test_tokenized.set_format(type=\"torch\")\n",
        "\n",
        "ds_val_tokenized = ds_val.map(\n",
        "    function=tokenize,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer},\n",
        "    batched=batched,\n",
        "    remove_columns=cols_to_drop,\n",
        ")\n",
        "ds_val_tokenized.set_format(type=\"torch\")\n",
        "\n",
        "EMOTION, TRIGGER, EPISODE, INPUT_IDS, TOKEN_TYPE_IDS, ATTENTION_MASK = (\n",
        "    ds_train_tokenized.column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yTyYTER_A-y"
      },
      "outputs": [],
      "source": [
        "print(f\"columns before tokenization: {ds_train.column_names}\")\n",
        "print(f\"columns after tokenization: {ds_train_tokenized.column_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi2RREaRay5p"
      },
      "outputs": [],
      "source": [
        "# Tokenization test\n",
        "tokens = tokenizer.convert_ids_to_tokens(ds_train_tokenized[\"input_ids\"][0])\n",
        "string = tokenizer.convert_tokens_to_string(tokens)\n",
        "original_string = ds_train[\"hist_curr\"][0] + ds_train[\"next\"][0]\n",
        "print(original_string)\n",
        "print(string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhYSVkR6ay5q"
      },
      "outputs": [],
      "source": [
        "# Class weighting algorithm\n",
        "\n",
        "\n",
        "def compute_class_weights(\n",
        "    emotions: pd.Series, root: bool = False, normalize: bool = False\n",
        ") -> torch.tensor:\n",
        "    labels, n_ones = np.unique(emotions.to_numpy(), return_counts=True)\n",
        "    l = len(emotions)\n",
        "    print(labels)\n",
        "    n_zeroes = np.array([l - n for n in n_ones])\n",
        "\n",
        "    weights = np.empty_like(n_ones, dtype=np.float32)\n",
        "    for class_num, (ones, zeroes) in enumerate(zip(n_ones, n_zeroes)):\n",
        "        if root:\n",
        "            weights[class_num] = np.sqrt(zeroes / (ones + 1e-4))\n",
        "        else:\n",
        "            weights[class_num] = zeroes / (ones + 1e-4)\n",
        "    if normalize:\n",
        "        sum = np.sum(weights)\n",
        "        weights = weights / sum\n",
        "\n",
        "    print(f\"weigts = {weights}\")\n",
        "    return torch.as_tensor(weights, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPX895PMay5q"
      },
      "source": [
        "# Bert Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euGU4lPday5q"
      },
      "outputs": [],
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self, num_emotions=7):\n",
        "        super(BERTClass, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "        self.dropout_emo = torch.nn.Dropout(0.1)\n",
        "        self.dropout_tri = torch.nn.Dropout(0.2)\n",
        "\n",
        "        # Funziona ma la loss resta uguale, probabilmente il problema è nel training a questo punto\n",
        "        # self.lstm = torch.nn.LSTM(self.bert.config.hidden_size, self.bert.config.hidden_size, batch_first=True)\n",
        "\n",
        "        # classifiers\n",
        "        self.l_emotions = torch.nn.Linear(self.bert.config.hidden_size, num_emotions)\n",
        "        self.l_triggers = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
        "        self.linear = torch.nn.Linear(\n",
        "            self.bert.config.hidden_size, self.bert.config.hidden_size\n",
        "        )\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "        output = output.pooler_output\n",
        "\n",
        "        # output, _ = self.lstm(output)\n",
        "        # output = self.linear(output)\n",
        "\n",
        "        # output_emotions = self.relu(output)\n",
        "        # output_triggers = self.relu(output)\n",
        "\n",
        "        output_emotions = self.dropout_emo(output)\n",
        "        output_triggers = self.dropout_tri(output)\n",
        "\n",
        "        output_emotions = self.l_emotions(output_emotions)\n",
        "        output_triggers = self.l_triggers(output_triggers)\n",
        "\n",
        "        return output_emotions, output_triggers\n",
        "\n",
        "    def freeze_params(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PNrvbGJay5r"
      },
      "source": [
        "# Training Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZo0EILtay5r"
      },
      "outputs": [],
      "source": [
        "def setup(learning_rate=1e-5, weight_decay=0.1, frozen=False):\n",
        "    num_emotions = len(uniq_emotions)\n",
        "\n",
        "    model = BERTClass(num_emotions)\n",
        "\n",
        "    model.to(device)\n",
        "    if frozen:\n",
        "        model.freeze_params()\n",
        "        # Verifying that the params are actually frozen\n",
        "        # TODO\n",
        "        # assert np.all(\n",
        "        #    [not param.requires_grad for _, param in model.named_parameters()]\n",
        "        # ), \"model did not freeze\"\n",
        "    else:\n",
        "        # Verifying that the params are not frozen\n",
        "        assert np.all(\n",
        "            [param.requires_grad for _, param in model.named_parameters()]\n",
        "        ), \"model unexpectedly frozen\"\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnLfOIfOay5t"
      },
      "outputs": [],
      "source": [
        "def loss_fn(\n",
        "    outputs_emotions,\n",
        "    outputs_triggers,\n",
        "    emotions_labels,\n",
        "    triggers_labels,\n",
        "    emotion_weights=None,\n",
        "    balance=0.5,\n",
        "):\n",
        "    assert balance < 1, \"loss balancing factor must be < 1\"\n",
        "\n",
        "    emotion_lf = torch.nn.CrossEntropyLoss(weight=emotion_weights)\n",
        "    emotion_loss = emotion_lf(outputs_emotions, emotions_labels) * balance\n",
        "\n",
        "    trigger_lf = torch.nn.BCEWithLogitsLoss()\n",
        "    trigger_loss = trigger_lf(\n",
        "        outputs_triggers, triggers_labels) * (1 - balance)\n",
        "\n",
        "    return (emotion_loss + trigger_loss) * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI83PVIYjeEr"
      },
      "outputs": [],
      "source": [
        "def get_metrics(y_true, y_pred):\n",
        "    cm_emotions = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    FP_emotions = (cm_emotions.sum(axis=0) - np.diag(cm_emotions)).sum()\n",
        "    FN_emotions = (cm_emotions.sum(axis=1) - np.diag(cm_emotions)).sum()\n",
        "    TP_emotions = np.diag(cm_emotions).sum()\n",
        "    TN_emotions = cm_emotions.sum() - (FP_emotions + FN_emotions + TP_emotions)\n",
        "\n",
        "    print(\"FP_emotions\")\n",
        "    print(FP_emotions)\n",
        "    print(\"FN_emotions\")\n",
        "    print(FN_emotions)\n",
        "    print(\"TP_emotions\")\n",
        "    print(TP_emotions)\n",
        "    print(\"TN_emotions\")\n",
        "    print(TN_emotions)\n",
        "    print(\"num_samples_emotions\")\n",
        "    print(len(y_true))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tskUATmQu_H"
      },
      "outputs": [],
      "source": [
        "DEBUG_MODE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pajpSproVVyS"
      },
      "outputs": [],
      "source": [
        "def decode_emotions(coded_emotion):\n",
        "    idx = np.argmax(coded_emotion)\n",
        "    emotions = list(emotion_mapping.keys())\n",
        "    return emotions[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMRpKVFpay5u"
      },
      "outputs": [],
      "source": [
        "# Training of the model\n",
        "def train_model(train_dl, model, optimizer, emotion_weights=None):\n",
        "    losses = []\n",
        "    correct_predictions_emotions = 0\n",
        "    correct_predictions_triggers = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    # activate dropout, batch norm\n",
        "    model.train()\n",
        "\n",
        "    # initialize progress bar\n",
        "    batches = tq.tqdm(\n",
        "        enumerate(train_dl), total=len(train_dl), leave=True, colour=\"steelblue\"\n",
        "    )\n",
        "    test_count = 0\n",
        "    for batch_idx, data in batches:\n",
        "        ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
        "        token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
        "        emotions_labels = data[\"emotion\"].to(device)\n",
        "        triggers_labels = data[\"trigger\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs_emotions, outputs_triggers = model(ids, mask, token_type_ids)\n",
        "\n",
        "        loss = loss_fn(\n",
        "            outputs_emotions,\n",
        "            outputs_triggers,\n",
        "            emotions_labels,\n",
        "            triggers_labels,\n",
        "            emotion_weights,\n",
        "        )\n",
        "        losses.append(loss.cpu().detach().numpy().item())\n",
        "\n",
        "        # apply thresh 0.5\n",
        "        outputs_emotions = torch.sigmoid(\n",
        "            outputs_emotions).cpu().detach().numpy()\n",
        "        outputs_triggers = (\n",
        "            torch.sigmoid(outputs_triggers).cpu().detach().numpy().round()\n",
        "        )\n",
        "\n",
        "        emotions_labels = emotions_labels.cpu().detach().numpy()\n",
        "        triggers_labels = triggers_labels.cpu().detach().numpy()\n",
        "\n",
        "        correct_predictions_emotions += np.sum(\n",
        "            np.argmax(outputs_emotions, axis=1) == np.argmax(\n",
        "                emotions_labels, axis=1)\n",
        "        )\n",
        "        correct_predictions_triggers += np.sum(\n",
        "            outputs_triggers == triggers_labels)\n",
        "\n",
        "        num_samples += triggers_labels.size\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Grad descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        batches.set_description(f\"\")\n",
        "        batches.set_postfix(batch_loss=loss.item())\n",
        "        if test_count >= 5 and DEBUG_MODE:\n",
        "            break\n",
        "        test_count += 1\n",
        "\n",
        "    # Si potrebbe fare una singola accuracy come media delle due, magari fuori dal training\n",
        "    accuracy_emotions = float(correct_predictions_emotions) / num_samples\n",
        "    accuracy_triggers = float(correct_predictions_triggers) / num_samples\n",
        "\n",
        "    # np.mean(losses)\n",
        "    return (model, accuracy_emotions, accuracy_triggers, losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGaacQsRay5u"
      },
      "outputs": [],
      "source": [
        "def eval_model(validation_dl, model, emotion_weights=None):\n",
        "    losses = []\n",
        "    correct_predictions_emotions = 0\n",
        "    correct_predictions_triggers = 0\n",
        "    num_samples = 0\n",
        "    # num_categories = next(iter(validation_dl))[\"labels\"].shape[1]\n",
        "\n",
        "    # accumulate data over each batch to compute the metrics\n",
        "    results = {\n",
        "        \"episode\": [],\n",
        "        \"emotion_pred\": [],\n",
        "        \"emotion_true\": [],\n",
        "        \"trigger_pred\": [],\n",
        "        \"trigger_true\": [],\n",
        "    }\n",
        "\n",
        "    # turn off dropout, fix batch norm\n",
        "    model.eval()\n",
        "\n",
        "    # show progress bar\n",
        "    batches = tq.tqdm(\n",
        "        enumerate(validation_dl),\n",
        "        total=len(validation_dl),\n",
        "        leave=True,\n",
        "        colour=\"steelblue\",\n",
        "    )\n",
        "\n",
        "    actual_emotions = []\n",
        "    pred_emotions = []\n",
        "    actual_trigger = []\n",
        "    predicted_triggers = []\n",
        "\n",
        "    debug_count = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in batches:\n",
        "            ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
        "            token_type_ids = data[\"token_type_ids\"].to(\n",
        "                device, dtype=torch.long)\n",
        "            emotions_labels = data[\"emotion\"].to(device)\n",
        "            triggers_labels = data[\"trigger\"].to(device)\n",
        "            outputs_emotions, outputs_triggers = model(\n",
        "                ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(\n",
        "                outputs_emotions,\n",
        "                outputs_triggers,\n",
        "                emotions_labels,\n",
        "                triggers_labels,\n",
        "                emotion_weights,\n",
        "            )\n",
        "            losses.append(loss.cpu().detach().numpy().item())\n",
        "\n",
        "            # get the predicted outputs\n",
        "            # training sigmoid is in BCEWithLogitsLoss\n",
        "            outputs_emotions = torch.sigmoid(\n",
        "                outputs_emotions).cpu().detach().numpy()\n",
        "            outputs_emotions = np.argmax(outputs_emotions, axis=1)\n",
        "            outputs_triggers = (\n",
        "                torch.sigmoid(outputs_triggers).cpu().detach().numpy().round()\n",
        "            )\n",
        "\n",
        "            # get the real labels\n",
        "            emotions_labels = emotions_labels.cpu().detach().numpy()\n",
        "            emotions_labels = np.argmax(emotions_labels, axis=1)\n",
        "            triggers_labels = triggers_labels.cpu().detach().numpy()\n",
        "\n",
        "            correct_predictions_emotions += np.sum(\n",
        "                outputs_emotions == emotions_labels)\n",
        "            correct_predictions_triggers += np.sum(\n",
        "                outputs_triggers == triggers_labels)\n",
        "\n",
        "            num_samples += triggers_labels.size\n",
        "\n",
        "            # update the results df\n",
        "            actual_emotions.extend(\n",
        "                [decode_emotions(emotion) for emotion in emotions_labels]\n",
        "            )\n",
        "            pred_emotions.extend(\n",
        "                [decode_emotions(emotion) for emotion in outputs_emotions]\n",
        "            )\n",
        "            actual_trigger.extend(triggers_labels)\n",
        "            predicted_triggers.extend(outputs_triggers)\n",
        "\n",
        "            results[\"episode\"].extend(data[\"episode\"])\n",
        "            results[\"emotion_pred\"].extend(\n",
        "                [decode_emotions(emotion) for emotion in outputs_emotions]\n",
        "            )\n",
        "            results[\"emotion_true\"].extend(\n",
        "                [decode_emotions(emotion) for emotion in data[\"emotion\"]]\n",
        "            )\n",
        "            for i in outputs_triggers:\n",
        "                results[\"trigger_pred\"].append(i)\n",
        "            # results[\"trigger_pred\"].extend(outputs_triggers.squeeze())\n",
        "            for i in data[\"trigger\"]:\n",
        "                results[\"trigger_true\"].append(i.numpy())\n",
        "            # results[\"trigger_true\"].extend(data[\"trigger\"].squeeze())\n",
        "\n",
        "            if debug_count >= 5 and DEBUG_MODE:\n",
        "                break\n",
        "            debug_count += 1\n",
        "\n",
        "    results[\"trigger_pred\"] = np.array(results[\"trigger_pred\"]).flatten()\n",
        "    results[\"trigger_true\"] = np.array(results[\"trigger_true\"]).flatten()\n",
        "\n",
        "    accuracy_emotions = float(correct_predictions_emotions) / num_samples\n",
        "    accuracy_triggers = float(correct_predictions_triggers) / num_samples\n",
        "\n",
        "    unrolled_f1_emotions = f1_score(\n",
        "        results[\"emotion_true\"], results[\"emotion_pred\"], average=\"weighted\"\n",
        "    )\n",
        "    unrolled_f1_triggers = f1_score(\n",
        "        results[\"trigger_true\"], results[\"trigger_pred\"], average=\"weighted\"\n",
        "    )\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    episode_f1_emotions = sequence_f1(\n",
        "        results_df[[\"episode\", \"emotion_true\"]],\n",
        "        results_df[\"emotion_pred\"],\n",
        "        target_column=\"emotion_true\",\n",
        "    )\n",
        "    episode_f1_triggers = sequence_f1(\n",
        "        results_df[[\"episode\", \"trigger_true\"]],\n",
        "        results_df[\"trigger_pred\"],\n",
        "        target_column=\"trigger_true\",\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        accuracy_emotions,\n",
        "        accuracy_triggers,\n",
        "        np.mean(losses),\n",
        "        unrolled_f1_emotions,\n",
        "        unrolled_f1_triggers,\n",
        "        episode_f1_emotions,\n",
        "        episode_f1_triggers,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjmLdgGYay5u"
      },
      "outputs": [],
      "source": [
        "def train_eval(\n",
        "    train_dl,\n",
        "    validation_dl,\n",
        "    model,\n",
        "    optimizer,\n",
        "    n_epochs=1,\n",
        "    save_name=\"0\",\n",
        "    emotion_weights=None,\n",
        "):\n",
        "    model_folder = Path.cwd().joinpath(\"models\")\n",
        "    if not model_folder.exists():\n",
        "        model_folder.mkdir(parents=True)\n",
        "\n",
        "    history = {\n",
        "        \"train_acc_emo\": [],\n",
        "        \"train_acc_tri\": [],\n",
        "        \"train_losses\": [],\n",
        "        \"val_acc_emo\": [],\n",
        "        \"val_acc_trig\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_f1_trigger_unrolled\": [],\n",
        "        \"val_f1_trigger_episode\": [],\n",
        "        \"val_f1_emotion_unrolled\": [],\n",
        "        \"val_f1_emotion_episode\": [],\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        print(f\"Epoch {epoch}/{n_epochs}\")\n",
        "        (\n",
        "            model,\n",
        "            accuracy_emotions,\n",
        "            accuracy_triggers,\n",
        "            train_loss,\n",
        "        ) = train_model(train_dl, model, optimizer, emotion_weights)\n",
        "        (\n",
        "            val_accuracy_emotions,\n",
        "            val_accuracy_triggers,\n",
        "            val_loss,\n",
        "            unrolled_f1_emotions,\n",
        "            unrolled_f1_triggers,\n",
        "            episode_f1_emotions,\n",
        "            episode_f1_triggers,\n",
        "        ) = eval_model(validation_dl, model, emotion_weights)\n",
        "\n",
        "        print(\n",
        "            f\"train_loss={np.mean(train_loss):.4f}, val_loss={np.mean(val_loss):.4f}, \"\n",
        "            f\"train_acc_emo={accuracy_emotions:.4f}, train_acc_tri={accuracy_triggers:.4f},\"\n",
        "            f\"val_acc_emo={val_accuracy_emotions:.4f}, val_acc_tri={val_accuracy_triggers:.4f}, \"\n",
        "        )\n",
        "\n",
        "        history[\"train_acc_emo\"].append(accuracy_emotions)\n",
        "        history[\"train_acc_tri\"].append(accuracy_triggers)\n",
        "        history[\"train_losses\"].append(train_loss)\n",
        "        history[\"val_acc_emo\"].append(val_accuracy_emotions)\n",
        "        history[\"val_acc_trig\"].append(val_accuracy_triggers)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_f1_trigger_unrolled\"].append(unrolled_f1_triggers)\n",
        "        history[\"val_f1_trigger_episode\"].append(episode_f1_triggers)\n",
        "        history[\"val_f1_emotion_episode\"].append(episode_f1_emotions)\n",
        "        history[\"val_f1_emotion_unrolled\"].append(unrolled_f1_emotions)\n",
        "\n",
        "        # save the best model\n",
        "        # if f1_overall > best_f1:\n",
        "        #   torch.save(\n",
        "        #      model.state_dict(),\n",
        "        #      Path.joinpath(model_folder, f\"model_{save_name}.bin\"),\n",
        "        # )\n",
        "        # best_f1 = f1_overall\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sK9CWMdVVyT"
      },
      "outputs": [],
      "source": [
        "def show_results(history, epoch=0):\n",
        "    print(\"RESULTS\")\n",
        "    print(f\"VAL ACC EMO: {history['val_acc_emo'][-1]:.3f}\")\n",
        "    print(f\"VAL ACC TRIG: {history['val_acc_trig'][-1]:.3f}\")\n",
        "    print(\n",
        "        f\"VAL F1 TRIG UNROLLED: {history['val_f1_trigger_unrolled'][-1]:.3f}\")\n",
        "    print(f\"VAL F1 TRIG EPISODE: {history['val_f1_trigger_episode'][-1]:.3f}\")\n",
        "    print(f\"VAL F1 EMO UNROLLED: {history['val_f1_emotion_unrolled'][-1]:.3f}\")\n",
        "    print(f\"VAL F1 EMO EPISODE: {history['val_f1_emotion_episode'][-1]:.3f}\")\n",
        "    tmp = []\n",
        "    for h in history[\"train_losses\"]:\n",
        "        tmp.extend(h)\n",
        "    plt.plot(tmp)\n",
        "    plt.title(\"TRAIN LOSS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cpv0jF3ay5v"
      },
      "outputs": [],
      "source": [
        "def create_data_loaders(tokenized_datasets, batch_size):\n",
        "    train_dl = torch.utils.data.DataLoader(\n",
        "        tokenized_datasets[\"train\"],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "    )\n",
        "\n",
        "    validation_dl = torch.utils.data.DataLoader(\n",
        "        tokenized_datasets[\"validation\"],\n",
        "        batch_size=batch_size * 4,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "    )\n",
        "\n",
        "    test_dl = torch.utils.data.DataLoader(\n",
        "        tokenized_datasets[\"test\"],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "    )\n",
        "    return train_dl, validation_dl, test_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-mBQdifay5v"
      },
      "outputs": [],
      "source": [
        "batch_size = 32 if not DEBUG_MODE else 2\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(\n",
        "    ds_train_tokenized, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "validation_dl = torch.utils.data.DataLoader(\n",
        "    ds_val_tokenized, batch_size=batch_size, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjkYlPyJay5v"
      },
      "source": [
        "## Training Custom loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KrlEPU6ay5v"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 1 if not (DEBUG_MODE) else 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRZu4iFBQu_K"
      },
      "outputs": [],
      "source": [
        "# FROZEN\n",
        "model_frozen, optimizer = setup(learning_rate=5e-5, frozen=True)\n",
        "\n",
        "history_frozen = train_eval(\n",
        "    train_dl,\n",
        "    validation_dl,\n",
        "    model=model_frozen,\n",
        "    optimizer=optimizer,\n",
        "    n_epochs=N_EPOCHS,\n",
        "    save_name=f\"test_model\",\n",
        ")\n",
        "\n",
        "show_results(history_frozen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5c1914CY7gf"
      },
      "outputs": [],
      "source": [
        "torch.save(model_frozen.state_dict(), Path(\"models/model_frozen.bin\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyh6Bb243CHN"
      },
      "outputs": [],
      "source": [
        "# import csv\n",
        "#\n",
        "# import collections\n",
        "#\n",
        "# counter = collections.defaultdict(int)\n",
        "# for row in history:\n",
        "#    counter[row[1]] += 1\n",
        "# for row in history:\n",
        "#    if counter[row[1]] >= 4:\n",
        "#        writer = csv.writer(open(\"test1.csv\", \"wb\"))\n",
        "#        writer.writerows(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV5E-KfnQu_L"
      },
      "outputs": [],
      "source": [
        "# FULL\n",
        "model_full, optimizer = setup(learning_rate=5e-5, frozen=False)\n",
        "\n",
        "history_full = train_eval(\n",
        "    train_dl,\n",
        "    validation_dl,\n",
        "    model=model_full,\n",
        "    optimizer=optimizer,\n",
        "    n_epochs=1,\n",
        "    save_name=f\"test_model\",\n",
        ")\n",
        "\n",
        "show_results(history_full)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOuxXNzEVVyV"
      },
      "source": [
        "## Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoIZbY8nVVyV"
      },
      "outputs": [],
      "source": [
        "emotion_weights = compute_class_weights(\n",
        "    df_train[\"emotion\"], root=True, normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfyje5iPVVyV"
      },
      "outputs": [],
      "source": [
        "# FROZEN weights\n",
        "model_frozen_weights, optimizer = setup(learning_rate=5e-5, frozen=True)\n",
        "\n",
        "history_frozen_weights = train_eval(\n",
        "    train_dl,\n",
        "    validation_dl,\n",
        "    model=model_frozen_weights,\n",
        "    optimizer=optimizer,\n",
        "    n_epochs=N_EPOCHS,\n",
        "    save_name=f\"test_model\",\n",
        "    emotion_weights=emotion_weights,\n",
        ")\n",
        "\n",
        "show_results(history_frozen_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6J3VoiHVVyV"
      },
      "outputs": [],
      "source": [
        "# FULL weights\n",
        "model_full_weights, optimizer = setup(learning_rate=5e-5, frozen=False)\n",
        "\n",
        "history_full_weights = train_eval(\n",
        "    train_dl,\n",
        "    validation_dl,\n",
        "    model=model_full_weights,\n",
        "    optimizer=optimizer,\n",
        "    n_epochs=N_EPOCHS,\n",
        "    save_name=f\"test_model\",\n",
        "    emotion_weights=emotion_weights,\n",
        ")\n",
        "\n",
        "show_results(history_full_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EygQk3Q3ay5w"
      },
      "source": [
        "## Training con Classe Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY5SyGsaay5w"
      },
      "outputs": [],
      "source": [
        "class BertTrainer(Trainer):\n",
        "    def __init__(self, model, training_args, train_ds, eval_ds, metrics):\n",
        "        super().__init__(model, training_args, train_ds, eval_ds, metrics)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        emotions_labels = inputs[\"emotions\"]\n",
        "        triggers_labels = inputs[\"triggers\"]\n",
        "        output_emotions, output_triggers = model(ids=ids, mask=mask)\n",
        "\n",
        "        custom_loss = self.loss_fn(\n",
        "            output_emotions, output_triggers, emotions_labels, triggers_labels\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            (custom_loss, output_emotions, output_triggers)\n",
        "            if return_outputs\n",
        "            else custom_loss\n",
        "        )\n",
        "\n",
        "    def loss_fn(outputs_emotions, outputs_triggers, emotions_labels, triggers_labels):\n",
        "        return torch.nn.CrossEntropyLoss(\n",
        "            outputs_emotions, emotions_labels\n",
        "        ) + torch.nn.BCELoss(outputs_triggers, triggers_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY34AUnn_A-5"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"\\\\test\",\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    # evaluate_during_training=True,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=8,\n",
        "    # seed=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN1wEj6C_A-5"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model_full,\n",
        "    args=training_args,\n",
        "    # data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    train_dataset=train_dl,\n",
        "    eval_dataset=validation_dl,\n",
        "    compute_metrics=sequence_f1,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXOTSg_3ay5w"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
