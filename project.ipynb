{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\porce\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\porce\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'c:\\Users\\porce\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'c:\\Users\\porce\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/b6/4d/fbe6d89fde59d8107f0a02816c4ac4542a8f9a85559fdf33c68282affcc1/transformers-4.38.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "     ---------------------------------------- 0.0/130.7 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/130.7 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 30.7/130.7 kB 325.1 kB/s eta 0:00:01\n",
      "     -------- ---------------------------- 30.7/130.7 kB 325.1 kB/s eta 0:00:01\n",
      "     -------- ---------------------------- 30.7/130.7 kB 325.1 kB/s eta 0:00:01\n",
      "     ----------------------- ------------- 81.9/130.7 kB 327.3 kB/s eta 0:00:01\n",
      "     -------------------------- ---------- 92.2/130.7 kB 374.1 kB/s eta 0:00:01\n",
      "     ------------------------------------ 130.7/130.7 kB 428.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\porce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/ab/28/d4b691840d73126d4c9845f8a22dad033ac872509b6d3a0d93b456eef424/huggingface_hub-0.21.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\porce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\porce\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/2b/9f/fbade56564ad486809c27b322d0f7e6a89c01f6b4fe208402e90d4443a99/PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/1d/af/4bd17254cdda1d8092460ee5561f013c4ca9c33ecf1aab81b44280327cab/regex-2023.12.25-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.12.25-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting requests (from transformers)\n",
      "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/c9/ea/5800f4941a713b2feed955b6a256aacc1ca68a6699916d2668622c075d38/tokenizers-0.15.2-cp312-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/7c/22/27ba66710dda4cdca147ad5d8666d9cf0e2035c0aecb5f4de4e6b0e1bc49/safetensors-0.4.2-cp312-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.2-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Obtaining dependency information for tqdm>=4.27 from https://files.pythonhosted.org/packages/2a/14/e75e52d521442e2fcc9f1df3c5e456aead034203d4797867980de558ab34/tqdm-4.66.2-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\porce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\porce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\porce\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/b6/7c/8debebb4f90174074b827c63242c23851bdf00a532489fba57fef3416e40/charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/c2/e7/a82b05cf63a603df6e68d59ae6a68bf5064484a0718ea5033660af4b54a9/idna-3.6-py3-none-any.whl.metadata\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/a2/73/a68704750a7679d0b6d3ad7aa8d4da8e14e151ae82e6fee774e6e0d05ec8/urllib3-2.2.1-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/ba/06/a07f096c664aeb9f01624f858c3add0a4e913d6c96257acb4fce61e7de14/certifi-2024.2.2-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/8.5 MB 5.3 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.3/8.5 MB 4.2 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.3/8.5 MB 2.4 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/8.5 MB 3.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.6/8.5 MB 2.9 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.8/8.5 MB 3.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.1/8.5 MB 3.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.1/8.5 MB 3.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.4/8.5 MB 3.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.5/8.5 MB 3.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.5/8.5 MB 3.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.8/8.5 MB 3.3 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.9/8.5 MB 3.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.1/8.5 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.3/8.5 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.3/8.5 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.6/8.5 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.7/8.5 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.7/8.5 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.0/8.5 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.0/8.5 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.3/8.5 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.4/8.5 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.5/8.5 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.8/8.5 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.9/8.5 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.9/8.5 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.1/8.5 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.2/8.5 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.5/8.5 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.7/8.5 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.7/8.5 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.0/8.5 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.1/8.5 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.1/8.5 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.4/8.5 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.4/8.5 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.6/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.9/8.5 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.9/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.1/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.2/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.2/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.4/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.5/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.7/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.9/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.9/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.1/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.2/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.2/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.4/8.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.4/8.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.7/8.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.9/8.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.0/8.5 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.3/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.4/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.4/8.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.5/8.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.5/8.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.5/8.5 MB 3.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "   ---------------------------------------- 0.0/346.4 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 61.4/346.4 kB 3.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 235.5/346.4 kB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 346.4/346.4 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "   ---------------------------------------- 0.0/138.7 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 41.0/138.7 kB ? eta -:--:--\n",
      "   -------------------- ------------------ 71.7/138.7 kB 787.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 138.7/138.7 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp312-cp312-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.9 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/268.9 kB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 268.9/268.9 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp312-none-win_amd64.whl (270 kB)\n",
      "   ---------------------------------------- 0.0/270.7 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 61.4/270.7 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 112.6/270.7 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/270.7 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 270.7/270.7 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.3/2.2 MB 6.8 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.9/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.9/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.4/2.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.0/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.0/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.6/62.6 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 163.8/163.8 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "   ---------------------------------------- 0.0/100.4 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 92.2/100.4 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 100.4/100.4 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "   ---------------------------------------- 0.0/61.6 kB ? eta -:--:--\n",
      "   ---------------------------------------  61.4/61.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 61.6/61.6 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 0.0/121.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 121.1/121.1 kB 3.5 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 huggingface-hub-0.21.4 idna-3.6 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.2 tokenizers-0.15.2 tqdm-4.66.2 transformers-4.38.2 urllib3-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\porce\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load corpus\n",
    "\n",
    "data_path = Path(\"data/MELD_train_efr.json\")\n",
    "assert data_path.exists(), \"Data file is not present\"\n",
    "# TODO download from GDrive?\n",
    "df = pd.read_json(data_path, dtype={\"speakers\": np.array, \"triggers\": np.array})\n",
    "EPISODE, SPEAKERS, EMOTIONS, UTTERANCES, TRIGGERS = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>speakers</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>utterance_0</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise]</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>utterance_1</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>utterance_2</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>utterance_3</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[also I was the point person on my company's t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utterance_4</td>\n",
       "      <td>[Joey, Rachel, Joey, Rachel]</td>\n",
       "      <td>[surprise, sadness, surprise, fear]</td>\n",
       "      <td>[But then who? The waitress I went out with la...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       episode                                           speakers  \\\n",
       "0  utterance_0  [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "1  utterance_1  [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "2  utterance_2  [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "3  utterance_3  [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "4  utterance_4                       [Joey, Rachel, Joey, Rachel]   \n",
       "\n",
       "                                            emotions  \\\n",
       "0     [neutral, neutral, neutral, neutral, surprise]   \n",
       "1  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "2  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "3  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "4                [surprise, sadness, surprise, fear]   \n",
       "\n",
       "                                          utterances  \\\n",
       "0  [also I was the point person on my company's t...   \n",
       "1  [also I was the point person on my company's t...   \n",
       "2  [also I was the point person on my company's t...   \n",
       "3  [also I was the point person on my company's t...   \n",
       "4  [But then who? The waitress I went out with la...   \n",
       "\n",
       "                                            triggers  \n",
       "0                          [0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4                               [0.0, 0.0, 1.0, 0.0]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>speakers</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4000</td>\n",
       "      <td>3350</td>\n",
       "      <td>3427</td>\n",
       "      <td>3998</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>utterance_0</td>\n",
       "      <td>[Monica, Chandler, Monica]</td>\n",
       "      <td>[neutral, neutral, joy]</td>\n",
       "      <td>[Happy?! Is that what I'm supposed to be Vic? ...</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            episode                    speakers                 emotions  \\\n",
       "count          4000                        4000                     4000   \n",
       "unique         4000                        3350                     3427   \n",
       "top     utterance_0  [Monica, Chandler, Monica]  [neutral, neutral, joy]   \n",
       "freq              1                          15                       30   \n",
       "\n",
       "                                               utterances         triggers  \n",
       "count                                                4000             4000  \n",
       "unique                                               3998              523  \n",
       "top     [Happy?! Is that what I'm supposed to be Vic? ...  [0.0, 1.0, 0.0]  \n",
       "freq                                                    2              191  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups: 832\n",
      "Avg group len: 4.8\n",
      "Longest group: 16\n",
      "Episodes not in a group: 128\n"
     ]
    }
   ],
   "source": [
    "### Look for how many groups of episodes with the same first utterance there are and their lenghts\n",
    "\n",
    "df.sort_values(by=UTTERANCES, inplace=True)\n",
    "groups = np.zeros((850,), dtype=int)\n",
    "\n",
    "index = 0\n",
    "count = 1\n",
    "for i in range(1, len(df[UTTERANCES])):\n",
    "    if df[UTTERANCES][i][0] == df[UTTERANCES][i - 1][0]:\n",
    "        ### still in the same group\n",
    "        count += 1\n",
    "    else:\n",
    "        ### found new group\n",
    "        groups[index] = count\n",
    "        index += 1\n",
    "        count = 1\n",
    "\n",
    "groups = groups[groups != 0]\n",
    "\n",
    "print(f\"Number of groups: {len(groups)}\")\n",
    "print(f\"Avg group len: {np.average(groups):.1f}\")\n",
    "print(f\"Longest group: {np.max(groups)}\")\n",
    "print(f\"Episodes not in a group: {groups[groups == 1].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of number of speakers:\n",
      "1 speakers:  214\n",
      "2 speakers:  2105\n",
      "3 speakers:  1030\n",
      "4 speakers:  405\n",
      "5 speakers:  161\n",
      "6 speakers:  74\n",
      "7 speakers:  10\n",
      "8 speakers:  1\n"
     ]
    }
   ],
   "source": [
    "### Count how many speakers there are in each episode\n",
    "\n",
    "speakers_count = df[SPEAKERS].apply(lambda arr: np.unique(arr).shape[0]).to_numpy()\n",
    "min_sp = np.min(speakers_count)\n",
    "max_sp = np.max(speakers_count)\n",
    "print(\"Distribution of number of speakers:\")\n",
    "for count in range(min_sp, max_sp + 1):\n",
    "    print(f\"{count} speakers:  {np.sum(speakers_count == count)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakers</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>[Joey, Gunther, Joey]</td>\n",
       "      <td>[joy, neutral, surprise]</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>[Joey, Gunther, Joey, Gunther]</td>\n",
       "      <td>[joy, neutral, surprise, surprise]</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>[Joey, Gunther, Joey, Gunther, Joey]</td>\n",
       "      <td>[joy, neutral, surprise, surprise, neutral]</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>[Joey, Gunther, Joey, Gunther, Joey, Gunther]</td>\n",
       "      <td>[joy, neutral, surprise, surprise, neutral, ne...</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>[Joey, Gunther, Joey, Gunther, Joey, Gunther, ...</td>\n",
       "      <td>[joy, neutral, surprise, surprise, neutral, ne...</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>[Singer, Joey, Phoebe, Chandler, Phoebe, Chand...</td>\n",
       "      <td>[joy, surprise, anger, neutral, neutral, neutr...</td>\n",
       "      <td>[Cause every time I see your face, I can't he...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>[Singer, Joey, Phoebe, Chandler, Phoebe, Chand...</td>\n",
       "      <td>[joy, surprise, anger, neutral, neutral, neutr...</td>\n",
       "      <td>[Cause every time I see your face, I can't he...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>[Singer, Joey, Phoebe, Chandler, Phoebe, Chand...</td>\n",
       "      <td>[joy, surprise, anger, neutral, neutral, neutr...</td>\n",
       "      <td>[Cause every time I see your face, I can't he...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>[Singer, Joey, Phoebe, Chandler, Phoebe, Chand...</td>\n",
       "      <td>[joy, surprise, anger, neutral, neutral, neutr...</td>\n",
       "      <td>[Cause every time I see your face, I can't he...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>[Joey, The Presenter, Joey]</td>\n",
       "      <td>[joy, neutral, surprise]</td>\n",
       "      <td>[Kay!, in the category of Favorite Returning ...</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               speakers  \\\n",
       "1061                              [Joey, Gunther, Joey]   \n",
       "1062                     [Joey, Gunther, Joey, Gunther]   \n",
       "1063               [Joey, Gunther, Joey, Gunther, Joey]   \n",
       "1064      [Joey, Gunther, Joey, Gunther, Joey, Gunther]   \n",
       "1065  [Joey, Gunther, Joey, Gunther, Joey, Gunther, ...   \n",
       "...                                                 ...   \n",
       "597   [Singer, Joey, Phoebe, Chandler, Phoebe, Chand...   \n",
       "598   [Singer, Joey, Phoebe, Chandler, Phoebe, Chand...   \n",
       "599   [Singer, Joey, Phoebe, Chandler, Phoebe, Chand...   \n",
       "600   [Singer, Joey, Phoebe, Chandler, Phoebe, Chand...   \n",
       "635                         [Joey, The Presenter, Joey]   \n",
       "\n",
       "                                               emotions  \\\n",
       "1061                           [joy, neutral, surprise]   \n",
       "1062                 [joy, neutral, surprise, surprise]   \n",
       "1063        [joy, neutral, surprise, surprise, neutral]   \n",
       "1064  [joy, neutral, surprise, surprise, neutral, ne...   \n",
       "1065  [joy, neutral, surprise, surprise, neutral, ne...   \n",
       "...                                                 ...   \n",
       "597   [joy, surprise, anger, neutral, neutral, neutr...   \n",
       "598   [joy, surprise, anger, neutral, neutral, neutr...   \n",
       "599   [joy, surprise, anger, neutral, neutral, neutr...   \n",
       "600   [joy, surprise, anger, neutral, neutral, neutr...   \n",
       "635                            [joy, neutral, surprise]   \n",
       "\n",
       "                                             utterances  \\\n",
       "1061  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1062  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1063  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1064  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1065  [\"Happy birthday to you!\", You're paying for t...   \n",
       "...                                                 ...   \n",
       "597   [Cause every time I see your face, I can't he...   \n",
       "598   [Cause every time I see your face, I can't he...   \n",
       "599   [Cause every time I see your face, I can't he...   \n",
       "600   [Cause every time I see your face, I can't he...   \n",
       "635   [Kay!, in the category of Favorite Returning ...   \n",
       "\n",
       "                                               triggers  \n",
       "1061                                    [0.0, 1.0, 0.0]  \n",
       "1062                               [0.0, 0.0, 1.0, 0.0]  \n",
       "1063                          [0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1064                     [0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1065                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "...                                                 ...  \n",
       "597       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "598   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
       "599   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "600   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "635                                     [0.0, 1.0, 0.0]  \n",
       "\n",
       "[4000 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Drop not useful column\n",
    "\n",
    "df.drop(columns=[EPISODE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove Nones from the triggers\n",
    "\n",
    "# TODO no conversion to int, we may need float laters for label smoothing or so\n",
    "df[TRIGGERS] = df[TRIGGERS].apply(\n",
    "    lambda trig_seq: np.array([0.0 if t is None else t for t in trig_seq])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>speakers</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>utterance_1061</td>\n",
       "      <td>[Joey, Gunther, Joey]</td>\n",
       "      <td>[joy, neutral, surprise]</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>utterance_1062</td>\n",
       "      <td>[Joey, Gunther, Joey, Gunther]</td>\n",
       "      <td>[joy, neutral, surprise, surprise]</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>utterance_1063</td>\n",
       "      <td>[Joey, Gunther, Joey, Gunther, Joey]</td>\n",
       "      <td>[joy, neutral, surprise, surprise, neutral]</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>utterance_1064</td>\n",
       "      <td>[Joey, Gunther, Joey, Gunther, Joey, Gunther]</td>\n",
       "      <td>[joy, neutral, surprise, surprise, neutral, ne...</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>utterance_1065</td>\n",
       "      <td>[Joey, Gunther, Joey, Gunther, Joey, Gunther, ...</td>\n",
       "      <td>[joy, neutral, surprise, surprise, neutral, ne...</td>\n",
       "      <td>[\"Happy birthday to you!\", You're paying for t...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             episode                                           speakers  \\\n",
       "1061  utterance_1061                              [Joey, Gunther, Joey]   \n",
       "1062  utterance_1062                     [Joey, Gunther, Joey, Gunther]   \n",
       "1063  utterance_1063               [Joey, Gunther, Joey, Gunther, Joey]   \n",
       "1064  utterance_1064      [Joey, Gunther, Joey, Gunther, Joey, Gunther]   \n",
       "1065  utterance_1065  [Joey, Gunther, Joey, Gunther, Joey, Gunther, ...   \n",
       "\n",
       "                                               emotions  \\\n",
       "1061                           [joy, neutral, surprise]   \n",
       "1062                 [joy, neutral, surprise, surprise]   \n",
       "1063        [joy, neutral, surprise, surprise, neutral]   \n",
       "1064  [joy, neutral, surprise, surprise, neutral, ne...   \n",
       "1065  [joy, neutral, surprise, surprise, neutral, ne...   \n",
       "\n",
       "                                             utterances  \\\n",
       "1061  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1062  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1063  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1064  [\"Happy birthday to you!\", You're paying for t...   \n",
       "1065  [\"Happy birthday to you!\", You're paying for t...   \n",
       "\n",
       "                                 triggers  \n",
       "1061                      [0.0, 1.0, 0.0]  \n",
       "1062                 [0.0, 0.0, 1.0, 0.0]  \n",
       "1063            [0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1064       [0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "1065  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing:\n",
    "If an episode contains the same utterances of the previous and a few more then the triggers from the previous episode are replicated in the current episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 0. 1.]\n",
      "[0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{df[TRIGGERS][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(1, len(df)):\n",
    "    # TODO discuss: the version that does all the checks is faster\n",
    "    is_continuation = np.all([u in df[UTTERANCES][i] for u in df[UTTERANCES][i - 1]])\n",
    "    # is_continuation = True\n",
    "    # j = 0\n",
    "    # while is_continuation and j < len(df[UTTERANCES][i - 1]):\n",
    "    #     is_continuation = df[UTTERANCES][i - 1][j] in df[UTTERANCES][i]\n",
    "    #     j += 1\n",
    "    if is_continuation:\n",
    "        count += 1\n",
    "        for k, t in enumerate(df[TRIGGERS][i - 1]):\n",
    "            df[TRIGGERS][i][k] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      "[0. 0. 1. 0.]\n",
      "[0. 0. 1. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0. 1.]\n",
      "[0. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{df[TRIGGERS][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Validation/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Val Test split: 80/10/10\n",
    "\n",
    "\n",
    "def split_data(df: pd.DataFrame, seed: int = 42):\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, test_size=0.2, train_size=0.8, random_state=seed\n",
    "    )\n",
    "\n",
    "    df_val, df_test = train_test_split(\n",
    "        df_test, test_size=0.5, train_size=0.5, random_state=seed\n",
    "    )\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train len: 3200\n",
      "df_val len: 400\n",
      "df_test len: 400\n"
     ]
    }
   ],
   "source": [
    "### Check\n",
    "df_train, df_val, df_test = split_data(df)\n",
    "print(f\"df_train len: {len(df_train)}\")\n",
    "print(f\"df_val len: {len(df_val)}\")\n",
    "print(f\"df_test len: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: give 2 series of sequences of triggers/emotions compute F1 inside each sequence and return avg, flatten out and compute F1\n",
    "###\n",
    "def sequence_f1(y_true, y_pred, avg: bool = True):\n",
    "    res = [\n",
    "        f1_score(y_true=y_t, y_pred=y_p, average=\"micro\")\n",
    "        for y_t, y_p in zip(y_true, y_pred)\n",
    "    ]\n",
    "    return np.average(res) if avg else res\n",
    "\n",
    "\n",
    "def unrolled_f1(y_true, y_pred):\n",
    "    y_t_flat = []\n",
    "    for l in y_true:\n",
    "        for e in l:\n",
    "            y_t_flat.append(e)\n",
    "\n",
    "    y_p_flat = []\n",
    "    for l in y_pred:\n",
    "        for e in l:\n",
    "            y_p_flat.append(e)\n",
    "\n",
    "    return f1_score(y_true=y_t_flat, y_pred=y_p_flat, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create baseline models\n",
    "\n",
    "\n",
    "class SequenceDummyClassifier(DummyClassifier):\n",
    "    def __init__(self, strategy: str, seed: int = 42) -> None:\n",
    "        self.seed = seed\n",
    "        # TODO proper exception\n",
    "        if not strategy.lower() in (\"random\", \"majority\"):\n",
    "            raise ValueError(\"strategy must be in [random, majority]\")\n",
    "        sklearn_strategy = \"uniform\" if strategy == \"random\" else \"most_frequent\"\n",
    "        super().__init__(strategy=sklearn_strategy, random_state=seed)\n",
    "\n",
    "    ### TODO discuss: problem = flattening sequences of != len from the df\n",
    "    ### sol1 = iterate over df and collect 1by1: bad for memory allocation\n",
    "    ### sol2 = pad the sequences in the df, create array, remove padding: can it be more efficient?\n",
    "    # np.array(df[UTTERANCES].tolist()).flatten() does not work because of the != len of the sequences\n",
    "\n",
    "    def _flatten_seq(self, df: pd.Series):\n",
    "        res = []\n",
    "        for l in df:\n",
    "            for e in l:\n",
    "                res.append(e)\n",
    "        return res\n",
    "\n",
    "    def _flatten_seq_(self, df: pd.Series):\n",
    "        max_len = np.max(df.apply(lambda s: len(s)).to_numpy())\n",
    "        dtype = type(df[0][0])\n",
    "        pad_element = dtype(999999)\n",
    "        ### Pad utterances with 0, flatten array\n",
    "        df = np.array(\n",
    "            df.apply(\n",
    "                lambda s: np.hstack(\n",
    "                    (\n",
    "                        s,\n",
    "                        np.repeat(\n",
    "                            [pad_element],\n",
    "                            repeats=(max_len - len(s)),\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "            ).to_list()\n",
    "        ).flatten()\n",
    "        ### Remove padding\n",
    "        return (\n",
    "            df[np.char.not_equal(df, pad_element)]\n",
    "            if dtype == str\n",
    "            else df[df != dtype(pad_element)]\n",
    "        )\n",
    "\n",
    "    def _deflatten_seq(self, seq, shape_like: pd.Series):\n",
    "        ### TODO discuss: we may think to use np.reshape but again the row len is not homogeneous!\n",
    "        data = iter(seq)\n",
    "        result = [[next(data) for _ in s] for s in shape_like]\n",
    "        return result\n",
    "\n",
    "    def fit(self, X: pd.Series, y: pd.Series):\n",
    "        X_flat = self._flatten_seq(X)\n",
    "        y_flat = self._flatten_seq(y)\n",
    "        super().fit(X=X_flat, y=y_flat)\n",
    "\n",
    "    def predict(self, X: pd.Series, return_flat: bool = False):\n",
    "        X_flat = self._flatten_seq(X)\n",
    "        y_flat = super().predict(X_flat)\n",
    "        return y_flat if return_flat else self._deflatten_seq(seq=y_flat, shape_like=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_f1(emotions_Random) : 0.4233349753853817\n",
      "unrolled_f1(emotions_Random) : 0.4330935251798561\n",
      "sequence_f1(triggers_Random) : 0.6519219611872475\n",
      "unrolled_f1(triggers_Random) : 0.6515107913669065\n",
      "sequence_f1(emotions_Majority) : 0.4233349753853817\n",
      "unrolled_f1(emotions_Majority) : 0.4330935251798561\n",
      "sequence_f1(triggers_Majority) : 0.6519219611872475\n",
      "unrolled_f1(triggers_Majority) : 0.6515107913669065\n"
     ]
    }
   ],
   "source": [
    "def experiment_baseline(df_train: pd.DataFrame, df_test: pd.DataFrame, seed: int = 42):\n",
    "\n",
    "    baseline_f1s = {}\n",
    "    baseline_results = {}\n",
    "\n",
    "    for strategy in (\"Random\", \"Majority\"):\n",
    "        for target in (EMOTIONS, TRIGGERS):\n",
    "            clf = SequenceDummyClassifier(strategy=strategy, seed=seed)\n",
    "            clf.fit(X=df_train[UTTERANCES], y=df_train[target])\n",
    "\n",
    "            res = clf.predict(X=df_test[UTTERANCES], return_flat=False)\n",
    "            baseline_results.update({f\"{target}_{strategy}\": res})\n",
    "\n",
    "            seq_f1 = sequence_f1(y_true=df_test[target], y_pred=res)\n",
    "            baseline_f1s.update({f\"sequence_f1({target}_{strategy})\": seq_f1})\n",
    "\n",
    "            unr_f1 = unrolled_f1(y_true=df_test[target], y_pred=res)\n",
    "            baseline_f1s.update({f\"unrolled_f1({target}_{strategy})\": unr_f1})\n",
    "\n",
    "    return baseline_f1s, baseline_results\n",
    "\n",
    "\n",
    "f1s, results = experiment_baseline(df_train, df_test)\n",
    "for k, v in f1s.items():\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_emotions):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, num_emotions)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "    def freeze_params(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight False\n",
      "bert.embeddings.position_embeddings.weight False\n",
      "bert.embeddings.token_type_embeddings.weight False\n",
      "bert.embeddings.LayerNorm.weight False\n",
      "bert.embeddings.LayerNorm.bias False\n",
      "bert.encoder.layer.0.attention.self.query.weight False\n",
      "bert.encoder.layer.0.attention.self.query.bias False\n",
      "bert.encoder.layer.0.attention.self.key.weight False\n",
      "bert.encoder.layer.0.attention.self.key.bias False\n",
      "bert.encoder.layer.0.attention.self.value.weight False\n",
      "bert.encoder.layer.0.attention.self.value.bias False\n",
      "bert.encoder.layer.0.attention.output.dense.weight False\n",
      "bert.encoder.layer.0.attention.output.dense.bias False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.0.intermediate.dense.weight False\n",
      "bert.encoder.layer.0.intermediate.dense.bias False\n",
      "bert.encoder.layer.0.output.dense.weight False\n",
      "bert.encoder.layer.0.output.dense.bias False\n",
      "bert.encoder.layer.0.output.LayerNorm.weight False\n",
      "bert.encoder.layer.0.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.attention.self.query.weight False\n",
      "bert.encoder.layer.1.attention.self.query.bias False\n",
      "bert.encoder.layer.1.attention.self.key.weight False\n",
      "bert.encoder.layer.1.attention.self.key.bias False\n",
      "bert.encoder.layer.1.attention.self.value.weight False\n",
      "bert.encoder.layer.1.attention.self.value.bias False\n",
      "bert.encoder.layer.1.attention.output.dense.weight False\n",
      "bert.encoder.layer.1.attention.output.dense.bias False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.1.intermediate.dense.weight False\n",
      "bert.encoder.layer.1.intermediate.dense.bias False\n",
      "bert.encoder.layer.1.output.dense.weight False\n",
      "bert.encoder.layer.1.output.dense.bias False\n",
      "bert.encoder.layer.1.output.LayerNorm.weight False\n",
      "bert.encoder.layer.1.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.attention.self.query.weight False\n",
      "bert.encoder.layer.2.attention.self.query.bias False\n",
      "bert.encoder.layer.2.attention.self.key.weight False\n",
      "bert.encoder.layer.2.attention.self.key.bias False\n",
      "bert.encoder.layer.2.attention.self.value.weight False\n",
      "bert.encoder.layer.2.attention.self.value.bias False\n",
      "bert.encoder.layer.2.attention.output.dense.weight False\n",
      "bert.encoder.layer.2.attention.output.dense.bias False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.2.intermediate.dense.weight False\n",
      "bert.encoder.layer.2.intermediate.dense.bias False\n",
      "bert.encoder.layer.2.output.dense.weight False\n",
      "bert.encoder.layer.2.output.dense.bias False\n",
      "bert.encoder.layer.2.output.LayerNorm.weight False\n",
      "bert.encoder.layer.2.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.attention.self.query.weight False\n",
      "bert.encoder.layer.3.attention.self.query.bias False\n",
      "bert.encoder.layer.3.attention.self.key.weight False\n",
      "bert.encoder.layer.3.attention.self.key.bias False\n",
      "bert.encoder.layer.3.attention.self.value.weight False\n",
      "bert.encoder.layer.3.attention.self.value.bias False\n",
      "bert.encoder.layer.3.attention.output.dense.weight False\n",
      "bert.encoder.layer.3.attention.output.dense.bias False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.3.intermediate.dense.weight False\n",
      "bert.encoder.layer.3.intermediate.dense.bias False\n",
      "bert.encoder.layer.3.output.dense.weight False\n",
      "bert.encoder.layer.3.output.dense.bias False\n",
      "bert.encoder.layer.3.output.LayerNorm.weight False\n",
      "bert.encoder.layer.3.output.LayerNorm.bias False\n",
      "bert.encoder.layer.4.attention.self.query.weight False\n",
      "bert.encoder.layer.4.attention.self.query.bias False\n",
      "bert.encoder.layer.4.attention.self.key.weight False\n",
      "bert.encoder.layer.4.attention.self.key.bias False\n",
      "bert.encoder.layer.4.attention.self.value.weight False\n",
      "bert.encoder.layer.4.attention.self.value.bias False\n",
      "bert.encoder.layer.4.attention.output.dense.weight False\n",
      "bert.encoder.layer.4.attention.output.dense.bias False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.4.intermediate.dense.weight False\n",
      "bert.encoder.layer.4.intermediate.dense.bias False\n",
      "bert.encoder.layer.4.output.dense.weight False\n",
      "bert.encoder.layer.4.output.dense.bias False\n",
      "bert.encoder.layer.4.output.LayerNorm.weight False\n",
      "bert.encoder.layer.4.output.LayerNorm.bias False\n",
      "bert.encoder.layer.5.attention.self.query.weight False\n",
      "bert.encoder.layer.5.attention.self.query.bias False\n",
      "bert.encoder.layer.5.attention.self.key.weight False\n",
      "bert.encoder.layer.5.attention.self.key.bias False\n",
      "bert.encoder.layer.5.attention.self.value.weight False\n",
      "bert.encoder.layer.5.attention.self.value.bias False\n",
      "bert.encoder.layer.5.attention.output.dense.weight False\n",
      "bert.encoder.layer.5.attention.output.dense.bias False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.5.intermediate.dense.weight False\n",
      "bert.encoder.layer.5.intermediate.dense.bias False\n",
      "bert.encoder.layer.5.output.dense.weight False\n",
      "bert.encoder.layer.5.output.dense.bias False\n",
      "bert.encoder.layer.5.output.LayerNorm.weight False\n",
      "bert.encoder.layer.5.output.LayerNorm.bias False\n",
      "bert.encoder.layer.6.attention.self.query.weight False\n",
      "bert.encoder.layer.6.attention.self.query.bias False\n",
      "bert.encoder.layer.6.attention.self.key.weight False\n",
      "bert.encoder.layer.6.attention.self.key.bias False\n",
      "bert.encoder.layer.6.attention.self.value.weight False\n",
      "bert.encoder.layer.6.attention.self.value.bias False\n",
      "bert.encoder.layer.6.attention.output.dense.weight False\n",
      "bert.encoder.layer.6.attention.output.dense.bias False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.6.intermediate.dense.weight False\n",
      "bert.encoder.layer.6.intermediate.dense.bias False\n",
      "bert.encoder.layer.6.output.dense.weight False\n",
      "bert.encoder.layer.6.output.dense.bias False\n",
      "bert.encoder.layer.6.output.LayerNorm.weight False\n",
      "bert.encoder.layer.6.output.LayerNorm.bias False\n",
      "bert.encoder.layer.7.attention.self.query.weight False\n",
      "bert.encoder.layer.7.attention.self.query.bias False\n",
      "bert.encoder.layer.7.attention.self.key.weight False\n",
      "bert.encoder.layer.7.attention.self.key.bias False\n",
      "bert.encoder.layer.7.attention.self.value.weight False\n",
      "bert.encoder.layer.7.attention.self.value.bias False\n",
      "bert.encoder.layer.7.attention.output.dense.weight False\n",
      "bert.encoder.layer.7.attention.output.dense.bias False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.7.intermediate.dense.weight False\n",
      "bert.encoder.layer.7.intermediate.dense.bias False\n",
      "bert.encoder.layer.7.output.dense.weight False\n",
      "bert.encoder.layer.7.output.dense.bias False\n",
      "bert.encoder.layer.7.output.LayerNorm.weight False\n",
      "bert.encoder.layer.7.output.LayerNorm.bias False\n",
      "bert.encoder.layer.8.attention.self.query.weight False\n",
      "bert.encoder.layer.8.attention.self.query.bias False\n",
      "bert.encoder.layer.8.attention.self.key.weight False\n",
      "bert.encoder.layer.8.attention.self.key.bias False\n",
      "bert.encoder.layer.8.attention.self.value.weight False\n",
      "bert.encoder.layer.8.attention.self.value.bias False\n",
      "bert.encoder.layer.8.attention.output.dense.weight False\n",
      "bert.encoder.layer.8.attention.output.dense.bias False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.8.intermediate.dense.weight False\n",
      "bert.encoder.layer.8.intermediate.dense.bias False\n",
      "bert.encoder.layer.8.output.dense.weight False\n",
      "bert.encoder.layer.8.output.dense.bias False\n",
      "bert.encoder.layer.8.output.LayerNorm.weight False\n",
      "bert.encoder.layer.8.output.LayerNorm.bias False\n",
      "bert.encoder.layer.9.attention.self.query.weight False\n",
      "bert.encoder.layer.9.attention.self.query.bias False\n",
      "bert.encoder.layer.9.attention.self.key.weight False\n",
      "bert.encoder.layer.9.attention.self.key.bias False\n",
      "bert.encoder.layer.9.attention.self.value.weight False\n",
      "bert.encoder.layer.9.attention.self.value.bias False\n",
      "bert.encoder.layer.9.attention.output.dense.weight False\n",
      "bert.encoder.layer.9.attention.output.dense.bias False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.9.intermediate.dense.weight False\n",
      "bert.encoder.layer.9.intermediate.dense.bias False\n",
      "bert.encoder.layer.9.output.dense.weight False\n",
      "bert.encoder.layer.9.output.dense.bias False\n",
      "bert.encoder.layer.9.output.LayerNorm.weight False\n",
      "bert.encoder.layer.9.output.LayerNorm.bias False\n",
      "bert.encoder.layer.10.attention.self.query.weight False\n",
      "bert.encoder.layer.10.attention.self.query.bias False\n",
      "bert.encoder.layer.10.attention.self.key.weight False\n",
      "bert.encoder.layer.10.attention.self.key.bias False\n",
      "bert.encoder.layer.10.attention.self.value.weight False\n",
      "bert.encoder.layer.10.attention.self.value.bias False\n",
      "bert.encoder.layer.10.attention.output.dense.weight False\n",
      "bert.encoder.layer.10.attention.output.dense.bias False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.10.intermediate.dense.weight False\n",
      "bert.encoder.layer.10.intermediate.dense.bias False\n",
      "bert.encoder.layer.10.output.dense.weight False\n",
      "bert.encoder.layer.10.output.dense.bias False\n",
      "bert.encoder.layer.10.output.LayerNorm.weight False\n",
      "bert.encoder.layer.10.output.LayerNorm.bias False\n",
      "bert.encoder.layer.11.attention.self.query.weight False\n",
      "bert.encoder.layer.11.attention.self.query.bias False\n",
      "bert.encoder.layer.11.attention.self.key.weight False\n",
      "bert.encoder.layer.11.attention.self.key.bias False\n",
      "bert.encoder.layer.11.attention.self.value.weight False\n",
      "bert.encoder.layer.11.attention.self.value.bias False\n",
      "bert.encoder.layer.11.attention.output.dense.weight False\n",
      "bert.encoder.layer.11.attention.output.dense.bias False\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias False\n",
      "bert.encoder.layer.11.intermediate.dense.weight False\n",
      "bert.encoder.layer.11.intermediate.dense.bias False\n",
      "bert.encoder.layer.11.output.dense.weight False\n",
      "bert.encoder.layer.11.output.dense.bias False\n",
      "bert.encoder.layer.11.output.LayerNorm.weight False\n",
      "bert.encoder.layer.11.output.LayerNorm.bias False\n",
      "bert.pooler.dense.weight False\n",
      "bert.pooler.dense.bias False\n",
      "classifier.weight True\n",
      "classifier.bias True\n",
      "bert.embeddings.word_embeddings.weight True\n",
      "bert.embeddings.position_embeddings.weight True\n",
      "bert.embeddings.token_type_embeddings.weight True\n",
      "bert.embeddings.LayerNorm.weight True\n",
      "bert.embeddings.LayerNorm.bias True\n",
      "bert.encoder.layer.0.attention.self.query.weight True\n",
      "bert.encoder.layer.0.attention.self.query.bias True\n",
      "bert.encoder.layer.0.attention.self.key.weight True\n",
      "bert.encoder.layer.0.attention.self.key.bias True\n",
      "bert.encoder.layer.0.attention.self.value.weight True\n",
      "bert.encoder.layer.0.attention.self.value.bias True\n",
      "bert.encoder.layer.0.attention.output.dense.weight True\n",
      "bert.encoder.layer.0.attention.output.dense.bias True\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.0.intermediate.dense.weight True\n",
      "bert.encoder.layer.0.intermediate.dense.bias True\n",
      "bert.encoder.layer.0.output.dense.weight True\n",
      "bert.encoder.layer.0.output.dense.bias True\n",
      "bert.encoder.layer.0.output.LayerNorm.weight True\n",
      "bert.encoder.layer.0.output.LayerNorm.bias True\n",
      "bert.encoder.layer.1.attention.self.query.weight True\n",
      "bert.encoder.layer.1.attention.self.query.bias True\n",
      "bert.encoder.layer.1.attention.self.key.weight True\n",
      "bert.encoder.layer.1.attention.self.key.bias True\n",
      "bert.encoder.layer.1.attention.self.value.weight True\n",
      "bert.encoder.layer.1.attention.self.value.bias True\n",
      "bert.encoder.layer.1.attention.output.dense.weight True\n",
      "bert.encoder.layer.1.attention.output.dense.bias True\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.1.intermediate.dense.weight True\n",
      "bert.encoder.layer.1.intermediate.dense.bias True\n",
      "bert.encoder.layer.1.output.dense.weight True\n",
      "bert.encoder.layer.1.output.dense.bias True\n",
      "bert.encoder.layer.1.output.LayerNorm.weight True\n",
      "bert.encoder.layer.1.output.LayerNorm.bias True\n",
      "bert.encoder.layer.2.attention.self.query.weight True\n",
      "bert.encoder.layer.2.attention.self.query.bias True\n",
      "bert.encoder.layer.2.attention.self.key.weight True\n",
      "bert.encoder.layer.2.attention.self.key.bias True\n",
      "bert.encoder.layer.2.attention.self.value.weight True\n",
      "bert.encoder.layer.2.attention.self.value.bias True\n",
      "bert.encoder.layer.2.attention.output.dense.weight True\n",
      "bert.encoder.layer.2.attention.output.dense.bias True\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.2.intermediate.dense.weight True\n",
      "bert.encoder.layer.2.intermediate.dense.bias True\n",
      "bert.encoder.layer.2.output.dense.weight True\n",
      "bert.encoder.layer.2.output.dense.bias True\n",
      "bert.encoder.layer.2.output.LayerNorm.weight True\n",
      "bert.encoder.layer.2.output.LayerNorm.bias True\n",
      "bert.encoder.layer.3.attention.self.query.weight True\n",
      "bert.encoder.layer.3.attention.self.query.bias True\n",
      "bert.encoder.layer.3.attention.self.key.weight True\n",
      "bert.encoder.layer.3.attention.self.key.bias True\n",
      "bert.encoder.layer.3.attention.self.value.weight True\n",
      "bert.encoder.layer.3.attention.self.value.bias True\n",
      "bert.encoder.layer.3.attention.output.dense.weight True\n",
      "bert.encoder.layer.3.attention.output.dense.bias True\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.3.intermediate.dense.weight True\n",
      "bert.encoder.layer.3.intermediate.dense.bias True\n",
      "bert.encoder.layer.3.output.dense.weight True\n",
      "bert.encoder.layer.3.output.dense.bias True\n",
      "bert.encoder.layer.3.output.LayerNorm.weight True\n",
      "bert.encoder.layer.3.output.LayerNorm.bias True\n",
      "bert.encoder.layer.4.attention.self.query.weight True\n",
      "bert.encoder.layer.4.attention.self.query.bias True\n",
      "bert.encoder.layer.4.attention.self.key.weight True\n",
      "bert.encoder.layer.4.attention.self.key.bias True\n",
      "bert.encoder.layer.4.attention.self.value.weight True\n",
      "bert.encoder.layer.4.attention.self.value.bias True\n",
      "bert.encoder.layer.4.attention.output.dense.weight True\n",
      "bert.encoder.layer.4.attention.output.dense.bias True\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.4.intermediate.dense.weight True\n",
      "bert.encoder.layer.4.intermediate.dense.bias True\n",
      "bert.encoder.layer.4.output.dense.weight True\n",
      "bert.encoder.layer.4.output.dense.bias True\n",
      "bert.encoder.layer.4.output.LayerNorm.weight True\n",
      "bert.encoder.layer.4.output.LayerNorm.bias True\n",
      "bert.encoder.layer.5.attention.self.query.weight True\n",
      "bert.encoder.layer.5.attention.self.query.bias True\n",
      "bert.encoder.layer.5.attention.self.key.weight True\n",
      "bert.encoder.layer.5.attention.self.key.bias True\n",
      "bert.encoder.layer.5.attention.self.value.weight True\n",
      "bert.encoder.layer.5.attention.self.value.bias True\n",
      "bert.encoder.layer.5.attention.output.dense.weight True\n",
      "bert.encoder.layer.5.attention.output.dense.bias True\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.5.intermediate.dense.weight True\n",
      "bert.encoder.layer.5.intermediate.dense.bias True\n",
      "bert.encoder.layer.5.output.dense.weight True\n",
      "bert.encoder.layer.5.output.dense.bias True\n",
      "bert.encoder.layer.5.output.LayerNorm.weight True\n",
      "bert.encoder.layer.5.output.LayerNorm.bias True\n",
      "bert.encoder.layer.6.attention.self.query.weight True\n",
      "bert.encoder.layer.6.attention.self.query.bias True\n",
      "bert.encoder.layer.6.attention.self.key.weight True\n",
      "bert.encoder.layer.6.attention.self.key.bias True\n",
      "bert.encoder.layer.6.attention.self.value.weight True\n",
      "bert.encoder.layer.6.attention.self.value.bias True\n",
      "bert.encoder.layer.6.attention.output.dense.weight True\n",
      "bert.encoder.layer.6.attention.output.dense.bias True\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.6.intermediate.dense.weight True\n",
      "bert.encoder.layer.6.intermediate.dense.bias True\n",
      "bert.encoder.layer.6.output.dense.weight True\n",
      "bert.encoder.layer.6.output.dense.bias True\n",
      "bert.encoder.layer.6.output.LayerNorm.weight True\n",
      "bert.encoder.layer.6.output.LayerNorm.bias True\n",
      "bert.encoder.layer.7.attention.self.query.weight True\n",
      "bert.encoder.layer.7.attention.self.query.bias True\n",
      "bert.encoder.layer.7.attention.self.key.weight True\n",
      "bert.encoder.layer.7.attention.self.key.bias True\n",
      "bert.encoder.layer.7.attention.self.value.weight True\n",
      "bert.encoder.layer.7.attention.self.value.bias True\n",
      "bert.encoder.layer.7.attention.output.dense.weight True\n",
      "bert.encoder.layer.7.attention.output.dense.bias True\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.7.intermediate.dense.weight True\n",
      "bert.encoder.layer.7.intermediate.dense.bias True\n",
      "bert.encoder.layer.7.output.dense.weight True\n",
      "bert.encoder.layer.7.output.dense.bias True\n",
      "bert.encoder.layer.7.output.LayerNorm.weight True\n",
      "bert.encoder.layer.7.output.LayerNorm.bias True\n",
      "bert.encoder.layer.8.attention.self.query.weight True\n",
      "bert.encoder.layer.8.attention.self.query.bias True\n",
      "bert.encoder.layer.8.attention.self.key.weight True\n",
      "bert.encoder.layer.8.attention.self.key.bias True\n",
      "bert.encoder.layer.8.attention.self.value.weight True\n",
      "bert.encoder.layer.8.attention.self.value.bias True\n",
      "bert.encoder.layer.8.attention.output.dense.weight True\n",
      "bert.encoder.layer.8.attention.output.dense.bias True\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.8.intermediate.dense.weight True\n",
      "bert.encoder.layer.8.intermediate.dense.bias True\n",
      "bert.encoder.layer.8.output.dense.weight True\n",
      "bert.encoder.layer.8.output.dense.bias True\n",
      "bert.encoder.layer.8.output.LayerNorm.weight True\n",
      "bert.encoder.layer.8.output.LayerNorm.bias True\n",
      "bert.encoder.layer.9.attention.self.query.weight True\n",
      "bert.encoder.layer.9.attention.self.query.bias True\n",
      "bert.encoder.layer.9.attention.self.key.weight True\n",
      "bert.encoder.layer.9.attention.self.key.bias True\n",
      "bert.encoder.layer.9.attention.self.value.weight True\n",
      "bert.encoder.layer.9.attention.self.value.bias True\n",
      "bert.encoder.layer.9.attention.output.dense.weight True\n",
      "bert.encoder.layer.9.attention.output.dense.bias True\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.9.intermediate.dense.weight True\n",
      "bert.encoder.layer.9.intermediate.dense.bias True\n",
      "bert.encoder.layer.9.output.dense.weight True\n",
      "bert.encoder.layer.9.output.dense.bias True\n",
      "bert.encoder.layer.9.output.LayerNorm.weight True\n",
      "bert.encoder.layer.9.output.LayerNorm.bias True\n",
      "bert.encoder.layer.10.attention.self.query.weight True\n",
      "bert.encoder.layer.10.attention.self.query.bias True\n",
      "bert.encoder.layer.10.attention.self.key.weight True\n",
      "bert.encoder.layer.10.attention.self.key.bias True\n",
      "bert.encoder.layer.10.attention.self.value.weight True\n",
      "bert.encoder.layer.10.attention.self.value.bias True\n",
      "bert.encoder.layer.10.attention.output.dense.weight True\n",
      "bert.encoder.layer.10.attention.output.dense.bias True\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.10.intermediate.dense.weight True\n",
      "bert.encoder.layer.10.intermediate.dense.bias True\n",
      "bert.encoder.layer.10.output.dense.weight True\n",
      "bert.encoder.layer.10.output.dense.bias True\n",
      "bert.encoder.layer.10.output.LayerNorm.weight True\n",
      "bert.encoder.layer.10.output.LayerNorm.bias True\n",
      "bert.encoder.layer.11.attention.self.query.weight True\n",
      "bert.encoder.layer.11.attention.self.query.bias True\n",
      "bert.encoder.layer.11.attention.self.key.weight True\n",
      "bert.encoder.layer.11.attention.self.key.bias True\n",
      "bert.encoder.layer.11.attention.self.value.weight True\n",
      "bert.encoder.layer.11.attention.self.value.bias True\n",
      "bert.encoder.layer.11.attention.output.dense.weight True\n",
      "bert.encoder.layer.11.attention.output.dense.bias True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias True\n",
      "bert.encoder.layer.11.intermediate.dense.weight True\n",
      "bert.encoder.layer.11.intermediate.dense.bias True\n",
      "bert.encoder.layer.11.output.dense.weight True\n",
      "bert.encoder.layer.11.output.dense.bias True\n",
      "bert.encoder.layer.11.output.LayerNorm.weight True\n",
      "bert.encoder.layer.11.output.LayerNorm.bias True\n",
      "bert.pooler.dense.weight True\n",
      "bert.pooler.dense.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "# TODO Emotions and triggers tuning of class definiton\n",
    "model_frozen = BERTClassifier(3)\n",
    "model_full = BERTClassifier(3)\n",
    "\n",
    "model_frozen.freeze_params()\n",
    "\n",
    "#Verifying that the params are actually frozen\n",
    "for name, param in model_frozen.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "\n",
    "for name, param in model_full.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [model_full, model_frozen]\n",
    "num_epochs = 5\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "for model in model_list:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    #Tokenizer initiation\n",
    "    # TODO Check tokenizer parameters\n",
    "    # TODO A Dataloader seems to be commonly used for this use cases\n",
    "    encoding = tokenizer(df_train, truncation=False, padding='max_length', return_tensors='pt')\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    #Training loop\n",
    "    #for epoch in range(num_epochs):\n",
    "    #    \n",
    "    #    for idx in range(len(df_test)):\n",
    "\n",
    "    #        text = df_train[idx].drop(TRIGGERS)\n",
    "    #        label = df_train[idx][TRIGGERS]\n",
    "\n",
    "    #        optimizer.zero_grad() \n",
    "\n",
    "    #        logits = model(batch_data)\n",
    "    #        loss = loss_fn(logits, batch_labels)\n",
    "    #        loss.backward()\n",
    "    #        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
