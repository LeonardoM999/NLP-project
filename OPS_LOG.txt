03 10
playground work
there are ~833 groups of dialogues with the same first utterance
only the last trigger is in the ds -> data augmentation?

03 11
definitive data exploration experiments
there can be up to 8 speakers per episode
train test split prepared for multiple seeds
dummyClassifier are not sequence models: we flatten the sequences (with strange ideas)
custom f1 metrics
baseline models

03 16 LP
Defined BERT model (both with full and freezed parameters) --> ongoing
Defined an *attempt* at training method --> ongoing

03 17 GDM
Analyzed Class imbalances: weights could be beneficial,
there's a strong Class imbalance.

Defined combined loss function: There's to better understand which 2 loss
functions are better, and which is the best way to combine them.
We could decide to penalize one of the 2 loss.
For the triggers a BCE is the obvious choice.
While a standard Cross Entropy could be the better one for emotions.
We could also use Sparse Cross Entropy, if we don't want to Hot Encode the Labels


Defined Bert model
Defined Tokenization function (to revise)
Defined training function (to be tested)

03 17 all 3
tokenization function 
Trainer API tests: subclassed Trainer to reimplement compute_loss for 2 outputs
also used standard Trainer class
Datasets/Dataloaders
we need to do the traditional training loop

03 23
flattening a Series of lists: nester fors still is the fastest, tried grouping by list lenght
dropped column SPEAKERS from the beginning and not in the tokenization
one hot encoding of emotions (with all the sequences and variable batch size!)