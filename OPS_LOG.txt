03 10
playground work
there are ~833 groups of dialogues with the same first utterance
only the last trigger is in the ds -> data augmentation?

03 11
definitive data exploration experiments
there can be up to 8 speakers per episode
train test split prepared for multiple seeds
dummyClassifier are not sequence models: we flatten the sequences (with strange ideas)
custom f1 metrics
baseline models

03 16 LP
Defined BERT model (both with full and freezed parameters) --> ongoing
Defined an *attempt* at training method --> ongoing

03 17 GDM
Analyzed Class imbalances: weights could be beneficial,
there's a strong Class imbalance.

Defined combined loss function: There's to better understand which 2 loss
functions are better, and which is the best way to combine them.
We could decide to penalize one of the 2 loss.
For the triggers a BCE is the obvious choice.
While a standard Cross Entropy could be the better one for emotions.
We could also use Sparse Cross Entropy, if we don't want to Hot Encode the Labels


Defined Bert model
Defined Tokenization function (to revise)
Defined training function (to be tested)

03 17 all 3
tokenization function 
Trainer API tests: subclassed Trainer to reimplement compute_loss for 2 outputs
also used standard Trainer class
Datasets/Dataloaders
we need to do the traditional training loop

03 23 LM
flattening a Series of lists: nester fors still is the fastest, tried grouping by list lenght
dropped column SPEAKERS from the beginning and not in the tokenization
one hot encoding of emotions (with all the sequences and variable batch size!)

03 23 LP
experiments on Trainer

03 24 all 3
experiments on the shapes of the input tensors (input_ids...) they are wrong and training loop/Trainer do not work
we want tensors of shape (batch_size, utterance_len) but we have (batch_size, episode_len, utterance_len) 
using tokenizer flag is_split_into_words merges the utterances but it does not change the shape of the tensors
we get shape (batch_size, 1, n*utterance_len)
so we need to consider 1 utterance at a time, we do it by explodi the dataset: (1 row per utterance)
 but we do not want to loose context!
we want to provide inputs of the type (target=u1, before=u0, after=u2) to have context:
 [CLS u1 SEP u0 SEP u2] or [u0 CLS u1 SEP u2] (same family)
 or [CLS u0 SEP u1 SEP u2] (CLS is for all the utterances)
we have to preprocess the exploded dataset so to have target, before, after in each row
careful when splitting in train/eval/test not to split a dialogue

afternoon:
correctly eploded the dataframe, making sure that the previous utterance of the first utterance of each episode and 
the next utterance of the last utterance are both empty

03 27 LM
train/test/val split is done withing the preprocessing section because it's easier and cleaner
context window: instead of having only 1 prev + 1 next utterance per row we can have as many as we want
column labels are next_1, next_2, next_3... previous_1, previous_2...
created function explode_add_context, called on each of train_df, test_df, val_df
metrics: made new from scratch to fit the new data model. To copmute f1 inside each episode we pass a dataframe 
that contains also the column "episode" as y_true, y_pred is a np array.
The baseline models become trivial: extending the sklearn class is redundant
Column "episode" changed from utterance_xyz to episode_xyz to save my mental health


